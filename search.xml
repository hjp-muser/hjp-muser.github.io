<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hexo+next渲染数学公式]]></title>
    <url>%2F2019%2F09%2F11%2Fhexo-next%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[博客中通常会用latex来写数学公式，但是浏览器并不总是能渲染这些公式，导致在浏览器上看是一堆latex代码，这就很烦了。有一种比较简便的方法去渲染这些公式，就是在浏览器中添加相关的mathjax插件。遗憾的是，并不能保证每个浏览器都会有相应的mathjax插件，并且浏览博客的读者浏览器也不能保证一定装了这些插件。为了从根源上解决问题，我们直接让hexo具备渲染mathjax的能力，这样无论浏览器是否开启mathjax插件，公式都可以完美呈现在读者面前。 参考博客：https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/ 1. 使用Kramed 代替 MarkedMarked渲染引擎不可以渲染mathjax，但是Kramed可以。因此将hexo默认的Marked引擎卸载，装上Kramed。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：12345// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125; 变成：12345function formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ // return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$'); return text;&#125; 2. 停止使用hexo-math卸载原来的hexo-math：1npm uninstall hexo-math --save 安装hexo-renderer-mathjax包：1npm install hexo-renderer-mathjax --save 3. 更改默认转义规则因为 hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 , 所以我们需要对默认的规则进行修改.首先， 打开&lt; your-hexo-project &gt;/node_modules/kramed/lib/rules/inline.js,把下列代码：1escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改为：1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 把下列代码：1em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改为：1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 4. 配置中开启mathjax在主题 _config.yml 中开启 Mathjax， 首先找到math字段，然后修改两个enable为true，也就是将下列代码：12345math: enable: false ... mathjax: enable: false 更改为：12345math: enable: true ... mathjax: enable: true 需要注意的是，不同版本的next主题，上面的math和mathjax的嵌套关系可能不一致，总之就是把math和mathjax的enable全都设置为true就可以了。 5. 博客中开启mathjax在博客的模板中添加mathjax: true，注意冒号后面是有一个空格的。123456---title: Testing Mathjax with Hexocategory: Uncategorizeddate: 2017/05/03mathjax: true--- 如果嫌弃每次都需要声明开启mathjax，可以在&lt; your-hexo-project &gt;/scaffolds/post.md新增一句mathjax: true1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:mathjax: true--- 6. 更新Mathjax的CDN链接（可选）这个是可选的，一般情况下直接下载hexo-renderer-mathjax是不需要修改CDN的。如果以上配置都弄好后，还是不能渲染公式，可以尝试更新Mathjax的CDN链接。更新方法是：打开/node_modules/hexo-renderer-mathjax/mathjax.html，然后把标签\更改为：1&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; 注意：网上给出的CDN链接都未必百分百可用，建议多找几个试一下。可以去搜索mathjax国内cdn，参考网址：https://www.bootcdn.cn/mathjax/]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG代码实现]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1. 算法伪代码 首先是构建模型。构建并随机初始化估计网络 $Q(s,a|\theta^Q)$ 和 $\mu(s|\theta^\mu)$ 的权重 $\theta^Q$ 和 $\theta^\mu$。 构建目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，并对权重赋值： $\theta^{Q\prime} \leftarrow \theta^Q$，$\theta^{\mu\prime} \leftarrow \theta^{\mu}$。（公式中的导数符号如果看不清楚放大一点就可以了。。。） 初始化经验回放池 $R$。 接下来进行训练。外层循环是进行M个episode的训练，每个episode是智能体从行动开始到任务结束（或任务超时）的过程。为了进行有效探索，对确定性的动作 $\mu(s_t)$ 加上噪声（随机过程）$\mathcal{N}$。 初始化随机过程 $\mathcal{N}$。 获取初始状态值 $s_1$。 内层循环的次数是每个episode的时间长度 $T$： 根据确定性策略选取动作，并对动作添加噪声：$a_t = \mu(s_t|\theta^\mu)$。 执行动作 $a_t$，获取奖励 $r_t$ 和新的状态值 $s_{t+1}$。 将一个transition($s_t$, $a_t$, $r_t$, $s_{t+1}$)加入经验回放池。 对经验回报池进行采样，随机抽取 $N$ 个transitions构成一个mini-batch。 通过最小化Q值均方损失函数 $L$ 更新 $\theta^Q$，通过计算策略梯度 $\bigtriangledown_{\theta^\mu}$ 更新 $\theta^\mu$ 通过soft-update更新目标网络参数： \theta^{Q\prime} \leftarrow \tau\theta^Q + (1-\tau)\theta^{Q\prime}\theta^{\mu\prime} \leftarrow \tau\theta^\mu + (1-\tau)\theta^{\mu\prime} 2. 代码实现（基于pytorch）下面用代码实现简单的DDPG示例，用于gym中的小游戏Pendulum-v0（让摆锤倒立）。 2.1 构建网络构建描述动作-值函数的网络，采用三层全连接神经网络，输入是3维状态向量和1维动作值，输出是Q值。隐藏层采用relu激活函数，输出层不需要激活函数。代码如下：12345678910111213141516class QNet(nn.Module): def __init__(self): super(QNet, self).__init__() self.fc_s = nn.Linear(3, 64) self.fc_a = nn.Linear(1,64) self.fc_q = nn.Linear(128, 32) self.fc_3 = nn.Linear(32,1) def forward(self, x, a): h1 = F.relu(self.fc_s(x)) h2 = F.relu(self.fc_a(a)) cat = torch.cat([h1,h2], dim=1) q = F.relu(self.fc_q(cat)) q = self.fc_3(q) return q 构建描述动作策略的网络，采用三层全连接神经网络，输入是3维状态向量，输出是动作（连续的控制信号）。隐藏层采用relu激活函数，输出层采用tanh激活函数，动作值范围限定在[-1, 1]。代码如下：123456789101112class MuNet(nn.Module): def __init__(self): super(MuNet, self).__init__() self.fc1 = nn.Linear(3, 128) self.fc2 = nn.Linear(128, 64) self.fc_mu = nn.Linear(64, 1) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2] return mu 2.2 构建经验回放池经验回放池实际上是一个队列，当经验回放池满时，会抛弃旧的经验值，加入新采样的经验值。采样时，从经验回放池中随机抽取batch_size个经验值作为一个transition返回给训练机进行学习，代码如下：12345678910111213141516171819202122232425class ReplayBuffer(): def __init__(self): self.buffer = collections.deque(maxlen=buffer_limit) def put(self, transition): self.buffer.append(transition) def sample(self, n): mini_batch = random.sample(self.buffer, n) s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] for transition in mini_batch: s, a, r, s_prime, done_mask = transition s_lst.append(s) a_lst.append([a]) r_lst.append([r]) s_prime_lst.append(s_prime) done_mask_lst.append([done_mask]) return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \ torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \ torch.tensor(done_mask_lst) def size(self): return len(self.buffer) 2.3 构建Ornstein Uhlenbeck（OU）噪声OU过程是一种序贯相关过程，在DDPG中用于实现探索。OU过程满足下面的随机微分方程： dx_t = \theta(\mu - x(t)) dt + \sigma dW_t其中 $dW_t$ 是维纳过程（也称为布朗运动），满足： \triangle z = \epsilon \sqrt{\triangle t} ,\quad \epsilon \sim N(0, 1)$\triangle z$ 是变化量，$\epsilon$ 是标准正态分布。 OU过程的实现代码如下：1234567891011class OrnsteinUhlenbeckNoise: def __init__(self, mu): self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1 self.mu = mu self.x_prev = np.zeros_like(self.mu) def __call__(self): x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \ self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape) self.x_prev = x return x 2.4 构建训练模型采用adam优化器，对估计网络的参数 $\theta^Q$ 和 $\theta^\mu$ 进行mini-batch梯度下降优化。通过最小化均方损失函数,求 $\theta^Q$ 的梯度，利用adam优化器更新参数 $\theta^Q$。均方损失函数如下： L = \frac{1}{N} \sum_i(y_i - Q(s_i,a_i|\theta^Q))^2y_i = r_i + \gamma Q\prime(s_{t+1}, \mu\prime(s_{t+1}|\theta^{\mu\prime}) | \theta^{Q\prime})虽然实际上 $y_i$ 中也包含参数 $\theta^Q$，但是计算过程中通常忽略 $y_i$ 的梯度。 通过计算策略梯度，利用adam优化器更新参数 $\theta^\mu$ \bigtriangledown_{\theta^\mu} J \approx \frac{1}{N} \sum_i \bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_i, a=\mu(s_i)} \bigtriangledown_{\theta^\mu} \mu(s|\theta^\mu)|_{s=s_i}最后采用soft-update对目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，代码如下：123def soft_update(net, net_target): for param_target, param in zip(net_target.parameters(), net.parameters()): param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau) 训练模型的代码如下：12345678910111213def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer): s,a,r,s_prime,done_mask = memory.sample(batch_size) target = r + gamma * q_target(s_prime, mu_target(s_prime)) q_loss = F.smooth_l1_loss(q(s,a), target.detach()) q_optimizer.zero_grad() q_loss.backward() q_optimizer.step() mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss. mu_optimizer.zero_grad() mu_loss.backward() mu_optimizer.step() 2.5 运行DDPG估计网络的参数采用随机初始化，目标网络的参数复制估计网络的参数值。外层循环总共执行 $N$ 个episode，内层循环是每个episode的最大时间步长 $T$，超过 $T$ 之后重置状态，开启新的episode。每个episode结束之后，对估计网络和目标网络的参数进行训练和更新。需要注意的是，前期不会直接训练，直到经验回放池中的样本超过某个阈值才会开始训练策略。代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041def main(): env = gym.make('Pendulum-v0') memory = ReplayBuffer() q, q_target = QNet(), QNet() q_target.load_state_dict(q.state_dict()) mu, mu_target = MuNet(), MuNet() mu_target.load_state_dict(mu.state_dict()) score = 0.0 print_interval = 20 mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu) q_optimizer = optim.Adam(q.parameters(), lr=lr_q) ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1)) for n_epi in range(N): s = env.reset() for t in range(T): # maximum length of episode is 200 for Pendulum-v0 a = mu(torch.from_numpy(s).float()) a = a.item() + ou_noise()[0] s_prime, r, done, info = env.step([a]) memory.put((s,a,r/100.0,s_prime,done)) score +=r s = s_prime if done: break if memory.size()&gt;train_threshold: for i in range(update_epoch): train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer) soft_update(mu, mu_target) soft_update(q, q_target) if n_epi%print_interval==0 and n_epi!=0: print("# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;".format(n_epi, score/print_interval)) score = 0.0 env.close()]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG论文笔记]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 简述提出了基于 deterministic policy gradient 的 DDPG(deep deterministic policy gradient) 算法，能够运用在连续的动作空间中，能够 learn policy “end to end”。 1.1 处理连续动作空间的问题 DQN存在的问题是只能处理低维度，离散的动作空间。 不能直接把Q-learning用在连续的动作空间中。因为Q-learning需要在每一次迭代中寻找最优的$a_t$。对于参数空间很大并且不受约束的近似函数和动作空间，寻找最优的$a_t$是非常非常慢的。 连续动作空间的离散化：离散化后的动作数量随着自由度呈指数增长，导致离散后的动作空间太大，很难进行有效地探索。而且过于稀疏的离散化抛弃了动作空间的本身的结构信息。 1.2 处理庞大状态空间的问题 非线性函数逼近器（例如神经网络）的收敛性无法保证，但是这种形式的函数逼近器对于庞大的状态空间的学习和泛化而言是必要的。 1.2 DDPG 的关键点 model-free, off-policy, actor-critic, using deep function approximators based on the the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) 借鉴DQN两个重要技巧，对神经网络表示的value functions进行学习： the network is trained off-policy with samples from a replay buffer to minimize correlations between samples. the network is trained with a target Q network to give consistent targets during temporal difference backups. use batch normalization (Ioffe &amp; Szegedy, 2015) DDPG的优点在于它的简洁，只需要actor-critic框架和简单的学习算法，能够在实际的控制问题上发挥优势，比需要很多动力学建模的规划算法效果好。 2. 相关知识重点是目标函数，动作-值函数以及它的贝尔曼方程（迭代形式），Q-learning算法（即off-policy方法，行动策略和目标策略不是同一种策略）。 2.1 符号定义 t 时刻观测值： $\boldsymbol{x_t}$ 假定环境完全可知，t 时刻状态值： $\boldsymbol{s_t} = \boldsymbol{x_t}$ t 时刻动作： $\boldsymbol{a_t} \in \mathbb{R}^N $ 奖励值： $r_t$，标量 动作策略： $\pi : \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ 初始状态分布： $p(s_1)$ 状态转移概率： $p(s_{t+1} | s_t, a_t)$ 奖励函数： $r(s_t, a_t)$ 2.2 公式定义 折扣奖励函数： $R_t = \sum_{i=t}^T \gamma ^ {i - t} r(s_i, a_i)$, $\gamma \in [0, 1]$ 折扣状态访问分布： $\rho^\pi(s’) := \int_{\mathcal{S}} \sum_{t=1}^\infty \gamma^{t-1} p_1(s) p(s \rightarrow s’, t, \pi) ds$ 目标函数： $J(\pi_\theta) = \int_\mathcal{S} \rho^{\pi}(s) \int_{\mathcal{A}} \pi_\theta(s, a) R_1(s, a) da ds =\mathbb{E}_{s_i \sim \rho^\pi, a_i \sim \pi}[R_1]$ 动作-值函数： $Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{i \gt t} \sim \rho^\pi, a_{i \gt t} \sim \pi}[R_t | s_t, a_t]$ 动作-值函数的贝尔曼方程（迭代形式）： $Q^\pi(s_t, a_t)=\mathbb{E}_{s_{t+1} \sim \rho^\pi} \left[r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} [Q^\pi (s_{t+1}, a_{t+1})] \right]$ 如果动作策略是确定性策略，表示为 $\mu : \mathcal{S} \rightarrow \mathcal{A}$，则动作-值函数的贝尔曼方程的形式为： Q^\mu(s_{t}, a_{t}) = \mathbb{E}_{s_{t+1} \sim \rho^\mu} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu (s_{t+1}))\right ]&lt;/span&gt; 假设用参数 $\theta^Q$ 去逼近动作-值函数并采用Q-learning，则定义损失函数如下： L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta, a_t \sim \beta} \left[(Q(s_t, a_t | \theta^Q) - y_t)^2 \right]&lt;/span&gt; y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q)&lt;/span&gt;注意虽然$y_t$中也含有$\theta^Q$，但是对$L(\theta^Q)$求梯度时，通常忽略$y_t$，不对其求导。 3. 算法DDPG算法基于DPG算法，并采纳DQN中的两个重要技巧。接下来简单介绍一下基础算法DPG。 3.1 DPG算法DPG算法采用确定性策略 $\mu(s | \theta^\mu)$，每一时刻都将状态映射成确定的动作，同时也采用actor-critic框架。critic用参数为$\theta^Q$的函数 $Q(s,a|\theta^Q)$表示，并通过Q-learning和贝尔曼方程的方式进行学习。actor对目标函数应用链式法则（即策略梯度）更新参数 $\theta^\mu$，： \begin{aligned} \bigtriangledown_{\theta^\mu} J &\approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \bigtriangledown_{\theta^\mu}Q(s, a|\theta^Q) |_{s=s_t, a=\mu(s_t|\theta^\mu)} \right] \\ &= \mathbb{E}_{s_t \sim \rho^\beta} \left[\bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_t, a=\mu(s_t)} \bigtriangledown_{\theta^\mu}\mu(s|\theta^\mu)|_{s=s_t} \right] \end{aligned}Q-learning采用的是异策略，行动策略表示为 $\beta$，目标策略表示为 $\mu$。 上述目标函数中，最外层是关于状态 $s_t$ 分布的平均，$s_t$ 就是通过执行 $\beta$ 策略采样来的。但是最里层动作-值函数如果展开成贝尔曼方程的形式，下一时刻采用的动作策略则是 $\mu$，如2.2节的第六条公式。 DPQ 采用 mini-batch的梯度更新规则。对于比较大的网络，采用 batch learning 是很难收敛的。 3.2 DDPG算法绝大多数优化算法都是假设样本是独立且均匀分布的，但是强化学习采集的样本本身就是一个序列，所以相邻间的样本是有关联的，不满足假设条件。另外，通常是采用mini-batch更新的方式，而不是在线更新。 为了解决样本相互关联的问题，DQN采用了经验回放池，具有固定大小，存储每次探索的 $(s_t, a_t, r_t, s_{t+1})$。当经验回放池满了之后，会丢弃最旧的数据，增加新采样数据。每次更新的时候，actor和critic都会统一从经验回放池中抽取一个mini-batch进行更新。经验回放池可以尽可能地设置得大一点，这样每次抽取的样本关联度会降低。 如果直接按照2.2节的第七条公式去更新网络 $Q(s, a|\theta^Q)$，在大多数环境中都是会发散的。因为该公式的计算目标值 $y_t$ 时也用了网络 $Q(s, a|\theta^Q)$ 的值。很明显，这个网络在还没有收敛的时候，作为目标的一部分进行学习，结果很容易发散。因为目标本身就好像在追求一个不稳定的值。 为了避免目标Q值 $y_t$ 中包含不稳定的网络 $Q(s, a|\theta^Q)$ 输出，采用称为“soft target update” 的更新方法。首先，重新复制一份actor网络，$\mu’(s|\theta^{\mu’})$和critic网络，$Q’(s, a|\theta^{Q’})$，它们用来计算目标Q值。目标网络 $\mu’(s|\theta^{\mu’})$ 和 $Q’(s, a|\theta^{Q’})$ 随着学习网络 $\mu(s|\theta^{\mu})$ 和 $Q(s, a|\theta^Q)$ 更新而更新，但是目标网络的更新幅度远小于学习网络： $\theta’ \leftarrow \tau \theta + (1 - \tau) \theta$， $\tau \ll 1$。这意味着目标网络变化缓慢，提高了目标值计算的稳定性。（不过应该没有彻底解决这一问题，毕竟还是目标网络用于目标值计算时也并非稳定的，只是变化缓慢而已。）但是降低目标网络的更新幅度，可能会让学习网络的更新变慢，也就是牺牲更新速度来提供收敛稳定性。不过文中提到，在实际中，学习的稳定性比学习的速度要重要得多。 另一个问题就是观测值包含不同元素(例如位置信息和速度信息)，这些元素包含不一样的物理量度，变化值范围不同，如果采用这些不同量度的元素，可能会导致网络学习效率变低，也很难在不同环境中泛化。因此需要将所有这些元素重新归一化到相同的范围。采用的方法是Ioffe 和 Szegedy 提出的batch normalization。这种方法会将每个mini-batch中的样本每个维度归一到相同的均值和方差。同时保留这个均值和方差，以用于测试的样本的归一化。对 $\mu$ 网络和 Q 网络的所有隐藏层和状态输入都进行归一化。 为了提高探索效率，采用的行动策略 $\mu’$是对目标策略 $\mu(s_t|\theta_t^\mu)$加上噪声 $\mathcal{N}$：$\mu’(s_t) = \mu(s_t|\theta_t^\mu) + \mathcal{N}$。论文中采用的噪声是Ornstein-Unlenbeck process（O-U过程）。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用HEXO搭建个人博客(Ubuntu 18)]]></title>
    <url>%2F2019%2F09%2F05%2F%E4%BD%BF%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[参考官方文档https://hexo.io/zh-cn/docs 1.安装nvm1wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh 将以下语句添加到.bashrc文件：12export NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm 开始安装1nvm install stable 2.安装HEXO1npm install -g hexo-cli 3.创建blog文件夹1234hexo init blog #通过hexo创建一个blog项目cd blog npm installhexo server #开发服务 4.配置git地址在blog项目根目录下里找到_config.yml文件，找到Deployment，然后按照如下修改：1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master 5.安装 hexo-deployer-git自动部署发布工具1npm install hexo-deployer-git --save 需要在博客目录下安装 6.生成静态文件部署到githubhexo clean &amp;&amp; hexo g &amp;&amp; hexo d 7.打开博客地址：yourname.github.io部署到github后可能得等一会才会生效，少则一分钟，多则半小时。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
