<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TRPO & PPO 论文笔记（下）]]></title>
    <url>%2F2019%2F11%2F15%2FTRPO-PPO-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[二、 PPO：近端策略优化1. PPO 概述标准的策略梯度方法是在每个数据样本都进行一次梯度更新，PPO 方法可以进行 mini-batch 更新。比起 TRPO，PPO 继承了它部分优点，但是更容易实现，更通用，和更简单的采样方法。TRPO 方法使用了二阶近似，PPO 旨在于用一阶近似来达到 TRPO 类似的效果，同时提高数据的利用效率。通过交替利用策略采样和对采样的数据进行多个 epoch的优化，来提高数据的利用效率。 在连续的任务上，“概率比截断”版本的 PPO 方法表现得最好。在离散的动作空间任务上，PPO 方法和 ACER 方法效果类似，但更容易实现。 2. 相关知识2.1 策略梯度策略梯度算法中策略梯度的公式推导结果为： \hat{g}=\hat{\mathbb{E}}_t [\nabla_\theta \log \pi_\theta(a_t|s_t)\hat{A}_t] \tag{2.1}其中 $\pi_\theta$ 表示随机策略，$\hat{A}_t$ 是时刻 $t$ 对优势函数的估计值。$\hat{\mathbb{E}}_t$ 表示一个 batch 的样本进行经验估计。 以上的策略梯度对应的目标函数是： L^{PG}(\theta)=\hat{\mathbb{E}}_t [\log \pi_\theta(a_t|s_t)\hat{A}_t] \tag{2.2}虽然可以对以上的目标函数进行多次 epoch 优化，但是这样做实际上是经验主义上的，并且找不到充分的理由，而且还会对较大规模的梯度更新造成破坏。 2.2 TRPOTRPO 中的代理目标函数为： \mathop{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right] \tag{2.3} subject \,\, to \,\, \hat{\mathbb{E}}_t \left[ KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{2.4} \le \delta然后对目标函数进行进行一阶近似，约束进行二阶近似，然后结合共轭梯度进行求解。 实际上为 TRPO 提供保证的理论是采用惩罚项，而非约束项（这项理论应用在 Natural PG 中，TRPO 是对这项理论改造后的结果），也就是优化无约束问题，代理目标函数为： \mathop{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{2.5} \label{2.5}其中 $\beta$ 可以看作惩罚因子，是一个超参数。实际上这种代理目标函数背后的理论是优化策略 $\pi$ 的下界，让每次策略更新后的新策略都能得到提升。但是 $\beta$ 的取值非常难以确定，即使在很简单的问题上，都没办法简单地找到 $\beta$ 值，甚至在同一个问题中，$\beta$ 的取值随着学习过程而改变。这就导致 TRPO 采用了硬约束的方式，而不是惩罚项。 所以现在的问题就是，如果我们想在学习过程中找到性能单调递增的策略，光靠简单地选择一个惩罚项系数和 SGD 是不够的，但是使用 TRPO 又太复杂，需要涉及二阶近似，计算量太大。这个时候 PPO 就出现了。 3. 截断代理目标函数令 $r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$，很明显，$r(\theta_{old})=1$，TRPO 的代理目标可以重写成： L^{CPI}(\theta)=\hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right]= \hat{\mathbb{E}}_t \left[ r_t(\theta)\hat{A}_t \right] \tag{3.1} \label{3.1}上标 $CPI$ 表示 conservative policy iteration，是一篇著名文章中提出来的策略迭代方法 [1]，以上的目标函数就是这篇文章所提出来的。注意，此时目标函数没有加上约束，可能会导致很大的梯度更新，导致策略梯度失去意义。 现在考虑当 $r_t(\theta)$ 偏移 $1$ 的时候进行惩罚，引入截断代理目标函数。将等式 $\eqref{3.1}$ 改写为： L^{CLIP}(\theta)=\hat{\mathbb{E}}_t \left[ min(r_t(\theta)\hat{A_t}, clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t) \right] \tag{3.2}其中 $\epsilon$ 是一个超参数。可以发现 $min$ 函数中的第一项就是 $L^{CPI}$，第二项对 $r_t(\theta)$ 进行裁剪，超出 $[1-\epsilon,1+\epsilon]$ 的部分直接抹去，最后对两项取最小值，确保 $L^{CLIP}$ 取得结果是 $L^{CPI}$ 的下界。 注意，不是什么时候都将 $r_t$ 的值限制在 $[1-\epsilon,1+\epsilon]$，只有当目标函数获得提升时，我们将会对 r 进行限制；当目标函数变差时，我们不会对 r 进行处理。 至于 $r_t$ 什么时候被限制在 $1-\epsilon$，什么时候被限制在 $1 + \epsilon$，这取决于 $\hat{A}_t$的符号。 如果 $\hat{A}_t$ 大于零，则 $r_t$ 将限制在 $&lt; 1 + \epsilon$ 范围内，注意此时没有将 $r_t$ 限制在 $&gt; 1-\epsilon$ 的范围。 如果 $\hat{A}_t$ 小于零，则 $r_t$ 将限制在 $&gt; 1 - \epsilon$ 范围内，注意此时没有将 $r_t$ 限制在 $&lt; 1+\epsilon$ 的范围。 具体如图 3-1 所示。 图 3-1 对 r 裁剪的两种情况 图 3-2 的结果也说明了 $L^{CLIP}$ 取得结果是 $L^{CPI}$ 的下界。图中的横轴表示沿着策略梯度方向插值的参数 $\theta$，横坐标为 $1$ 时是初始参数，往左是沿着梯度下降，往右是沿着梯度上升。可以发现沿着梯度上升时， $L^{CLIP}$ 永远在 $L^{CPI}$ 的下方。 图 3-2 几种代理目标函数的取值 4. 自适应 KL 惩罚系数除了第 3 节所说的截断代理目标函数的方法，本文还提出利用一个对 KL 的自适应惩罚项系数来构建代理目标，将新旧策略的 KL 散度值限定在一个目标 KL 散度值 $d_{targ}$ 附近。文中说这种方法的效果不如截断代理目标函数的方法好，不过可以作为补充和 baseline。 实现过程如下： 首先利用 SGD 对带有惩罚项的代理目标函数（等式 $\eqref{2.5}$ ）进行几个 epochs 的优化： L^{KLPEN} = \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{4.1} \label{4.1} 计算当前新旧策略的 KL 散度值： $d = \hat{\mathbb{E}}_t KL \left[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right]$ 如果 $d &lt; d_{targ}/1.5$，$\beta \leftarrow \beta /2$ 如果 $ d &gt; d_{targ}*1.5 $ ，$ \beta \leftarrow \beta*2 $ 更新后的 $\beta$ 值将用于下一次更新。其中 $d_{targ}$ 是一个超参数，$1.5$ 和 $2$ 都是一个启发值，可以自己设定。文中说算法对这两个启发值不是很敏感。初始的 $\beta$ 也是一个超参数，但是不敏感，会随着算法持续自适应更新。 5. PPO 算法5.1 代理目标函数改进算法中涉及对优势值 $\hat{A}_t$ 的估计，为了降低优势估计值的方差，可以使用“广义优势估计”（GAE）方法[2]： \hat{A}_t^{GAE} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} \tag{5.1}当策略往前看 $T$ 步时（$T$ 小于 episode 的总数），上面公式变为： \hat{A}^{GAE}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \cdots +(\gamma \lambda)^{T-t+1} \delta_{T-1} \tag{5.2} \label{5.2} where \,\,\,\, \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \tag{5.3}当 $\lambda = 1$ 时，等式 $\eqref{5.2}$ 变为： \hat{A}^{(T)}_t = -V(S_t)+r_t+\gamma_{t+1}+\cdots+\gamma^{T-t+1}r_{T-1}+\gamma^{T-t}V(S_T) \tag{5.4}当 $\lambda = 0$ 时，等式 $\eqref{5.2}$ 变为 \hat{A}^{(1)}_t = -V(S_t)+r_t+\gamma V(S_{t+1}) \tag{5.5}在论文 [2] 中证明 $\hat{A}^{GAE}_t$ 实质上是 $\hat{A}^{(1)}_t,\hat{A}^{(2)}_t,\cdots,\hat{A}^{(k)}_t$ 的指数加权和。 如果策略网络和状态网络共用一套参数，代理目标函数还需要加上状态值函数的误差项。另外，为了鼓励智能体探索，可以给目标函数添加一个熵奖励项。最终，代理目标函数的形式为： L_t^{CLIP+VF+S}(\theta) = \hat{\mathbb{E}}_t[L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t)] \tag{5.6}其中 $c_1$，$c_2$ 是超参数，$S$ 表示熵奖励函数，$L_t^{VF}$ 表示状态值函数的误差项 $(V_\theta(s_t)-V_t^{targ})^2$ 。 但是实际上论文中的实验并没有加上 $L_t^{VF}$ 和 $L_t^{S}$ 这两项，说明它们未必在任何时候都能取得效果。 5.2 PPO 伪代码PPO 方法的伪代码如下： 首先用 $N$ 个智能体并行收集 $T$ 步的数据，构成大小为 $NT$ 的数据集，然后使用 minibatch SGD （或 Adam）方法优化代理目标函数。 6. 工程设置和实验比较网络结构和 TRPO 论文中的类似，两层全连接层，每层 64 个神经元，激活函数使用 tanh 非线性函数，输出为高斯分布策略的均值，标准差用另外一个独立的可变的参数来表示。另外，代理目标函数没有加上状态函数误差项和熵奖励项。 工程设置没有其他特别注意的点，下面是实验比较中采用的一些方法。 6.1 不同代理目标函数的比较 使用 7 个环境，3 个随机种子，结果取最后 100 个 episode 总奖励和。然后归一化，定义随机策略的结果为 0，最好的结果为 1 。最后跑 21 次 取平均。 超参数取多几个，然后进行比较： 6.2 连续动作空间的其他算法比较 6.3 两个有意思的评价标准(1) average reward per episode over entire training period 可以用来评价学习的速度。 (2) average reward per episode over last 100 episodes of training 可以用来评价最终学习的效果。 参考文献[1] S. Kakade and J. Langford. “Approximately optimal approximate reinforcement learn- ing”. In: ICML. Vol. 2. 2002, pp. 267–274. [2] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. “High-dimensional contin- uous control using generalized advantage estimation”. In: arXiv preprint arXiv:1506.02438 (2015).]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>PG</tag>
        <tag>PPO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TRPO & PPO 论文笔记（上）]]></title>
    <url>%2F2019%2F10%2F18%2FTRPO-PPO-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要涉及到策略梯度系列的两个算法，TRPO 和 PPO。TRPO 先提出来，PPO 实质上是对 TRPO 的改进。两篇论文的题目为 Trust Region Policy Optimization 和 Proximal Policy Optimization Algorithms 。 一、 TRPO：置信域策略优化1. TRPO简述本文提出了 Trust Region Policy Optimization (TRPO) 算法，主要是对 natural policy gradient 算法的改进，适用于大型的非线性策略函数，例如神经网络。传统方法中，基于策略梯度的模型有一个缺点就是采样效率太低，需要大量的样本才能让模型学习。 本文证明了可以通过最小化某个特定的目标函数，让策略每次都得到非平凡的提升。通过一系列的近似，将原本的目标函数改成实际的算法，称之为 TRPO 算法。 本文中给出两种具体的实现方式：single-path方法，vine方法。TRPO 算法可以同时优化非线性策略网络中成千上万的参数，这对于传统的策略梯度方法而言几乎是不可能的。 2. 相关知识定义随机策略 $\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$，定义 $\eta(\pi)$ 为期望折扣回报，公式如下： \eta(\pi) = \mathbb{E}_{s_0, a_0, \cdots} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t) \right] \tag{2.1} s_0 \sim \rho_0(s_0), \,\, a_t \sim \pi(a_t | s_t), \,\, s_{t+1} \sim \rho(s_{t+1} | s_t, a_t)动作值函数，价值函数和优势函数的定义： Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, \cdots} \left[ \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}) \right] \tag{2.2} V_\pi(s_{t}) = \mathbb{E}_{a_t, s_{t+1}, \cdots} \left[ \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}) \right] \tag{2.3} A_\pi(s, a) = Q(s, a) - V(s) \tag{2.4} a_t \sim \pi(a_t | s_t), \,\, s_{t+1} \sim \rho(s_{t+1} | s_t, a_t)假设另一个策略 $\tilde{\pi}$，可以通过证明得到策略 $\tilde{\pi}$ 比策略 $\pi$ 的累积优势： \eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0, a_0, \cdots \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] \tag{2.5} \label{2_5}上面的等式在论文的附录 A 中有证明。这是一个极其重要的等式，后面可以保证策略的优化是单调的，也就是不断朝着优势的方向优化。其中 $\mathbb{E}_{s_0, a_0, \cdots, \tilde{\pi}} [\dots]$ 表示动作服从策略 $\tilde{\pi}$，即 $a_t \sim \tilde{\pi}[\cdot | s_t]$。 令 $\rho_\pi$ 为折扣状态访问概率，写成如下公式： \rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) \cdots , \tag{2.6}\label{2_6}现在重写公式 $\eqref{2_5}$，将原本在时间 $t$ 维度上的叠加写成在状态 $s$ 维度上的叠加，结合公式 $\eqref{2_6}$，可以得到： \begin{aligned} \eta(\tilde{\pi}) &= \eta(\pi) + \sum_{t=0}^\infty \sum_s P(s_t = s | \tilde{\pi}) \sum_a \tilde{\pi}(a|s) \gamma^t A_\pi(s, a) \\ &= \eta(\pi) + \sum_s \sum_{t=0} \gamma^t P(s_t=s | \tilde{\pi}) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \\ &= \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \end{aligned} \tag{2.7} \label{2_7}等式 $\eqref{2_7}$ 意味着，只要保证在每个状态 $s$ 满足 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) \ge 0$，那么策略更新 $\tilde{\pi} \rightarrow \pi$ 就一定能保证提高策略性能 $\eta$ 或者至少保持不变（当 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) = 0$ 时）。 那么策略 $\tilde{\pi}$ 就可以按照贪婪算法来进行迭代更新，即$\tilde{\pi}(s) = argmax_a A_\pi(s, a)$，只要至少一个状态-动作对拥有正的优势值 $A_\pi(s, a)$，并且该状态的访问概率 $P(s)$ 不为零，策略 $\tilde{\pi}$ 的性能 $\eta(\tilde{\pi})$ 就能得到提升。但是由于预测误差和估计误差的存在，对于部分状态 $s$ 而言，可能会存在 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) &lt; 0$。 等式 $\eqref{2_7}$ 中有关于新策略 $\tilde{\pi}$ 的状态访问概率 $\rho_{\tilde{\pi}}(s)$，这在实际的计算中是非常难以得到的，很难计算得到每个状态在新策略下的访问概率，甚至在旧策略 $\pi(s)$ 下的访问概率都不是通过直接计算（通常是无模型的环境），而是通过很多次采样求平均得到，更何况新策略的具体分布都不知道，也没法进行采样。为了便于计算，本文假设在每一步微小更新后，新策略 $\tilde{\pi}$ 和策略 $\pi$ 的状态访问概率是一样的，于是得到等式 $\eqref{2_8_}$： L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \tag{2.8} \label{2_8_}假设 $\pi_\theta(a|s)$ 是对参数向量 $\theta$ 完全可导的，可以证明 $L(\tilde{\pi})$ 和 $\eta(\tilde{\pi})$ 一阶近似(证明过程参考文献 [1] 的 Theorem 1)。也就是对于任意参数 $\theta_0$，都存在： \begin{aligned} L_{\pi_{\theta_0}}(\pi_{\theta_0}) &= \eta(\pi_{\theta_0}) \\ \triangledown_\theta L_{\pi_{\theta_0}}(\pi_\theta) |_{\theta = \theta_0} &= \triangledown_\theta \eta(\pi_\theta) |_{\theta = \theta_0} \end{aligned} \tag{2.9} \label{2_9}等式 $\eqref{2_9}$ 意味着只要步长足够小，$\pi_{\theta_0} \rightarrow \tilde{\pi}$，就能提升 $L_{\pi_{\theta_{old}}}$的性能，当然也能提升 $\eta$ 的性能。但是这个步长究竟取多小，才是这篇文章研究的主题。 Kakade 和 Langford 等人在 2002 年提出了一种 conservative policy iteration 的方法（参考文献[2]），通过这种方法可以保证每次更新后策略的性能 $\eta$ 都有一个下界。 策略迭代的公式如下所示： \pi_{new}(a|s) = (1 - \alpha) \pi_{old}(a|s) + \alpha \pi'(a|s) \\ where \,\, \pi' = \mathop{argmin}_{\pi'} L_{\pi_{old}}(\pi') \tag{2.10} \label{2_10}Kakade 和 Langford 等人证明了依据这个策略迭代后，$\eta$ 有一个下界，如下所示： \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1 - \gamma(1-\alpha))(1-\gamma)}\alpha^2 \\ where \,\, \epsilon = \mathop{max}_s |E_{a \sim \pi'(a|s)}[A_\pi(s,a)]| \tag{2.11} \label{2_11}其中 $\alpha, \gamma \in [0, 1]$，上面的不等式也可以简单写成： \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2 \tag{2.12} \label{2_12}因为 $\alpha \ll 1$，所以不等式右边的减数只是变大一点点，也就是 $\eta$ 的下限变弱一点（变小）。 但是等式 $\eqref{2_10}$ 是一个混合的策略更新，也就是说既需要新策略，也需要旧策略来进行迭代，况且 $\pi’$ 的计算量也很大。所以本文旨在于找出适用于一般的随机策略的更新方式。 3. 单调提升的保证不等式 $\eqref{2_11}$ 意味着只要不等式右边的值在策略更新过程中得到提升，那么真实的性能 $\eta$ 也能得到提升。但是之前不等式 $\eqref{2_11}$ 的成立是针对一种混合策略迭代（conservative policy iteration），本文首次证明这个不等式对于一般的随机策略也是成立的，只要将 $\alpha$ 替换为衡量 $\pi$ 和 $\tilde{\pi}$ 的距离。一般的随机策略比混合策略更加适用于实际问题，因此这种延拓是很有必要的。 本文首先将 $\pi$ 和 $\tilde{\pi}$ 间的距离定义为总变差散度（Total variation divergence），定义为 $D_{TV}(p || q) = \frac{1}{2} \sum_i |p_i - q_i|$，这是离散型的概率分布 $p$ 和 $q$，对应的连续型概率分布则将求和改为积分。 根据 $D_{TV}(p || q)$ 的定义，定义 $D_{TV}^{max}$ 如下： D_{TV}^{max}(\pi, \tilde{\pi}) = \mathop{max}_s D_{TV}(\pi(\cdot|s) \parallel \tilde{\pi}(\cdot|s)) \tag{3.1}定理Ⅰ：令 $\alpha=D_{TV}^{max}(\pi, \tilde{\pi})$，可以证明下列不等式 $\eqref{3_2}$ 是成立的，证明见附录 A。 \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2 \\ where \,\, \epsilon = \mathop{max}_{s, \, a} |A_\pi(s,a)| \tag{3.2} \label{3_2}至于为什么可以令 $\alpha=D_{TV}^{max}(\pi, \tilde{\pi})$，附录 A 倒数第三段有讲，可以参考文献[3]。 根据 TV 散度和 KL 散度的关系：$D_{TV}(p || q)^2 \le D_{KL}(p || q)$，可以得到不等式$\eqref{3_3}$。（TV 散度和 KL 散度其实都是 F 散度的具体形式） \eta(\tilde{\pi}) \ge L_{\pi}(\tilde{\pi}) - CD_{KL}^{max}(\pi, \tilde{\pi}),\\ where \,\, C = \frac{4\epsilon\gamma}{(1-\gamma)^2} \tag{3.3} \label{3_3}根据不等式 $\eqref{3_3}$ 得到寻找 $\tilde{\pi}$ 的算法，如图所示。 上述的算法可以保证产生性能单调提升的策略序列，也就是 $\eta(\pi_0) \le \eta(\pi_1) \le \eta(\pi_2) \le \dots$ 证明如下： let \,\, M_i(\pi) = L_{\pi_i}(\pi) - CD^{max}_{KL}(\pi_i, \pi) \\ \eta(\pi_{i+1}) \ge M_i(\pi_{i+1}) \,\, by \,\, equation \,\, (3.3) \\ \eta(\pi_i) = M_i (\pi_i) \\ \eta(\pi_{i+1}) - \eta(\pi_i) \ge M_i(\pi_{i+1}) - M(\pi_i) \tag{3.4}在上述的算法中，需要假设每次估计的优势值 $A_\pi(\pi)$ 都是正确的。另外，KL 散度是作为一个惩罚项去更新策略 $\tilde{\pi}$，但是惩罚项系数 $C$ 计算复杂度高，且依赖于正确的估计优势值 $A_\pi(\pi)$。另外大的惩罚系数 $C$ 通常会导致学习步长太小，但是又很难选择一个小的惩罚系数。这就引出 TRPO 方法，通过添加 KL 散度约束而不是惩罚项系数 $C$ 来更新策略。 4. 参数化策略的优化（TRPO 登场）上一小节的理论推导没有涉及到策略的参数化，以及假设在每个状态策略的性能都能被准确的估计。现在开始推导更加实际的算法，基于随机初始参数化的策略和有限的样本情况下。 参数化策略 $\pi(a|s)$ 写成 $\pi_\theta(a|s)$，对上一节的符号进行简写： $\eta(\theta) := \eta(\pi_\theta)$，$L_\theta(\tilde{\theta}) := L_{\pi_\theta}(\pi_\tilde{\theta})$，$D_{KL}(\theta \parallel \tilde{\theta}) := D_{KL}(\pi_\theta \parallel \pi_{\tilde{\theta}})$。上一小节证明了 $\eta(\theta) \ge L_{\theta_{old}}(\theta) - CD^{max}_{KL}(\theta_{old}, \theta)$，当 $\theta = \theta_{old}$时等号成立。因此可以通过下面的最大化公式来更新 $\theta$，保证目标 $\eta$ 单调提升： \theta = \mathop{maximize}_\theta [L_{\theta_{old}}(\theta) - CD^{max}_{KL}(\theta_{old}, \theta)] \tag{4.1}但是实际计算中，如果采用惩罚系数 $C$，$\theta$ 更新的步长通常非常小，收敛速度非常慢。因此采用一种可以采用稍大步长的方式，在新旧策略的 KL 散度上添加约束，代替理论上采用的惩罚系数 $C$。这个约束就称为 trust region constraint（置信域），如公式 $\eqref{4_2}$ 所示。 \mathop{maximize}_\theta L_{\theta_{old}}(\theta) \\ subject \,\, to \,\, D^{max}_{KL}(\theta_{old},\theta) \le \delta \tag{4.2} \label{4_2}但是还存在另外一个问题：约束条件 $D^{max}_{KL}(\theta_{old},\theta) \le \delta$ 表示在状态空间的每一点都要满足约束（结合KL散度的定义来看），这对于状态空间较大的普遍情况是不切实际的。因此采用了一个技巧，将 $D^{max}_{KL}$ 用 $\overline{D}^{\rho}_{KL}$ 代替： \overline{D}^{\rho}_{KL}(\theta_1, \theta_2) := \mathbb{E}_{s \sim \rho}[D_{KL}(\pi_1(\cdot| s) \parallel \pi_2(\cdot | s))] \tag{4.3}$D^{max}_{KL}$ 和 $\overline{D}^{\rho}_{KL}$ 的区别就在于前者需要采集到状态空间的每个点，而后者只需要利用已经采集到的状态空间中的样本。因此公式 $\eqref{4_2}$ 可以写成以下形式： \mathop{maximize}_\theta L_{\theta_{old}}(\theta) \\ subject \,\, to \,\, \overline{D}^{\rho_{\theta_{old}}}_{KL}(\theta_{old},\theta) \le \delta \tag{4.4} \label{4_4}5. 算法模型展开公式 $\eqref{4_4}$ 中的 $L_{\theta_{old}}$，可以得到: \mathop{maximize}_\theta \sum_s \rho_{\theta_{old}}(s) \sum_a \pi_\theta(a|s) A_{\theta_{old}}(s, a) \\ subject \,\, to \,\, \overline{D}^{\rho_{\theta_{old}}}_{KL}(\theta_{old}, \theta) \le \delta \tag{5.1} \label{5_1}接下来采用三个小技巧将公式 $\eqref{5_1}$ 改写成可以利用蒙特卡洛方法进行逼近的形式： 将 $\sum_s \rho_{\theta_{old}}(s)[\cdots]$ 替换为 $\frac{1}{1-\lambda}\mathbb{E}_{s \sim \rho_{\theta_{old}}}[\cdots]$。参考公式 $\eqref{2_6}$，对 $\lambda^iP(s_i)$ 求和取平均即可得到。 将优势值 $A_{\theta_{old}}$ 替换为 $Q$ 值 $Q_{\theta_{old}}$。 利用重要性采样方法替换在动作上的累加，采样分布设为 $q$，可以得到： \sum_a \pi_\theta(a|s_n)A_{\theta_{old}}(s_n, a) = \mathbb{E}_{a \sim q} \left[\frac{\pi_\theta(a|s_n)}{q(a|s_n)} A_{\theta_{old}}(s_n, a) \right] 最终公式 $\eqref{5_1}$ 改写成以下形式： \mathop{maximize}_\theta \,\, \mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim q} \left[\frac{\pi_\theta(a|s)}{q(a|s)} A_{\theta_{old}}(s, a)\right] \\ subject \,\, to \,\, \mathbb{E}_{s \sim \rho_{\theta_{old}}} [D_{KL}(\pi_{\theta_{old}}(\cdot | s) \parallel \pi_\theta(\cdot | s))] \le \delta \tag{5.2} \label{5_2}直观上来看，在梯度上升法中，我们判断最陡峭的方向，之后朝着那个方向前进。但是如果学习率太高的话，这样的行动也许让我们远离真正的目标函数，甚至会变成灾难。 而在信赖域中，我们用 $\delta$ 变量限制我们的搜索区域。前面已经用数学证明过，这样的区域可以保证在它到达局部或者全局最优策略之前，它的优化策略会优于当前策略。 当我们不断迭代下取，就可以到达最优点。 剩下要做的就是将公式 $\eqref{5_2}$ 中的期望利用采样的平均来代替，以及用经验估计值代替 $Q$ 值。采样的模式有两种，一种称为 single path，另一种称为 vine。 5.1 Single Path这种模式下的采样，首先采集一系列初始状态 $s_0 \sim \rho_0$，然后从每个初始状态开始，按照策略 $\pi_{\theta_{old}}$（也就是令 $q = \pi_{\theta_{old}}$） 进行一段时间仿真，生成轨迹 $s_0, a_0, s_1, a_1, \cdots , s_{T-1}, a_{T-1}, s_T$。最后 $Q_{\theta_{old}}(s, a)$ 的计算是利用轨迹中每一对 $(s_t, a_t)$ 计算未来折扣回报，其中下标 $t= 0, 1, 2, \cdots, T-1, T$。 5.2 Vine首先也是采集一系列初始状态 $s_0 \sim \rho_0$，然后从每个初始状态开始，按照策略 $\pi_{\theta_i}$ 进行一段时间仿真，生成轨迹。紧接着，我们沿着这些轨迹，选择 N 个状态组成一个子集，称为 rollout set。对于每个 rollout set 中的状态 $s_n$，我们根据 $a_{n, k} \sim q(\cdot | s_n)$ 采样 $K$ 个动作。$q(\cdot | s_n)$ 可以取值为 $q(\cdot | s_n) = \pi_i(\cdot | s_n)$，也可以是均匀分布，论文中建议对于连续问题采用第一种，对于离散问题采用第二种。 对于每个 $s_n$ 和动作 $a_{n, k}$，通过以 $s_n$ 和动作 $a_{n, k}$ 为起点进行一段仿真，得到一小段轨迹，然后估计 $\hat{Q}_{\theta_i}(s_n, a_n, k)$ 的值。 因为 vine 模式在每个状态上多做了一些 rollout，可以得到方差更小的 Q 值估计。但是 vine 模式也有对应的缺点，需要调用更多次仿真器，并且对于真实的物理系统，还需要系统支持回退到任意一个过去的状态。两种模式的采样示意图如下所示： 5.3 算法流程 使用 single path 或者 vine 模式进行采样，采集一系列的状态——动作对，并且利用蒙特卡洛方法估计它们的Q值。 对这些样本求平均，建立公式 $\eqref{5_2}$ 中的目标函数和约束。 解决带有约束的优化问题，对参数 $\theta$ 进行更新。使用的是共轭梯度算法（conjugate gradient algorithm）和线性搜索（line search）。 5.4 解决带约束的优化问题最后需要求解公式 $\eqref{5_2}$ ，即求解： maximize \,\, L_{\theta_{old}}(\theta) \,\, subject \,\, to \,\, \overline{D}_{KL}(\theta_{old},\theta) \le \delta \tag{5.3}算法分成两个部分：1) 对目标函数进行一阶逼近，对约束条件进行二阶逼近，然后计算梯度方向。2) 在该方向上进行线性搜索，保证我们可以在非线性约束条件下提升非线性目标函数。 首先对目标函数进行一阶逼近，以及对约束条件进行二阶逼近 [4]，结果如下： 因此目标函数可以重写为： \begin{aligned} \theta_{k+1}=\mathop{argmax}_\theta[g^T(\theta-\theta_k)] \\ s.t. \,\, \frac{1}{2}(\theta-\theta_k)F(\theta-\theta_k) \le \delta \end{aligned} \tag{5.4}通过构造拉格朗日乘法，以及结合 KKT 条件，可以解出： \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TF^{-1}g}}F^{-1}g \tag{5.5} \label{5_5}问题来了，$F^{-1}$ 的计算复杂度很高。所以，论文中利用了共轭梯度直接计算 $F^{-1}g$，而不去求逆。 共轭梯度算法的伪代码如下： 共轭梯度法类似于梯度下降法，但是它可以在最多 $N$ 次迭代之中找到最优点，其中 $N$ 表示模型之中的参数数量。 根据共轭梯度算法，如果 $Q \in \mathbb{R}^{n\times n}$ 是一个正定矩阵，那么最小值 $x^*$ 就等于: Q x^*=b \,\,\,\, \Rightarrow \,\,\,\, x^*=Q^{-1}b$Q$ 是一个正定矩阵，恰好我们的 $F$ 也是一个正定矩阵。因此可以通过共轭梯度算法求出搜索方向 $s \approx A^{-1}g$ 。那么等式 $\eqref{5_5}$ 可以写成： \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{s^TFs}}s \tag{5.6} \label{5_6}其中学习率为： \beta=\sqrt{\frac{2\delta}{s^TFs}} \tag{5.7}此时使用线性搜索方法，如果该学习率下得到的新的策略比原有的策略的 loss 有改进，或者满足 $\overline{D}_{KL}(\theta_{old}, \theta) \le \delta$ ，则直接使用新策略网络替代原有网络，否则指数减少 $\beta$，直到达到理想的训练效果。 所以 TRPO 算法的完整流程为： 不过，在 CGA 算法中，我们需要计算 $Qd_k$，也就是 $Fg_k$，论文中给出了简化该矩阵-向量乘积的方法。 首先假设分布 $\pi_\theta(\cdot|x)$ 可以用参数向量 $\mu_\theta(x)$ 来描述（$\mu$ 是分布的均值），即 $\pi(\mu|x)$ ，那么两个策略的 $KL$ 散度可以重新写成： D_{KL}(\pi_{\theta_{old}}(\cdot|x) \parallel \pi_{\theta}(\cdot|x))=kl(\mu_{\theta}(x),\mu_{old}) \tag{5.8} \label{5_8}其中 $kl$ 是两个均值参数向量对应的分布的 $KL$ 散度。对等式 $\eqref{5_8}$ 进行二次求导，可以获得： \frac{\partial \mu_a(x)}{\partial \theta_i} \frac{\partial \mu_b(x)}{\partial \theta_j} k''_{ab}(\mu_\theta(x),\mu_{old})+\frac{\partial^2\mu_a(x)}{\partial\theta_i \partial\theta_j}kl'_a(\mu_\theta(x),\mu_{old}) \tag{5.9} \label{5_9}式子 $\eqref{5_9}$ 的第一项表示对 $kl(\mu_{\theta}(x),\mu_{old})$ 进行二次求偏导，其中有求和的操作。如果 $\mu$ 用神经网络表示，那么第二项明显为 $0$，所以只剩第一项。令 $J := \frac{\partial \mu_a(x)}{\partial \theta_i}$ ，第一项可以写为 $J^TMJ$，其中 $M$ 表示均值参数向量 $\mu$ 的分布的费舍尔信息矩阵。$J$ 的计算通过神经网络的反向传播算法可以轻易求解，而 $M$ 的计算依赖于 $\mu$ 描述的分布的具体形式。注意到此时求解费舍尔信息矩阵 $M$ 需要的不再是一组数据 $x$，而是它们的均值 $\mu$。 现在，计算 $Fg$ 就可以写成 $J^TMJg$。 不过共轭梯度算法中需要迭代 $N$ 次，$N$ 表示模型中参数的数量。每一次迭代都要重新计算费舍尔信息矩阵和一个向量的乘积。论文中运用一个技巧，就是只迭代 $k=10$ 次，而不是迭代 $k=N$ 次，因为他们发现就算 $k$ 继续迭代，效果也提升不了多少，反而大大增加计算复杂度。 另外，文中还利用了另一个减少计算复杂度的技巧：因为费舍尔信息矩阵仅仅是一个度量量度，只要数据量足够，算出的值与真实值偏差不大，因此文中只采用 $10 \%$ 的数据计算费舍尔信息矩阵，大量减少计算复杂度。 6. 工程上的设置6.1 机器人实验机器人控制实验总共设计了三种，分别是蛇形机器人、跳跳机器人和两足机器人。如下图所示。 比较感兴趣的是他们状态和奖励怎么定义，上面三种机器人状态空间都是广义位置和广义速度，各自奖励函数定义如下： 蛇形机器人的状态空间有 10 个维度，奖励函数定义为：$r(x,u)=v_x-10^{-5} |u|^2$ ，其中 $v_x$ 表示在前进方向的速度，$u$ 表示所有所有关节的功耗。 跳跳机器人的状态空间有 12 个维度，奖励函数定义为：$r(x,u)=v_x-10^{-5} |u|^2+nonterminal$，如果回合未结束，即机器人没有倒，则 $nonterminal=1$，否则为 0。相当于一个额外的奖励。 两足机器人的状态空间有 18 个维度，奖励函数与跳跳机器人相同，并且额外增加了足底压力的惩罚，压力越大，惩罚越大。这是为了让两足机器人有更加顺滑的行走，而不是变成跳跳机器人。 实验中设定 $KL$ 散度的阈值 $\delta=0.01$。其余参数的设置如下表所示： 6.2 Atari 游戏实验参数设置如下： 6.3 网络结构对于机器人的控制，状态空间和动作空间都是连续的，我们利用高斯分布对策略进行建模。神经网络采用全连接层，输入是状态空间，输出是高斯分布的期望值，网络参数为 $\{W_i,b_i\}^L_{i=1}$。另外还有一组单独的参数 $r$ 表示标准差的 $log$ 值，与神经网络的输出（均值）共同描述高斯分布。即策略最终可以描述为： \pi_\theta(a|s)=\mathcal{N}\left(mean=NeuralNet(s;\{W_i,b_i\}^L_{i=1}),stdev=exp(r)\right)对于离散的动作空间，神经网络的输入是状态空间，对输出采用 $softmax$ 函数，取概率最大的动作。 7. 总结近年来，学界开始将优化方法中的信赖域（Trust region）方法引入增强学习，并在各种实验场景中取得了良好的效果。其中典型的有 TPRO 和受其影响衍生出的一系列前沿算法，如 PPO，Trust-PCL，ACER等。其中 PPO 已成为 OpenAI 的默认增强学习算法。 信赖域系增强学习方法，顾名思义，来源于优化论中的信赖域方法和机器学习中增强学习的结合。 先介绍一下信赖域方法。在 Jorge Nocedal 和 Stephen J. Wright 的《Numerical Optimization》一书的第 2.2 节介绍了解优化问题的两种策略：Line search 和 Trust region。本质上它们的作用都是在优化迭代过程中从当前点找寻下一点。它们的最大区别是先确定步长还是先确定方向。Line search 方法先确定方向再确定步长。而 Trust region 方法则先把搜索范围缩小到一个小的范围，小到能够用另一个函数（Model function）去近似目标函数（Objective function），然后通过优化这个 Model function 来得到参数更新的方向及步长。在该书的第三和第四章分别着力介绍了 Line search 和Trust region方法。 策略梯度方法是强化学习中非常重要而且历史悠久的一类方法。下面简单介绍一下策略梯度方法的重要历史节点。早在 1992 年 Ronald J. Williams 提出的 REINFORCE 算法就是 PG 的雏形。它的基本思想是考虑由参数 $\theta$ 控制的随机策略 $\pi(\theta)$，然后通过优化与策略相关的目标函数 $J(\theta)$（比如累积折扣回报和）来更新策略的参数。 之后， 行动者-评论家（Actor-Critic, AC）方法提出以解决原始策略梯度方法中方差较大，样本利用率低的问题。1998 年， Amari 提出《Natural Gradient Works Efficiently in Learning》， 通过自然梯度（Natural gradient）代替标准梯度（Standard gradient）来解决梯度算法面临平坦区域过早收敛或收敛速度过慢的问题，直观上就是，它使得参数在相对于参数不太敏感的地方，步长大；而在敏感的地方，步长小。此后，Kakade 在 2001 年的论文《Natural Policy Gradient》将自然梯度引入增强学习中的 PG 方法。策略参数的更新方向就变为自然梯度。Peters 等人在 2005 年的论文《Natural Actor-Critic》中讨论了它与 AC 框架的结合。之后在论文《Reinforcement Learning of Motor Skills with Policy Gradients》中对这些工作有些总结。 8. 参考文献[1] Sutton, R. S., McAllester, D., Singh, S., &amp; Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, 1057–1063. [2] Kakade, S., &amp; Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. 1Proceedings of the 19th International Conference on Machine Learning, 267–274. [3] Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and mixing times. American Mathematical Society, 2009. [4] Yi Da, Xu, Policy Gradient mathematics. 2019]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>PG</tag>
        <tag>TRPO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN代码实现]]></title>
    <url>%2F2019%2F10%2F16%2FDQN%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[DQN强化学习方法系列主要是由两篇文章提出，分别是 Playing Atari with Deep Reinforcement Learning 和 Human-level control through deep reinforcement learning。这两篇文章讲述的具体方法在之前的博客 DQN相关论文笔记中有过介绍，在这篇文章中分析DQN强化学习方法的代码实现细节。 1. 算法为代码 构建模型。确定在线 Q 网络（也称为估计 Q 网络）的层数和隐藏层神经元个数，并随机初始化权重。目标 Q 网络的架构与在线 Q 网络一致，权重初始化为在线 Q 网络的权重。 初始化经验回放池 $\mathcal{D}$。 进行训练。外循环为 M 个 episode 的训练。每个 episode 是智能体从行动开始到任务结束（或）任务超时的过程。 内循环为 T 个时间步长： 利用 $\epsilon$ -贪婪算法选择随机动作 $a_t$。 在仿真器中执行动作 $a_t$ 获得奖励 $r_t$ 和新的状态 $s_{t+1}$。 将一个 transition $(s_t, a_t, r_t, s_{t+1})$ 存入经验回放池。 对经验回报池进行采样，随机抽取 N 个 transition 构成一个 Mini-batch。 通过梯度下降最小化均方损失函数 $(y_j - Q(s_j, a_j; \theta))^2$ 更新参数 $\theta$。 每隔 $C$ 个时间步长将在线 Q 网络的权重 $\theta$ 赋值给目标 Q 网络的权重 $\theta^-$ 2. 代码实现（基于pytorch）下面用代码实现简单的 DQN 示例， 环境为 gym 中的小游戏 Cartpole-v0（让小车上的木棍保持竖立）。 2.1 构建网络构建 Q 值网络，采用两层全连接网络。输入是 4 维状态向量，输出是 2 个动作对应的 Q 值。策略采用 $\epsilon-$ 贪婪策略，有 $\epsilon$ 的概率选择随机动作，$1-\epsilon$的概率选择贪婪动作。123456789101112131415161718class Qnet(nn.Module): def __init__(self): super(Qnet, self).__init__() self.fc1 = nn.Linear(4, 256) self.fc2 = nn.Linear(256, 2) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x def sample_action(self, obs, epsilon): out = self.forward(obs) coin = random.random() if coin &lt; epsilon: return random.randint(0, 1) else: return out.argmax().item() 2.2 构建经验回放池经验回放池实际上是一个队列，当经验回放池满时，会抛弃旧的经验值，加入新采样的经验值。采样时，从经验回放池中随机抽取batch_size个经验值作为一个transition返回给训练机进行学习，代码如下：123456789101112131415161718192021222324252627class ReplayBuffer(): def __init__(self): self.buffer = collections.deque(maxlen=buffer_limit) def put(self, transition): self.buffer.append(transition) def sample(self, n): mini_batch = random.sample(self.buffer, n) s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] for transition in mini_batch: s, a, r, s_prime, done_mask = transition s_lst.append(s) a_lst.append([a]) r_lst.append([r]) s_prime_lst.append(s_prime) done_mask_lst.append([done_mask]) return torch.tensor(s_lst, dtype=torch.float), \ torch.tensor(a_lst), \ torch.tensor(r_lst, dtype=torch.float), \ torch.tensor(s_prime_lst, dtype=torch.float), \ torch.tensor(done_mask_lst) def size(self): return len(self.buffer) 2.3 构建训练模型估计 Q 网络的参数采用随机初始化，目标 Q 网络的参数复制估计 Q 网络的参数。$\epsilon$会随着 episode 的增长而降低，也就是后面的策略越来越靠近贪婪策略。外层循环是 $N$ 个 episode，内层循环是 $T$ 个时间步长。每个时间步长进行动作采样，仿真执行，存储经验这三个步骤。 注意伪代码中每个时间步长都更新一次，这里是每个 episode 更新一次，视具体情况而定，如果每个 episode 包含的时间步长很多，则选择每个时间步长更新一次，也可以是每隔几个时间步长更新一次。 另外还需要注意的是前期不会直接进行更新，而是等到经验回放池中的样本数量超过某个阈值才会开始训练。 最后就是每隔 $C$ 个 episode 将估计 Q 网络的参数复制给在线 Q 网络。1234567891011121314151617181920212223242526272829303132333435363738394041424344def train(): env = gym.make('CartPole-v1') q = Qnet() q_target = Qnet() q_target.load_state_dict(q.state_dict()) memory = ReplayBuffer() optimizer = optim.Adam(q.parameters(), lr=learning_rate) print_interval = 20 score = 0.0 for n_epi in range(N_episode): # epsilon = max(min_epsilon, max_epsilon - 0.01 * (n_epi / 200)) epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (n_epi / N_episode)) s = env.reset() for t in range(T): a = q.sample_action(torch.from_numpy(s).float(), epsilon) s_prime, r, done, info = env.step(a) done_mask = 0.0 if done else 1.0 memory.put((s,a,r,s_prime,done_mask)) s = s_prime score += r if done: break if memory.size() &gt; 2000: update(q, q_target, memory, optimizer) if n_epi%copy_time == 0: q_target.load_state_dict(q.state_dict()) if n_epi%print_interval==0 and n_epi!=0: print("# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, epsilon : &#123;:.1f&#125;%".format( n_epi, score/print_interval, memory.size(), epsilon*100)) score = 0.0 env.close() torch.save( &#123; 'q_state_dict': q.state_dict(), 'q_target_state_dict': q_target.state_dict(), 'optim_state_dict': optimizer.state_dict() &#125;, SAVE_PATH) 2.4 梯度下降更新参数采用adam优化器，对估计网络的参数 $\theta^Q$ 和 $\theta^\mu$ 进行mini-batch梯度下降优化。通过最小化均方损失函数,求 $\theta^Q$ 的梯度，利用adam优化器更新参数 $\theta^Q$。均方损失函数如下： L = \frac{1}{N} \sum_i((r + Q(s',a'|\theta_i^-)) - Q(s,a|\theta_i))^2其中 $\theta_i^-$ 是经过 $C$ 个步长从估计 Q 网络中复制而来，在上面的构建训练模型中有体现。 更新过程是从经验回放池中抽取 batch_size 个训练样本，利用梯度下降法进行反向传播更新参数。12345678910111213def update(q, q_target, memory, optimizer): for i in range(update_epoch): s, a, r, s_prime, done_mask = memory.sample(batch_size) q_out = q(s) q_a = q_out.gather(1,a) max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) target = r + gamma * max_q_prime * done_mask loss = F.smooth_l1_loss(q_a, target) optimizer.zero_grad() loss.backward() optimizer.step() 2.6 运行DDPG训练好模型之后，将网络的参数保存，然后在评测的时候重新载入模型参数。采用的策略也算是 $\epsilon-$ 贪婪策略，不过 $\epsilon$ 值采用最小值。代码如下所示：12345678910111213141516def evaluate(): q = Qnet() checkpoint = torch.load(SAVE_PATH) q.load_state_dict(checkpoint['q_state_dict']) q.eval() env = gym.make('CartPole-v1') while(1): s = env.reset() done = False while not done: a = q.sample_action(torch.from_numpy(s).float(), min_epsilon) s_prime, r, done, info = env.step(a) env.render() s = s_prime env.close() 2.7 超参数的设置超参数的设置很重要，但没有特定的方法指导，所以全凭经验。简单总结一些经验： 确定学习步长一般先确定它的数量级，比如学习步长的数量级为 $10^{-4}$，然后再对系数进行微调。在不影响后期收敛速度的情况下，学习步长选择低一点的。 batch_size 的大小对学习的稳定性有影响。如果 batch_size 太小，模型可能一开始取得较优值，后面训练的性能却开始下降，这是因为模型过拟合。所以 batch_size 不能太小。但是也不能太大，否则收敛速度过慢，训练过程太久。 copy_time 太大模型会陷入过拟合，太小则模型收敛速度过慢。 123456789101112learning_rate = 0.0002 #0.0001N_episode = 10000max_epsilon = 0.08min_epsilon = 0.001 #0.01T = 600train_threshold = 2000update_epoch = 10copy_time = 50SAVE_PATH = 'model/dqn.pt'batch_size = 64 #32buffer_limit = 50000gamma = 0.99]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先经验回放]]></title>
    <url>%2F2019%2F09%2F21%2F%E4%BC%98%E5%85%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%2F</url>
    <content type="text"><![CDATA[论文题目：Prioritized Experience Replay 1. 论文简述在 Nature DQN 和 Double DQN 论文中经验回放池的采样是基于均匀分布采样，一种更合理的方式应该考虑这些经验中哪些更具有对训练更有价值，也就是给这些经验值分配不同的优先级权重，在采样时这些重要的经验被抽取的概率对更大。DQN 论文中提及很早之前有研究做过一种 “Prioritized sweeping” 方法，就是实现经验回放的不均匀采样。本篇论文在前人的研究基础上提出一种新的框架——优先经验回放，使优先级更大的经验被选中的几率更大。“DQN + 优先经验回放”的方法在 Atari 游戏的测试中比 “DQN + 均匀经验回放”的方法更好（49个游戏有41个性能更优越）。 1.1 优先经验回放简述在线强化学习利用每次获得的经验流 $(s_t, a_t, r_t, s_{t+1})$ 来更新参数。最简单的更新方式就是每次获得一个经验流，在进行一次更新后就抛弃。这样做有两个问题： 由于状态之间存在关联，因此用来更新的经验值并非是独立的，这破坏了很多算法模型的独立性假设。 有些经验是十分罕见和宝贵的，有可能下一次学习还需要继续用到，因此直接丢弃是不理智的。 采用经验回放的方法可以减少需要用于训练的经验，加快训练，同时牺牲一些计算资源和存储资源来减少智能体和环境的交互。对于强化学习智能体而言，计算资源、存储资源和环境交互的次数相比，是一种 cheaper 资源。 优先经验回放背后的关键思想是有些经验在用于智能体的学习时会比另外一些经验更有意义。另外，有些经验可能在当前对于智能体的学习并不是很有帮助，但是在智能体能力提升之后，可能对智能体的学习会增加。 优先经验回放用 TD 误差的大小来衡量哪些经验对学习过程具有更大的贡献。但是使用优先经验回放会带来一点多样性的损失和引入偏差。本文引入随机优先 (stochastic prioritization) 来缓解多样性损失的问题，并通过重要性采样来纠正偏差。 优先经验回放的可以对每一个 transition（RL 中交互的原子单位，以下称为“经验”）计算优先级指标，也可以对一个经验序列计算优先级指标。 1.2 相关知识1993 年 Moore 和 Atkeson 等人就提出了 “prioritized sweeping” 的概念，根据状态更新后的变化值来对每个状态进行排序，优先选出更新后最优的状态。 但是这种方法只适用于有模型的强化学习问题。本文提出的优先经验回放是用在无模型的强化学习问题上，另外本文还新采用了两个技巧——随机优先方法 (stochastic prioritization) 和 重要性采样。 TD 误差其实提供了一种更新后的变化值的描述。在Q网络中，TD 误差就是目标 Q 网络计算的目标 Q 值和在线 Q 网络计算的 Q 值之间的差距。注意到在经验回放池里面的不同的样本由于 TD 误差的不同，对我们反向传播的作用是不一样的。TD 误差越大，那么对我们反向传播的作用越大。而TD 误差小的样本，由于TD 误差小，对反向梯度的计算影响不大。 2. 算法模型2.1 衡量经验优先级的标准优先经验回放最重要的部分是衡量每个经验优先级的标准。本文提出的第一种方法就是使用 TD 误差 来衡量经验的优先级。 TD 误差是指下一次更新后值的变化，如果 TD 误差大，说明用来更新的经验值具备更多的信息，因此优先级也更高。不过这种方法在一些环境下并不适用，那就是当奖励值是带有噪声的情况下。论文后面也讨论了其他衡量经验优先级的标准，目前先假定这种标准就是 TD 误差。 算法会将上一次计算的 TD 误差连同用于更新的经验共同存入经验回放池。如果是新的经验，并不知道其对应的 TD 误差，算法会给予这种经验最高的优先级，保证所有的经验都至少被回放一遍。 2.2 随机优先方法 (Stochastic prioritization)每次都抽取 TD 误差最大的那个经验的经验回放方法称之为贪婪经验回放方法 (greedy TD-error prioritization)。这种方法有以下三个问题： 一般为了减少遍历经验回放池的巨大时间代价，每次只会对被抽取的经验更新它的 TD 误差值。也就是说如果一个经验一开始它的 TD 误差值很小，那么可能很长时间内都不会回放这个经验。 对噪声脉冲十分敏感（也就是比较随机的奖励值），逼近目标函数的误差也会加重这种算法的不稳定性。 贪婪经验回放会只专注于一小部分初始 TD 误差值比较高的经验，导致缺乏多样性，系统会过拟合。 为了解决贪婪经验回放存在的若干问题，本文引入了随机优先经验回放的方法，是均匀经验回放和贪婪经验回放两种方法的折衷。随机优先经验回放让经验池中每个经验被抽中的概率与它们的 TD 误差值呈现单调关系，但是也保证对于低优先级的经验同样会有非零的概率被抽中。对于经验 $i$，被抽中的概率表示如下： P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}其中 $p_i &gt; 0$ 表示经验 $i$ 的优先级，指数 $\alpha$ 表示依赖优先级进行经验回放的程度，如果 $\alpha=0$，表示均匀经验回放。对于 $p_i$ 的定义，本文给出两种方法（实际应用大部分都使用第一种定义）： $p_i = |\delta_i| + \epsilon$，其中 $\epsilon$ 是一个正的常数，防止经验 $i$ 的 初始TD 误差值为0。 $p_i = \frac{1}{rank(i)}$，其中 $rank(i)$ 是根据 $|\delta_i|$ 的排名。 为了提高采样效率，采样的时间复杂度不能和经验回放池的大小 $N$ 成正比。对于第一种方法，采用基于 “sum-tree” 数据结构实现，采样和更新的时间复杂度是 $O(log N)$。对于第二种方法，采用二进制堆构建的优先队列，采样的时间复杂度为 $O(1)$，更新的时间复杂度为 $O(log N)$。 采样时，采用 $k$ 段概率相等的分段线性函数来近似经验的累积密度函数，采样时先根据概率抽取一段经验序列，再从一段经验中均匀随机抽取一个经验。如果是采用mini-batch的梯度优化方法，可以将 minibatch 的大小设置为 $k$，然后从每段经验序列中都抽取一个经验。 2.3 偏差补偿（偏差纠正）引入随机优先级概念后，仍然会存在问题。注意到，如果是通过正常的经验重放，则使用随机更新规则。因此，对经验进行抽样的方式必须与它们的原始分布相匹配。如果采用的是均匀经验回放，那么采样的方法也相应是随机采样，这样每个经验都会有同等的概率被抽到，因而不会引入偏差。 但是如果采用了优先经验回放，就需要采用优先级采样而抛弃随机采样，这样就会向高优先级的样本引入偏差（即更高概率被抽中）。这种情况下，更新模型权重会有过拟合的风险。与低优先级经验相比，具有高优先级的经验样本可能多次用于训练。因此，模型只会使用一小部分经验更新权重。 为了纠正这种偏差，可以使用重要性采样 (importance-sampling)，通过减少常见样本的权重来调整更新模型。 来纠正这种偏差： \omega_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta注意公式中 $P(i)$ 才是经验 $i$ 被抽取的概率，$\frac{1}{P(i)}$ 是它的倒数。$\beta$ 的作用是控制这些重要性采样权重对学习的影响程度。在实际运用中，$\beta$ 参数在训练过程中会逐步上升到 1。随着 $\beta$ 的提高，上述公式对高优先级的样本的权重几乎不更新，而对低优先级的样本的权重进行较大的提升。因为当后期动作-值 Q 开始收敛时，无偏性的更新对误差收敛是至关重要的。 采用重要性采样还可以限制梯度的大小，这对于深度网络的更新是十分有利的。深度网络的更新步长一般不能设置太大，而采用优先经验回放进行更新时，会明显增加高 TD 误差的经验用于网络更新的几率，这样会使深度网络的更新不稳定。采用重要性采样后，网络更新的梯度会受到限制。为了提高算法模型训练的稳定性，通常让 $\omega$ 除以 $1 / max_i \, \omega_i$ 进行标准化，保证梯度更新可以受到限制。 所以最终重要性采样的权重计算公式如下： \omega_j = \left( N \cdot P(j) \right)^{-\beta} / max_i \, \omega_i2.4 PER 伪代码综合“衡量经验优先级的标准”，“随机优先方法”和“重要性采样补偿偏差”，得到优先经验回放的算法伪代码如下： 注意伪代码中没有写出随机优先回放的技巧，不过具体实现中是要用到的。另外也没有说明当经验回放池满了之后怎么执行替换操作。有两种实现方式，一种是把优先级最低的经验给替换掉，另一种是轮流替换到每个位置。 3. 实现细节3.1 PER 具体实现相关细节我们不能只根据优先级对所有经验回放样本进行排序来实现优先经验回放。这样做对经验样本插入的时间复杂度为 $O(nlogn)$，采样过程的时间复杂度为 $O(n)$，因此这个效率并不高。需要另外引入一些数据结构来减小时间和空间复杂度。 上面 2.2 节提到对于经验优先级 $p(i)$ 的定义有两种方式，一种称为“排名优先级” (Rank-based prioritization)，另外一种称为“比例优先级” (Proportional prioritization)。下面对这两种定义给出具体的实现细节。（其实论文对于这部分的介绍比较笼统，建议直接看代码） 3.1.1 Rank-based prioritization 实现细节采用基于数组的二叉堆实现的优先队列来存储经验。运行时间上的改进来自于避免对采样分布的分区进行过多的重新计算。（这里论文并没有介绍很多） 3.1.2 Proportional prioritization 实现细节在这里使用的是 “sum-tree” 数据结构，它是二叉树，每个节点最多只有两个子节点。 每片树叶存储每个样本的优先级, 每个树枝节点只有两个分叉, 节点的值是两个分叉的和，那么根节点的值就是所有优先级的总和 $p_{total}$。这种数据结构给优先级的累计和的计算带来便利，插入（更新树）和采样的时间复杂度降为 $O(log N)$。 3.2 采样细节假设需要从经验池中抽取 $k$ 个经验（$minibatch = k$），首先将累积优先级范围 $[0, p_{total}]$ 等划分为 $k$ 个序列，然后在每个序列中进行均匀随机采样，最后将对应的经验从数据结构中剥离出来。 3.3 $SumTree$ 实现细节优先经验回放池的数据结构分为三块内容：树的节点索引、节点数据、以及一个单独存放经验的结构。 $SumTree$ 是一种树形结构, 每个叶子存储每个样本的优先级。每个父节点只有两个分支, 父节点的值是两个分支的和, 所以 $SumTree$ 的顶端就是所有优先级的和，如下图所示。 可以发现，叶结点的个数等于之前所有层的节点加起来再加1，设叶结点个数为 $N$，则整棵树的大小为 $2 * N - 1$。 另外还有一个数组（称为 $Data$ 结构）存储所有经验，相当于经验池。$Data$ 结构如下图所示： 注意 $SumTree$ 树和 $Data$ 数组存储的东西不一样，前者存储的是优先级，后者存储的是经验($transition$)。存储优先级的时候是从 $SumTree$ 的叶子节点开始的，其索引是从 $N - 1$ 开始。而这个优先级对应的经验在 $Data$ 数组中的存储是从 $0$ 开始的，可以看出优先值和对应的经验的索引差为 $N - 1$。 从经验池抽样时, 我们会将优先级的总和除以 $batchsize$（设为 $k$）, 分成 $k$ 那么多区间，每个区间的优先级变化范围是 n = \frac{sum(p)}{k}假设如图将所有节点的优先级加起来是 $42$ 的话, 我们需要抽 $6$ 个样本, 这时的区间拥有的优先级是这样： $[0-7]$，$[7-14]$，$[14-21]$，$[21-28]$, $[28-35]$, $[35-42]$ 然后在每个区间里随机选取一个数 $s$，从根节点开始比较，即 $idx=0$，如果左边的子节点比 $s$ 大，则走左边子节点这条，如果左边子节点小于 $s$，则走右子节点，但 $s$ 值要减去左子节点的数值，按照这个规则，一直找到叶结点，返回其索引，以及对应的优先级，还有从 对应的经验。 比如在区间 $[21-28]$ 里选到了 $24$, 就按照这个 $24$ 从最顶上的根节点开始向下比较. 首先看到根节点下面有两个子节点, 左边的子节点 $29$ 比 $24$ 大,所以走左边那条路。接着再对比 $29$ 下面的左边那个点 $13$, 这时 $13$ 比选中的 $24$ 小, 那我们就走右边的路, 并且将手中的值减去 $13$, 变成 $24-13=11$。接着拿着 $11$ 和 $16$ 左下角的 $12$ 比, 结果 $12$ 比 $11$ 大, 那我们就选 $12$ 当做这次选到的优先级，并且可以知道 $12$ 这个节点在树中的索引为 $9$，并且叶子节点的总数 $N = 8$，所以对应的经验在 $Data$ 经验池的索引为 $9 - (N - 1) = 2$。因此从 $Data$ 经验池中顺序取出第三个经验。 从上面 $SumTree$ 的结构图中我们可以注意到，第三个叶子节点优先级最高，它覆盖的采样区间为 $13-25$，也是最长的，因此会比其他节点更容易被采样到。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>经验回放池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN相关论文笔记（下）]]></title>
    <url>%2F2019%2F09%2F16%2FDQN%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要提及下面四篇关于DQN的著名论文的后两篇： [1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. 1–9. Retrieved from http://arxiv.org/abs/1312.5602 [2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 [3] Van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep reinforcement learning with double Q-Learning. 30th AAAI Conference on Artificial Intelligence, AAAI 2016, 2094–2100. [4] Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., &amp; De Frcitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. 33rd International Conference on Machine Learning, ICML 2016, 4(9), 2939–2947. DQN（Deep Q-Learning）算是 DRL 的开山之作，算是采用了 Value function approximation 的 critic-only 类算法，实现了从感知到动作的端对端学习法，由 DeepMind 在 NIPS 2013 上提出[1]，后在 Nature 2015 上提出改进版本[2]。Double-DQN[3] 和 Dualing-DQN[4] 都是 DQN 的改进版本，前者对训练算法进行了改进，后者对模型结构进行了改进。 Q-learning 存在对 Q 值估计偏高的问题，可能会导致非最优解和学习过程稳定性下降。DQN 是基于 Q-learning 的模型，所以本质上也有这个问题。 一、 Double DQN：解决 Q 值估计偏高 [3]1. Double DQN 简述1.1 Q-learning 存在 Q 值估计偏高问题直觉上，Q-learning 算法中估算目标 Q 值时采用的目标策略是最大化的贪婪策略，因此对目标策略的 Q 值估计值往往会偏高，最终导致确定状态下输出的动作的估计 Q 值也会偏高。另外环境噪声等因素也会让动作的估计 Q 值偏高。总而言之，只要学习过程出现动作-值不准确，无论误差来源于什么地方，动作-值都会被高估。这是 Q-learning 算法中普遍遇到的问题，因此有必要解决这一问题，提高算法的稳定性。 那么是否 Q 值估计偏高一定会给算法带来问题呢？不一定。理想状态下，如果所有的动作-值都被统一地估计偏高相同的程度，对于选取动作是没有影响的，因此算法的性能也不会受到影响。但是，这种估计偏高的程度并不是统一地出现在每个动作-值的估计上，有些动作-值估计偏高的程度大，而有些动作-值估计偏高的程度小，如果非最优动作的动作-值被偏高估计超过最优动作的估计动作-值，那就会造成算法性能的下降。 由于 DQN 算法是基于 Q-learning，因此 DQN 本质上也存在这个问题。 1.2 本篇论文的工作本篇论文提出的 Double DQN 是根据 (van Hasselt, 2010) 的 idea 提出的。本篇论文首先证明了 Q-learning 的估计偏高问题的确会影响到算法的性能，并且这是一种普遍现象。进而本文证明 Double Q-learing 能缓解这种问题，并结合 Double Q-learing 和 DQN 提出了一种解决估计偏高，提高 DQN 算法性能的方法，同时在 Atari 各项游戏中取得更优的成绩。 2. 相关知识关于 Q-learning 和 DQN 的背景知识在这里不再赘述，可以阅读《DQN 相关论文笔记（上）》。 Double Q-learning 背后的思想是将动作的选择和动作-值的估计解耦，让它们使用不同的 Q 函数（网络）。注意 Nature DQN 虽然提出了独立目标 Q 网络，但是实际上目标 Q 网络的参数只是相当于在线 Q 网络的参数的延迟更新，本质上它们的参数是同一个参数。 van Hasselt 在2010年提出了 Double Q-learning 的思想。Double Q-learning 采用两组独立的参数 $\theta$ 和 $\theta’$，每个样本随机更新其中一个参数。为了进行更清晰的对比，将传统的在线 Q-learning 算法 的目标 Q 函数改写为： y_t^Q = r_{t+1} + \gamma Q(s_{t+1},\mathop{argmax}_a Q(S_{t+1},a;\theta_t);\theta_t)double Q-learning 的目标 Q 函数： y_t^{DoubleQ} = r_{t+1} + \gamma Q(s_{t+1}, \mathop{argmax}_a Q(s_{t+1}, a; \theta_t);\theta_t')可以看到仍然采用在线 Q 函数的当前的参数 $\theta_t$ 来执行贪婪策略（动作选择），但是采用新的参数 $\theta_t’$ 来重新公平地评估这个策略。第二组参数 $\theta_t’$ 可以通过对称地交换 $\theta$ 和 $\theta’$ 的角色进行更新，也就是说 $\theta$ 和 $\theta’$ 轮流交换负责动作选择和动作评估两个过程。 3. 估计误差引发过度估计证明本文证明了无论什么样的估计误差，都会给动作-值的带来偏高的估计。 论文中给出了定理一，证明一旦存在估计误差，传统的在线 Q-learning 算法的目标 Q 函数的估计一定会偏高，也就是具备非零下限；而 Double Q-learning 可以让下限为零。定理一如下： Theorem 1. Consider a state $s$ in which all the true optimal action values are equal at $Q_*(s,a)=V_*(s)$ for some $V_*(s)$. Let $Q_t$ be arbitrary value estimates that are on the whole unbiased in the sense that $\sum_a(Q_t(s,a)-V_*(s))=0$, but that are not all correct, such that $\frac{1}{m}\sum_a(Q_t(s,a)-V_*(s))^2=C$ for some $C &gt; 0$, where $m \ge 2$ is the number of actons in s. Under these conditions, $max_a Q_t(s,a) \ge V_*(s) + \sqrt{\frac{C}{m-1}}$. This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learing estimate is zero. 定理一表明即使动作-值估计的平均值是正确的，但是一旦有某个动作-值估计错误，带来的估计误差都会让目标 Q 函数的估计值偏高。下限 $\sqrt{\frac{C}{m-1}}$ 需要结合具体值分析。一般来说动作个数 $m$ 越多，估计偏高的程度越大。 本文通过实验证明了传统的在线 Q-learning 出现估计偏高的问题是很普遍的： 并不是只有特定的真实动作-值函数才会导致目标 Q 值函数出现估计偏高，不同的真实动作-值函数都会导致这种问题。 并不是阶数不够的动作-值逼近函数才会出现估计值偏高的问题，阶数高的动作-值逼近函数也会出现估计值偏高的问题，并且可能会更加严重。因为高阶的动作-值逼近函数会匹配所有的样本点，但是也存在过拟合问题，对于潜在的未被采集的样本点，可能估计偏离很大。 估计偏高的问题会随着训练过程不断传播，导致越来越严重，从而让训练过程发散，造成算法性能下降。 4. 算法模型4.1 Double DQN 的改进结合 Double Q-learning 和 DQN 已有的结构，本文提出利用在线 Q 网络执行贪婪策略的动作选择，但是用目标 Q 网络对动作-值进行评估。Double DQN 的目标 Q 函数如下： y_t^{DoubleDQN} = r_{t+1} + \gamma Q(s_{t+1}, \mathop{argmax}_a Q(s_{t+1}, a; \theta_t), \theta_t^-)对比 DQN 的目标 Q 函数： y_t^{DQN} = r_{t+1} + \gamma \mathop{argmax}_a Q(s_{t+1}, a; \theta_{t}^-)注意 Double DQN 的目标 Q 网络的参数更新与 Nature DQN 中的更新方式相同，也就是每隔一段周期，目标 Q 网络的参数直接复制为在线 Q 网络的参数的副本。 4.2 Double DQN 和 DQN 的区别DQN 动作的选择和动作的评估是依托于同一个目标 Q 网络的，也就是说从目标 Q 网络中执行贪婪策略选择动作之后，继续用目标 Q 网络对该动作进行评估。 然而 Double DQN 中动作的选择和动作的评估是解耦的，并不是依托于同一个网络。动作的选择依托的是在线 Q 网络，而动作的评估依托的是目标 Q 网络。也就是说先从在线 Q 网络中执行贪婪策略选择动作之后，再用目标 Q 网络对该动作重新进行评估。 5. 工程设置上的调整 将目标 Q 网络参数的更新周期从 10000 变成 30000，增大了更新周期。 $\epsilon$ -贪婪策略的超参数 $\epsilon$ 在学习时变成从 0.1 到 0.01，在评估时，用的是 $\epsilon = 0.001$。 给 Q 网络的最后一层加上偏置项，所有动作共享这个偏置项。 二、 Dueling DQN: 优势函数 [4]这篇论文提出了针对 model-free RL 的 dueling network框架。它是对传统 DQN 架构层面上的改动，将基于状态的 V 函数（value function）和状态相关的优势函数（advantage function）分离。Advantage 函数的思想基于1993年 Baird 提出的 Advantage updating。除了传统的 V 函数外，引入的 Advantage 函数 A(x, u) 的定义是当采取动作 $u$ 相比于采取当前最优动作能多带来多少累积折扣奖励。简单粗暴得说，就是选这个动作比当前最优动作（或其它动作）好多少。 1. Dueling DQN 简述1.1 Dueling DQN 的改进点Dueling DQN 是一种更适用于 model-free RL 的神经网络架构，如下图所示： 上图的第一行代表之前的 DQN 网络架构，第二行代表 Dueling DQN 网络架构。可以看出 Dueling 网络框架末端有两条分流（绿线前的红色柱子），分别代表状态函数和优势函数。不过 Dueling 网络框架只采用一套卷积层，也就是说状态函数和优势函数共享一套卷积网络参数。分流出来的状态函数和优势函数最后通过一层特殊的聚集层（图中的绿线）汇合成动作-值函数 Q。 直觉上，Dueling 网络架构可以学习到哪些状态是有价值的，而不需要学习每个状态中每个动作的价值。可以预见到，引入优势函数后，对于新加入的相似动作可以很快学习，因为它们可以基于现有的状态函数来学习。另外这种 Dueling 网络架构可以运用在很多 model-free 的 RL 方法上，不仅局限于 DQN。 论文中举了一个赛车游戏的例子：状态函数专注于远处（地平线）和分数，也就是长期目标，优势函数专注于附近障碍，也就是短期目标。这样状态函数和动作函数就能学习到不同时间层次的策略。这种学习的做法有几个好处： 一是状态函数可以得到更多的学习机会，因为以往一次只更新一个动作对应的 Q 函数。 二是状态函数的泛化性更好，当动作越多时优势越明显。直观上看，当有新动作加入时，它并不需要从零开始学习。 三是因为 Q 函数在动作和状态维度上的绝对数值往往差很多，这会引起噪声和贪婪策略的突变，而用该方法可以改善这个问题。 因此，通过解耦计算 V(s)，找出对于那些任何行为都不会被影响的状态尤其有用。在这种情况下，不必计算每个动作的值。例如，向右或向左移动仅在存在碰撞风险时才去关注。而且，在大多数状态下，无论选择何种行动，对发生的事情没有任何影响。 1.2 Dueling DQN 的来源早在1993年 Baird 就提出将状态函数和优势函数分开的概念。1995年 Harmon 他们发现优势函数学习比 Q-learning 收敛速度更快，然后在1996年 Harmon 和 Baird 就提出只依赖于优势函数的学习方法。 Dueling 网络架构可以用在很多 Model-free 的 RL 方法上，不仅仅是 DQN 这类值函数逼近的方法，还可以运用在策略梯度一类的方法上。2000 年 Sutton 首先将优势函数用在策略梯度的方法上，2015 年 Schulman 等人估计优势函数的值来降低策略梯度算法的方差。 2. 相关知识关于 RL、 Q-learning、DQN 和 Double DQN 的基础知识可以参考前面的笔记，这里不再赘述。这里着重介绍一下优势 A 函数的定义和意义，以及优先经验回放方法（Prioritized Replay）。 2.1 优势函数优势函数的定义如下： A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)需要注意的是，策略 $\pi$ 的所有动作的平均优势为零，即： \mathbb{E}_{a \sim \pi(s)}[A^\pi(s,a)] = 0这个等式可以明显从状态函数 $V^\pi(s)$ 的定义看出： V^\pi(s) = \mathbb{E}_{a \sim \pi(s)}[Q^\pi(s,a)]直觉上，状态 V 函数是评估当前特定状态的价值，它是综合执行所有动作带来的价值后给出的期望（平均）价值。而动作-值 Q 函数评估的是特定的动作在该状态下带来的价值。很明显，动作-值 Q 函数减去状态 V 函数得到的优势 A 函数，表征的是一个动作在当前状态下的重要性。这种重要性忽略当前状态的价值，只强调动作本身具备多大的价值（相对于其他动作而言）。 从公式的定义上来看，动作-值函数的值越大，优势函数的值也越大，似乎没有必要单独对优势函数进行学习。但这是针对所有动作的动作-值函数都能正确估计到当前状态的价值的情况下，也就是说估计的状态函数在不同的动作上是确定的并且是相等的。但是在利用动作-值函数进行学习的算法中，并不能保证针对每个动作输出的动作-值都包含着对当前状态的价值的正确估计。因此通过解耦估计，Dueling DQN 可以直观地了解哪些状态是（或不是）有价值的，在有价值的状态下找到优势函数 A 值最大的动作比单纯找到动作-值函数 Q 值最大的动作更有意义。 2.2 优先经验回放Schaul 在2016年将 Prioritized replay 方法和 DDQN 方法结合起来，并在 Atari 系列游戏取得当年最优的分数。 优先经验回放背后的思想是提高那些具有高预期学习进度的经验元组的回放概率。计算学习进度的预期是通过 TD 误差的绝对值公式。经验池中 TD 误差绝对值越大的样本被抽取出来训练的概率越大，加快了最优策略的学习 Dueling 网络结构可以作为很多创新性的算法的一个补充，也就是说无论算法模型采用均匀经验回放还是优先经验回放，采用这种网络机构都可以提升算法的性能。 3. 算法模型3.1 Dueling 网络的分流和汇合Dueling 网络结构的低层卷积层和 DQN 是相同的，但是在卷积层之后不是单个全连接层，而是两个分流的全连接层，分别估计价值函数和优势函数。然后两个分流的输出在下一层被合并成一个动作-值函数，为每个动作输出 Q 值。 将两条全连接层的分流合并输出一个动作-值估计 Q 值需要考虑以下两个约束条件： 优势函数满足 $\mathbb{E}_{a \sim \pi(s)} [A^\pi(s,a)]=0$ 对于确定的策略，动作 $a^* = argmax_{a’ \in \mathcal{A}} Q(s, a’)$，满足 $Q(s, a^*) = V(s)$，因此 $A(s, a^*) = 0$ 在 Dueling 网络架构中，全连接层状态网络分流可以表示为 $V(s;\theta, \beta)$，这是一个标量。另一条全连接层优势网络分流可以表示为 $A(s,a;\theta, \alpha)$，但这是一个矢量。参数 $\theta$ 表示网络卷积层的参数，$\alpha$ 和 $\beta$ 分别表示两条全连接层分流的参数。 根据优势函数的定义，动作-值函数表示为： Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha)注意到 $V(s;\theta,\beta)$ 是标量，因此计算时需要将其复制 $|\mathcal{A}|$ 次。 3.2 优势函数的限定但是我们需要意识到上述公式，对状态函数和优势函数的估计并不总是正确的。对于一个确定的Q，有无数种 V 和 A 的组合可以得到 Q ，也就是说无法找到确定的 V 和 A。文中称之为可识别性问题（identifiability issue）。再用通俗一点的例子来讲，现在需要学习的是状态 V 网络和优势 A 网络，需要让这两个网络合起来的动作-值 Q 网络达到最优。但是可能状态 V 网络学习得并不好（假设学到了一个偏低的评估值），为了让动作-值 Q 网络符合目标网络的要求，优势 A 网络的学习也会产生一定的偏差（也就是学到一个偏高的估计值）。这样虽然看起来动作-值 Q 接近了目标值，但是状态网络和优势网络都学习得不好，等到实际运用网络的时候，状态网络和优势网络的弊端就会暴露出来，从而导致动作-值 Q 网络的性能低于预期。 因此我们需要对 A 进行一定的限定，强制我们的优势函数在选中的行动上具有0优势。也就是保证该状态下各种动作的优势函数大小排序关系不变的前提下，限制动作的优势值，缩小 Q 值的变化范围，从而加强状态函数 V 的学习。在 Dueling 网络架构下，状态 V 网络的学习本质上是要比优势 A 网络的学习要重要的。 论文中第一种做法是利用所有动作中优势函数的最大值对估计的优势函数进行限定，公式如下： Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + \left(A(s,a;\theta, \alpha) - \mathop{max}_{a' \in \mathcal{A}} A(s,a';\theta,\alpha) \right)个人觉得利用最大操作对优势函数进行限定可能存在限定过于严格的问题，也就是说动作-值函数的值过度依赖于状态函数的值，从而导致算法的探索性能下降（受限于局部最优状态）。 因此论文提出了第二种做法，利用所有动作中优势函数的平均值对估计的优势函数进行限定，公式如下： Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + \left(A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \mathop{\sum}_{a' \in \mathcal{A}}(s,a';\theta,\alpha) \right)另外论文还尝试了利用“最大化限定 + softmax Q 值”的方法，不过结果与第二种做法差别不是很大。 加上对优势函数的限定，虽然会破坏原始公式的语义（因为多减了一个限定常数），但是这种限定的方法会让算法更加稳定，因为动作的优势值不必每次都需要学习达到最优值，只要达到平均优势值即可，同时加上对优势函数的限定也会加强状态函数的学习。 其实论文中并没有太多关于为什么对优势函数进行限定的解释，但限定的这个步骤实际上是论文最大的创新点之一。 4. 工程设置上的调整4.1 网络结构网络的卷积层和Nature DQN 是一样的，也是三层卷积层，激活函数用的都是非线性激活函数。不过 Dueling 网络结构在卷积层之后分成状态流和优势流，都是用 512 个神经元的全连接层来表示。状态流输出一个状态值，优势流输出 N 个动作优势值（N 是动作的个数）。 4.2 优化过程的调整学习率比 Nature DQN 稍微降低，变成6.25 10-5。同时采用了 gradient clipping* 技术，在反向传播时将梯度的绝对值钳制在小于或等于10的范围内。 4.3 优先经验回放采用 Schaul 等人在2016年提出的 Prioritized Experience Replay 优先经验回放技术。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN相关论文笔记（上）]]></title>
    <url>%2F2019%2F09%2F12%2FDQN%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要提及下面四篇关于DQN的著名论文的前两篇： [1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. 1–9. Retrieved from http://arxiv.org/abs/1312.5602 [2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 [3] Van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep reinforcement learning with double Q-Learning. 30th AAAI Conference on Artificial Intelligence, AAAI 2016, 2094–2100. [4] Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., &amp; De Frcitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. 33rd International Conference on Machine Learning, ICML 2016, 4(9), 2939–2947. DQN（Deep Q-Learning）算是 DRL 的开山之作，算是采用了 Value function approximation 的 critic-only 类算法，实现了从感知到动作的端对端学习法，由 DeepMind 在 NIPS 2013 上提出[1]，后在 Nature 2015 上提出改进版本[2]。Double-DQN[3] 和 Dualing-DQN[4] 都是 DQN 的改进版本，前者对训练算法进行了改进，后者对模型结构进行了改进。 一、 DQN：成功将DL和RL结合 [1]1. DQN 简述2013年这篇论文第一个提出利用深层强化学习模型从高维度传感器信号中学习控制策略。模型由卷积神经网络构成，通过本文提出的方法（Q-learning的变种），实现从原始像素输入到值函数输出。 1.1 RL 结合深度学习的问题在深度学习盛行之前，RL需要依靠手工设计特征，并组合成值函数或策略函数。RL性能好坏很大部分取决于特征的质量。深度学习盛行之后，可以直接中原始的计算机视觉和语音信号中自动提取特征。深度学习的方式是利用一堆神经网络（例如卷积神经网络，多层感知机，循环神经网络，有限玻尔兹曼机等）进行深层联合，并采用监督或非监督模型进行学习。自然而然考虑是否能将深度学习和强化学习进行结合，利用深度学习进行特征提取，而不必手工设计特征。 但是将深度学习与强化学习结合并非易事，起码存在三个关键问题： 采用深度学习的方式通常需要很多带标签的训练数据，但是RL每次与环境交互只能获得一个奖励值，与深度学习的训练集相比，奖励值十分稀疏，并且还会带有噪声以及延迟。 深度学习算法假设数据集样本是独立的，但是在强化学习中，状态序列具有很大的关联度，状态之间并不能认为是服从独立分布的。 另外RL当学习到新的动作策略时，状态分布和动作分布都会发生改变，然而深度学习是假定数据具有固定的分布。 1.2 本篇论文的工作本篇论文采用卷积神经网络来克服深度学习和RL结合的问题，并直接从视觉信号（像素）学习到成功的控制策略。提出Q-learning的改进版本对网络参数进行训练。同时为了解决数据关联度高，训练过程中数据分布不稳定的问题，本文采用了经验回放的机制，，每次随机抽取先前的一小部分transition进行训练，这样可以平滑训练集的分布。 2. 相关知识动作空间是离散的，表示为 $\mathcal{A} = {1, \dots, K}$。 在每个时间步长，agent 会从 $\mathcal{A}$ 中挑选一个动作 $a_t$。 本文认为环境是部分可观察的，因此只用当前的观测值 $x_t$ 来表示当前环境的状态是不够的。因此将 $t$ 时刻的状态 $s_t$ 定义为动作值和观测值的序列，也就是 $s_t = x_1, a_1, x_2, \cdots, a_{t-1}, x_t$。 （这里描述的环境是动态的，部分感知的，因此状态的描述需要依赖于动作值和观察值的序列，但是如果环境是完全感知的，静态的，一般都是将 $x_t$ 等同于 $s_t$。DDPG的论文就是这么做的） 定义 $t$ 时刻的折扣奖励 $R_t = \sum_{t’=t}^T \gamma^{t’-t}r_{t’}$，其中 $T$ 是结束的时刻，$\gamma$ 是折扣因子。 定义最优动作-值函数 $Q^*(s,a) = max_\pi \mathbb{E} [R_t|s = s_t, a = a_t, \pi]$，$\pi$ 是动作分布（动作空间的各个动作被选择的概率）。最优动作-值函数满足贝尔曼方程，也就是可以写成： Q^*(s,a) = \mathbb{E}_{s' \sim \mathcal{E}} \left[r + \gamma max_{a'} \, Q^*(s', a') |s, a \right]上述公式中的 $r$ 和 $s’$ 由 $s$，$a$ 决定。通过上述的贝尔曼方程形式的动作-值函数，可以得到一种求解最优动作-值函数的迭代方法： Q_{i+1}(s, a) = \mathbb{E} [r + \gamma max_{a'} Q_i(s', a') | s, a]其中 $i$ 是迭代次数，当 $i \rightarrow \infty$时，$Q_i \rightarrow Q^*$。 但是实际上这种方法非常有限，对于状态空间庞大的环境，根本不适用。因为这种迭代形式的方法需要为每一个 $\{s, a\}$ 序列都求解一个动作-值，无法对庞大的状态空间进行泛化。 为了应对庞大的状态空间，过去的RL方法通常用线性函数（有时候也用非线性函数）来对动作-值函数进行逼近，表示为 $Q(s,a;\theta) \approx Q^*(s,a)$。而本文就是将线性函数逼近的方法改成神经网络逼近，该网络称之为 Q 网络。 Q 网络的参数更新通过最小化损失函数，其定义为 Q 网络输出的 Q 值和目标 Q 值的均方误差函数： L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[(y_i - Q_i(s,a;\theta_i))^2 \right]y_i = \mathbb{E}_{s' \sim \mathcal{E}} \left[r + \gamma max_{a'} Q(s', a'; \theta_{i-1})|s, a \right]其中 $\rho(s, a)$ 称之为行为分布，也就是 $s$ 和 $a$ 序列的概率分布。 $y_t$ 是目标 Q 值，$\theta_{i-1}$ 在迭代过程中是保持不变的。需要注意的是，和监督学习不同，监督学习的目标通常是固定的，并且与网络的参数无关。然而此处目标的计算也依赖于网络的参数。 对均方误差损失函数计算 $\theta_i$ 的梯度： \nabla {\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cot), s' \sim \mathcal{E}} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]在实际计算中，并不会计算梯度的完整期望，一般为了简便采用SGD方法来最小化上述损失函数。也就是，在每个时间不长，只选择一个样本来计算损失函数的梯度，代替损失函数梯度的期望值，这种方法就是我们熟悉的 Q-learning 方法。 Q-learning 方法是 model-free 和 off-policy 类型的方法。 model-free 是指不需要对环境进行建模，而是通过从环境中收集样本进行学习。另外，Q-learning 的目标策略和行动策略不是同一种策略，这种方式也称为 off-policy。目标策略采用的是贪婪策略，即 $a = max_a Q(s,a;\theta)$，行动策略采用的是 $\epsilon$ -贪婪策略，有 $1-\epsilon$ 的概率选择贪婪策略，有 $\epsilon$ 的概率选择随机策略。 在深度学习兴起之前，人们普遍认为将 model-free 强化学习算法和非线性函数逼近，或者 off-policy 结合的方法都可能导致 Q 网络发散。所以传统的强化学习一般都采用线性函数逼近动作-值函数。近年来，由于深度学习的兴起，越来越多研究采用深度学习方法与强化学习方法结合。有一篇工作和本文提出的 DQN 方法比较像，称之为 neural fitted Q-learning（NFQ）。不过有两点主要区别： NFQ 采用批梯度下降方法最小化损失函数，DQN 采用随机梯度下降方法，相对来说，随机梯度下降方法的计算代价更小。 NFQ 采用深层自动编码器对视觉输入信号进行特征提取，利用非监督方法训练得到低维度的状态表示。然后再将低维度的状态表示应用 NFQ 学习控制策略（相当于将两种网络组合到一起，两种网络是单独训练的）。但是 DQN 直接应用视觉输入信号，不做任何处理，最终会学习到和动作-值显著关联的特征，也就是学习到的特征会根据获取到的动作-值而变化（相当于特征提取网络嵌入到 Q 网络，它们是一起训练的）。虽然文中是说不做任何处理，但是实际上还是对视觉信号做了灰度化以及下采样的预处理的，只不过输入依然是像素点而已。 3. 算法模型3.1 DQN 的关键点DQN 模型的目标是将强化学习和深层神经网络结合起来，只需要输入原始 RGB 图像并通过 SGD 对样本进行训练便可以输出最优 Q 网络。 DQN 模型中最重要的一个技巧是采用了经验回放机制。在每个时间步长，都会将经验 $e_t = (s_t, a_t, r_t, s_{t+1})$ 存储在数据集 $\mathcal{D} = \{e_1, e_2, \dots, e_N\}$ 中（其实终止标志 $done$ 也要存进去，表示当前状态是否还有后继状态），并且会保留许多 episode 的经验，即新的 episode 开启时，经验回放池 $\mathcal{D}$ 不会重置。 DQN 模型中采用 off-policy，与Q-learning是一样的，目标策略采用贪婪策略，行动策略采用 $\epsilon$ -贪婪策略。 因为从经验回放池中抽取的历史经验具有不一样的长度（因为状态实际上是一个动作-观测值序列），因此定义函数 $\phi$ 来将长度不一致的历史经验输出为固定长度的历史经验表示。函数 $\phi$ 的具体定义参考3.3节。 算法伪代码如下所示： 等式（3）就是上面的均方误差损失函数： \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot), s' \sim \mathcal{E}} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]因为样本是从经验回放池 $\mathcal{D}$ 中均匀采样，因此写成下面这种形式会更清晰一些：\nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]$U(\mathcal{D})$ 表示经验回放池 $\mathcal{D}$ 中样本的均匀分布。 3.2 DQN 的优点DQN 与传统的在线 Q-learning相比，有以下一些优点： 每个时间步长的经验都有可能用于未来很多次参数更新过程，因此提高了数据的利用效率。 比起 Q-learning 直接用连续的样本学习，DQN 采用经验回放池的机制打破了数据间的关联性，降低了每次更新的方差。 通过使用经验重放，对行为分布的许多先前状态进行平均，平滑了训练样本数据分布，避免了参数的振荡或发散。另外 DQN 必须采用 off-policy，因为当前参数和生成样本时的参数已经不同，而使用旧参数生成的动作来采样更新当前参数，很容易陷入局部最优点或者震荡。 4. 工程上的设置4.1 预处理函数 $\phi$ 的定义 将原本 210 * 160 像素，128 种颜色的图像下采样为 110 * 84 像素的灰度图像。 然后裁剪 84 * 84 像素的图像区域，主要包含 agent 正在运作的相关区域。 最后 $\phi$ 将裁剪后的历史状态最后 4 帧图像堆叠成一个向量，输入到 Q 网络中。 4.2 网络结构 第一层隐藏层采用 16 个 8 * 8 卷积核，步长为 4。采用非线性激活函数。 第二层隐藏层采用 32 个 4 * 4 卷积核，步长为 2。采用非线性激活函数。 最后一层隐藏层采用全连接层，并输出 256 个非线性激活函数的值。 输出层采用全连接线性的方式，并为每个动作输出一个 Q 值。 非线性激活函数第二篇论文中有提及，应该是 $max(0, x)$。 4.3 其他设置 由于每个游戏环境的 reward 范围不同，因此论文将正的 reward 限制为1，负的 reward 限制为-1，reward 值为0时保持不变。 采用 RMSProp 优化算法，mini-batch 的大小是32。$\epsilon$ -贪婪算法中的超参数 $\epsilon$ 在前面一百万时间步长中从1 退火到 0.1，最后保持不变。 训练一千万时间步长，经验回放池中存储最近一百万个帧。 游戏画面每隔 $k$ 帧，agent 就会选择一个新的动作。因为游戏仿真器的运算速度大于 DQN 模型计算动作的速度，为了防止画面卡顿，故此采用这样一个跳帧的方法。 $k$ 一般取值为4。 性能的度量标准采用的是预测的动作-价值函数 Q，而不是总的奖励值。 对于学习后的模型，选择动作还是采用 $\epsilon$ -贪婪策略，其中 $\epsilon$ 取值0.05。 5. 可以提升的地方经验回放池只能存储最近的 $N$ 条经验，采样用的是均匀分布。但是一种更合理的方式应该是给这些经验值分配不同的重要性权重，从而将新的经验代替那些不重要的经验。 二、 Nature DQN：独立目标函数 [2]1. 使用深层网络描述 Q 函数的问题已经有相关研究指出使用非线性函数逼近器对动作-值函数（Q 函数）进行逼近存在发散问题。这种不稳定来源于以下一些原因： 用于训练的状态是一个序列，该序列中的前后的状态高度相关。 Q 网络的参数有微小更新就会导致策略发生巨大变化，并因此导致训练样本分布的巨大变化。 目标函数中使用的 Q 函数（目标 Q 函数）和待优化 Q 函数（在线 Q 函数或估计 Q 函数）之间存在参数联系，每次更新的目标都是固定上次更新的参数得来，优化目标跟着优化过程一直在变。 第一篇论文解决了前两个问题，但第三个问题还是存在的。目标 Q 函数参数的计算仍然依赖于估计 Q 函数的参数。 2. 论文中提出的解决方法在第一篇 DQN 论文中，通过使用经验回放有效的解决了前两个问题，通过存储并随机采样经验来打破了样本之间的相关性，同时平滑了训练样本数据分布。 第三个问题则是这次改进完成的。通过让在线 Q 函数参数更新一定周期之后再去更新目标 Q 函数的参数，从而降低了目标 Q 函数与在线 Q 函数之间的相关性。 迭代更新 i 次后的损失函数表示如下： L_i(\theta_i) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} \left[ (r + \gamma max_{a'} Q(s',a';\theta_i^-) - Q(s,a;\theta_i))^2 \right]对比上篇论文3.1节的公式，可以发现目标 Q 函数中的参数是 $\theta^-$，而不是$\theta_{i-1}$。$\theta^-$ 是每经过 C 个步长从在线 Q 函数中复制而来，然后保留不变。在目标 Q 网络参数更新和在线 Q 网络参数更新之间增加一个时延可以减缓参数更新的分散和抖动。 最后本篇论文中还有一个小改进：进行误差裁剪。将上述损失函数中的 $r+\gamma Q(s,a;\theta^- - Q(s,a;\theta_i))$ 裁剪到范围（-1， 1），这可以提高算法的稳定性。 本篇论文的伪代码如下： 可以发现比上篇论文的伪代码多了独立目标 Q 网络的参数 $\theta^-$。另外奖励裁剪和误差裁剪并没有体现在伪代码中，但实际应用时是用到的。 3. 工程上的设置3.1 预处理 对每一帧进行编码时，取当前帧和前一帧每个像素颜色值的最大值。 将 RGB 帧转换为灰度帧，并裁剪大小为84 * 84。 预处理函数 $\phi$ 将每四个邻近帧作为状态输入到 Q 网络。 3.2 网络结构 不像以前一些方法，采用状态-动作对作为输入。DQN 的 Q 网络将状态表示作为输入，然后为每个动作输出一个 Q 值。 输入层是经过预处理和函数 $\phi$ 映射的 84 * 84 * 4 的灰度像素值 第一层隐藏层采用32个8 * 8，步长为4的卷积核，采用非线性激活函数（可能是 $\max(0,x)$）。 第二层隐藏层采用64个4 * 4，步长为2的卷积核，采用非线性激活函数。 第三层隐藏层采用64个3 * 3，步长为1的卷积核，采用非线性激活函数。 第四层隐藏层采用全连接层，输出为512维向量，采用非线性激活函数。 输出层是一个全连接层，对每个动作值对应一个输出。 这篇 DQN 论文相对于第一篇的网络结构更加复杂和庞大，隐藏层多了一层。 3.3 超参数的选取所有超参数的选取并非通过系统的搜索而得到，只是在一些游戏上做非正式的搜索得到的。因为超参数的选取十分重要，因此放一下论文中的截图，以备之后参考： 3.4 其他设置奖励裁剪、优化算法、跳帧数与第一篇 DQN 论文是一样的。 3.5 评估过程的设置每个游戏玩30次，采用 $\epsilon$ -贪婪策略，其中$\epsilon=0.05$。 4. DQN 学习到的特征表示DQN 可以将视觉上相似的画面表示成邻近的特征，并且还可以将奖励相似但视觉上不相似的画面也表示为邻近的特征，这说明 DQN 学习到的特征能够很好地预测 Q 值。上图是采用 t-SNE 方法绘制的最后一层隐藏层输出的状态特征分布图。颜色越红，表示动作-值越大。左下方，右上方和中下方三组图，每组里面的图像从视觉上看是相似的，经过 DQN 输出的特征表示也是邻近的。而左上方，中上方和右下方三组图，虽然每组里面的图像从视觉上看不相似，但是它们的动作-值是相似的，因此经过 DQN 输出的特征表示也是邻近的。 5. DQN 的核心点这篇论文中指出 DQN 的核心之处有三点： 使用了经验回放池 使用了独立的目标 Q 函数 深度卷积网络的设计 6. DQN 目前不能解决的问题long-term credit assignment 问题，也就是无法处理需要长远规划的策略。如果决策需要考虑的时间维度太长，DQN 可能无法学习出比较合适的策略。 7. DQN 的神经生物学基础DQN 是端到端强化学习方法，特征的学习和策略的学习并不是分开的，而是结合通过卷积神经网络结合在一起。获得的奖励会随时影响到卷积网络中的特征学习，然后制定出更好的策略，获取更好的奖励。在生物神经学中，已经有证据表明，在感知学习过程中，奖励信号可能会影响灵长类视觉皮质内表征的特征。 另外 DQN 中最重要的技巧——经验回放池，在神经生物学中也可以找到相似的机制。哺乳动物的海马体中存在一种物理机制，会在休息时间将最近经历过的经验轨迹重新激活（快速回放）。这意味着可以推测出动作-值函数可以通过历史经验进行学习。 不过海马体中对重要经验会存在更为深刻的印象，因此经验回放池也可以对经历过的经验分配不同的偏置权重，这是强化学习中的另一个话题，称为 prioritized sweeping。（上一篇论文中也提到过）]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next渲染数学公式]]></title>
    <url>%2F2019%2F09%2F11%2Fhexo-next%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[博客中通常会用latex来写数学公式，但是浏览器并不总是能渲染这些公式，导致在浏览器上看是一堆latex代码，这就很烦了。有一种比较简便的方法去渲染这些公式，就是在浏览器中添加相关的mathjax插件。遗憾的是，并不能保证每个浏览器都会有相应的mathjax插件，并且浏览博客的读者浏览器也不能保证一定装了这些插件。为了从根源上解决问题，我们直接让hexo具备渲染mathjax的能力，这样无论浏览器是否开启mathjax插件，公式都可以完美呈现在读者面前。 参考博客：https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/ 1. 使用Kramed 代替 MarkedMarked渲染引擎不可以渲染mathjax，但是Kramed可以。因此将hexo默认的Marked引擎卸载，装上Kramed。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：12345// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125; 变成：12345function formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ // return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$'); return text;&#125; 2. 停止使用hexo-math卸载原来的hexo-math：1npm uninstall hexo-math --save 安装hexo-renderer-mathjax包：1npm install hexo-renderer-mathjax --save 3. 更改默认转义规则因为 hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 , 所以我们需要对默认的规则进行修改.首先， 打开&lt; your-hexo-project &gt;/node_modules/kramed/lib/rules/inline.js,把下列代码：1escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改为：1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 把下列代码：1em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改为：1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 4. 配置中开启mathjax在主题 _config.yml 中开启 Mathjax， 首先找到math字段，然后修改两个enable为true，也就是将下列代码：12345math: enable: false ... mathjax: enable: false 更改为：12345math: enable: true ... mathjax: enable: true 需要注意的是，不同版本的next主题，上面的math和mathjax的嵌套关系可能不一致，总之就是把math和mathjax的enable全都设置为true就可以了。 5. 博客中开启mathjax在博客的模板中添加mathjax: true，注意冒号后面是有一个空格的。123456---title: Testing Mathjax with Hexocategory: Uncategorizeddate: 2017/05/03mathjax: true--- 如果嫌弃每次都需要声明开启mathjax，可以在&lt; your-hexo-project &gt;/scaffolds/post.md新增一句mathjax: true1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:mathjax: true--- 6. 更新Mathjax的CDN链接（可选）这个是可选的，一般情况下直接下载hexo-renderer-mathjax是不需要修改CDN的。如果以上配置都弄好后，还是不能渲染公式，可以尝试更新Mathjax的CDN链接。更新方法是：打开/node_modules/hexo-renderer-mathjax/mathjax.html，然后把标签\更改为：1&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; 注意：网上给出的CDN链接都未必百分百可用，建议多找几个试一下。可以去搜索mathjax国内cdn，参考网址：https://www.bootcdn.cn/mathjax/]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG代码实现]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1. 算法伪代码 首先是构建模型。构建并随机初始化估计网络 $Q(s,a|\theta^Q)$ 和 $\mu(s|\theta^\mu)$ 的权重 $\theta^Q$ 和 $\theta^\mu$。 构建目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，并对权重赋值： $\theta^{Q\prime} \leftarrow \theta^Q$，$\theta^{\mu\prime} \leftarrow \theta^{\mu}$。（公式中的导数符号如果看不清楚放大一点就可以了。。。） 初始化经验回放池 $R$。 接下来进行训练。外层循环是进行M个episode的训练，每个episode是智能体从行动开始到任务结束（或任务超时）的过程。为了进行有效探索，对确定性的动作 $\mu(s_t)$ 加上噪声（随机过程）$\mathcal{N}$。 初始化随机过程 $\mathcal{N}$。 获取初始状态值 $s_1$。 内层循环的次数是每个episode的时间长度 $T$： 根据确定性策略选取动作，并对动作添加噪声：$a_t = \mu(s_t|\theta^\mu)$。 执行动作 $a_t$，获取奖励 $r_t$ 和新的状态值 $s_{t+1}$。 将一个transition($s_t$, $a_t$, $r_t$, $s_{t+1}$)加入经验回放池。 对经验回报池进行采样，随机抽取 $N$ 个transitions构成一个mini-batch。 通过最小化Q值均方损失函数 $L$ 更新 $\theta^Q$，通过计算策略梯度 $\bigtriangledown_{\theta^\mu}$ 更新 $\theta^\mu$ 通过soft-update更新目标网络参数： \theta^{Q\prime} \leftarrow \tau\theta^Q + (1-\tau)\theta^{Q\prime}\theta^{\mu\prime} \leftarrow \tau\theta^\mu + (1-\tau)\theta^{\mu\prime} 2. 代码实现（基于pytorch）下面用代码实现简单的DDPG示例，用于gym中的小游戏Pendulum-v0（让摆锤倒立）。 2.1 构建网络构建描述动作-值函数的网络，采用三层全连接神经网络，输入是3维状态向量和1维动作值，输出是Q值。隐藏层采用relu激活函数，输出层不需要激活函数。代码如下：12345678910111213141516class QNet(nn.Module): def __init__(self): super(QNet, self).__init__() self.fc_s = nn.Linear(3, 64) self.fc_a = nn.Linear(1,64) self.fc_q = nn.Linear(128, 32) self.fc_3 = nn.Linear(32,1) def forward(self, x, a): h1 = F.relu(self.fc_s(x)) h2 = F.relu(self.fc_a(a)) cat = torch.cat([h1,h2], dim=1) q = F.relu(self.fc_q(cat)) q = self.fc_3(q) return q 构建描述动作策略的网络，采用三层全连接神经网络，输入是3维状态向量，输出是动作（连续的控制信号）。隐藏层采用relu激活函数，输出层采用tanh激活函数，动作值范围限定在[-1, 1]。代码如下：123456789101112class MuNet(nn.Module): def __init__(self): super(MuNet, self).__init__() self.fc1 = nn.Linear(3, 128) self.fc2 = nn.Linear(128, 64) self.fc_mu = nn.Linear(64, 1) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2] return mu 2.2 构建经验回放池经验回放池实际上是一个队列，当经验回放池满时，会抛弃旧的经验值，加入新采样的经验值。采样时，从经验回放池中随机抽取batch_size个经验值作为一个transition返回给训练机进行学习，代码如下：12345678910111213141516171819202122232425class ReplayBuffer(): def __init__(self): self.buffer = collections.deque(maxlen=buffer_limit) def put(self, transition): self.buffer.append(transition) def sample(self, n): mini_batch = random.sample(self.buffer, n) s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] for transition in mini_batch: s, a, r, s_prime, done_mask = transition s_lst.append(s) a_lst.append([a]) r_lst.append([r]) s_prime_lst.append(s_prime) done_mask_lst.append([done_mask]) return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \ torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \ torch.tensor(done_mask_lst) def size(self): return len(self.buffer) 2.3 构建Ornstein Uhlenbeck（OU）噪声OU过程是一种序贯相关过程，在DDPG中用于实现探索。OU过程满足下面的随机微分方程： dx_t = \theta(\mu - x(t)) dt + \sigma dW_t其中 $dW_t$ 是维纳过程（也称为布朗运动），满足： \triangle z = \epsilon \sqrt{\triangle t} ,\quad \epsilon \sim N(0, 1)$\triangle z$ 是变化量，$\epsilon$ 是标准正态分布。 OU过程的实现代码如下：1234567891011class OrnsteinUhlenbeckNoise: def __init__(self, mu): self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1 self.mu = mu self.x_prev = np.zeros_like(self.mu) def __call__(self): x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \ self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape) self.x_prev = x return x 2.4 构建训练模型估计网络的参数采用随机初始化，目标网络的参数复制估计网络的参数值。外层循环总共执行 $N$ 个episode，内层循环是每个episode的最大时间步长 $T$，超过 $T$ 之后重置状态，开启新的episode。每个episode结束之后，对估计网络和目标网络的参数进行训练和更新。需要注意的是，前期不会直接训练，直到经验回放池中的样本数量超过某个阈值才会开始训练策略。训练模型的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def train(): env = gym.make('Pendulum-v0') memory = ReplayBuffer() q, q_target = QNet(), QNet() q_target.load_state_dict(q.state_dict()) mu, mu_target = MuNet(), MuNet() mu_target.load_state_dict(mu.state_dict()) score = 0.0 print_interval = 20 mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu) q_optimizer = optim.Adam(q.parameters(), lr=lr_q) ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1)) for n_epi in range(N): s = env.reset() for t in range(T): # maximum length of episode is 200 for Pendulum-v0 a = mu(torch.from_numpy(s).float()) a = a.item() + ou_noise()[0] s_prime, r, done, info = env.step([a]) memory.put((s,a,r/100.0,s_prime,done)) score +=r s = s_prime if done: break if memory.size()&gt;train_threshold: for i in range(update_epoch): update(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer) soft_update(mu, mu_target) soft_update(q, q_target) if n_epi%print_interval==0 and n_epi!=0: print("# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;".format(n_epi, score/print_interval)) score = 0.0 torch.save(&#123; 'q_state_dict': q.state_dict(), 'q_target_state_dict': q_target.state_dict(), 'mu_state_dict': mu.state_dict(), 'mu_target_state_dict': mu_target.state_dict(), 'q_optimizer_state_dict': q_optimizer.state_dict(), 'mu_optimizer_state_dict': mu_optimizer.state_dict() &#125;, SAVE_PATH) 2.5 梯度下降更新参数采用adam优化器，对估计网络的参数 $\theta^Q$ 和 $\theta^\mu$ 进行mini-batch梯度下降优化。通过最小化均方损失函数,求 $\theta^Q$ 的梯度，利用adam优化器更新参数 $\theta^Q$。均方损失函数如下： L = \frac{1}{N} \sum_i(y_i - Q(s_i,a_i|\theta^Q))^2y_i = r_i + \gamma Q\prime(s_{t+1}, \mu\prime(s_{t+1}|\theta^{\mu\prime}) | \theta^{Q\prime})虽然实际上 $y_i$ 中也包含参数 $\theta^Q$，但是计算过程中通常忽略 $y_i$ 的梯度。 通过计算策略梯度，利用adam优化器更新参数 $\theta^\mu$ \bigtriangledown_{\theta^\mu} J \approx \frac{1}{N} \sum_i \bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_i, a=\mu(s_i)} \bigtriangledown_{\theta^\mu} \mu(s|\theta^\mu)|_{s=s_i}12345678910111213def update(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer): s,a,r,s_prime,done_mask = memory.sample(batch_size) target = r + gamma * q_target(s_prime, mu_target(s_prime)) q_loss = F.smooth_l1_loss(q(s,a), target.detach()) q_optimizer.zero_grad() q_loss.backward() q_optimizer.step() mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss. mu_optimizer.zero_grad() mu_loss.backward() mu_optimizer.step() 最后采用soft-update对目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，代码如下：123def soft_update(net, net_target): for param_target, param in zip(net_target.parameters(), net.parameters()): param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau) 2.6 运行DDPG训练好模型之后，将网络的参数保存，然后在评测的时候重新载入模型参数。采用确定性的动作进行评测，代码如下所示：1234567891011121314def evaluate(): env = gym.make('Pendulum-v0') checkpoint = torch.load(SAVE_PATH) mu = MuNet() mu.load_state_dict(checkpoint['mu_state_dict']) mu.eval() while 1: s = env.reset() done = False while not done: a = mu(torch.from_numpy(s).float()) s_prime, r, done, info = env.step([a.item()]) env.render() s = s_prime 2.7 超参数的设置超参数的设置很重要，但没有特定的方法指导，所以全凭经验。本实验设置的超参数如下所示： 1234567891011lr_mu = 0.0005lr_q = 0.001gamma = 0.99batch_size = 32buffer_limit = 50000tau = 0.005 # for target network soft updateupdate_epoch = 10train_threshold = 2000T = 300N = 2000SAVE_PATH = 'PATH/ddpg.pt']]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG论文笔记]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 简述论文题目：《CONTINUOUS CONTROL WITH DEEP REINFORCEMENTLEARNING》。该论文提出了基于 deterministic policy gradient 的 DDPG(deep deterministic policy gradient) 算法，能够运用在连续的动作空间中，能够 learn policy “end to end”。 1.1 处理连续动作空间的问题 DQN存在的问题是只能处理低维度，离散的动作空间。 不能直接把Q-learning用在连续的动作空间中。因为Q-learning需要在每一次迭代中寻找最优的$a_t$。对于参数空间很大并且不受约束的近似函数和动作空间，寻找最优的$a_t$是非常非常慢的。 连续动作空间的离散化：离散化后的动作数量随着自由度呈指数增长，导致离散后的动作空间太大，很难进行有效地探索。而且过于稀疏的离散化抛弃了动作空间的本身的结构信息。 1.2 处理庞大状态空间的问题 非线性函数逼近器（例如神经网络）的收敛性无法保证，但是这种形式的函数逼近器对于庞大的状态空间的学习和泛化而言是必要的。 1.2 DDPG 的关键点 model-free, off-policy, actor-critic, using deep function approximators based on the the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) 借鉴DQN两个重要技巧，对神经网络表示的value functions进行学习： the network is trained off-policy with samples from a replay buffer to minimize correlations between samples. the network is trained with a target Q network to give consistent targets during temporal difference backups. use batch normalization (Ioffe &amp; Szegedy, 2015) DDPG的优点在于它的简洁，只需要actor-critic框架和简单的学习算法，能够在实际的控制问题上发挥优势，比需要很多动力学建模的规划算法效果好。 2. 相关知识重点是目标函数，动作-值函数以及它的贝尔曼方程（迭代形式），Q-learning算法（即off-policy方法，行动策略和目标策略不是同一种策略）。 2.1 符号定义 t 时刻观测值： $\boldsymbol{x_t}$ 假定环境完全可知，t 时刻状态值： $\boldsymbol{s_t} = \boldsymbol{x_t}$ t 时刻动作： $\boldsymbol{a_t} \in \mathbb{R}^N $ 奖励值： $r_t$，标量 动作策略： $\pi : \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ 初始状态分布： $p(s_1)$ 状态转移概率： $p(s_{t+1} | s_t, a_t)$ 奖励函数： $r(s_t, a_t)$ 2.2 公式定义 折扣奖励函数： $R_t = \sum_{i=t}^T \gamma ^ {i - t} r(s_i, a_i)$, $\gamma \in [0, 1]$ 折扣状态访问分布： $\rho^\pi(s’) := \int_{\mathcal{S}} \sum_{t=1}^\infty \gamma^{t-1} p_1(s) p(s \rightarrow s’, t, \pi) ds$ 目标函数： $J(\pi_\theta) = \int_\mathcal{S} \rho^{\pi}(s) \int_{\mathcal{A}} \pi_\theta(s, a) R_1(s, a) da ds =\mathbb{E}_{s_i \sim \rho^\pi, a_i \sim \pi}[R_1]$ 动作-值函数： $Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{i \gt t} \sim \rho^\pi, a_{i \gt t} \sim \pi}[R_t | s_t, a_t]$ 动作-值函数的贝尔曼方程（迭代形式）： $Q^\pi(s_t, a_t)=\mathbb{E}_{s_{t+1} \sim \rho^\pi} \left[r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} [Q^\pi (s_{t+1}, a_{t+1})] \right]$ 如果动作策略是确定性策略，表示为 $\mu : \mathcal{S} \rightarrow \mathcal{A}$，则动作-值函数的贝尔曼方程的形式为： Q^\mu(s_{t}, a_{t}) = \mathbb{E}_{s_{t+1} \sim \rho^\mu} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu (s_{t+1}))\right ] 假设用参数 $\theta^Q$ 去逼近动作-值函数并采用Q-learning，则定义损失函数如下： L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta, a_t \sim \beta} \left[(Q(s_t, a_t | \theta^Q) - y_t)^2 \right] y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q)注意虽然$y_t$中也含有$\theta^Q$，但是对$L(\theta^Q)$求梯度时，通常忽略$y_t$，不对其求导。 3. 算法DDPG算法基于DPG算法，并采纳DQN中的两个重要技巧。接下来简单介绍一下基础算法DPG。 3.1 DPG算法DPG算法采用确定性策略 $\mu(s | \theta^\mu)$，每一时刻都将状态映射成确定的动作，同时也采用actor-critic框架。critic用参数为$\theta^Q$的函数 $Q(s,a|\theta^Q)$表示，并通过Q-learning和贝尔曼方程的方式进行学习。actor对目标函数应用链式法则（即策略梯度）更新参数 $\theta^\mu$： \begin{aligned} \bigtriangledown_{\theta^\mu} J &\approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \bigtriangledown_{\theta^\mu}Q(s, a|\theta^Q) |_{s=s_t, a=\mu(s_t|\theta^\mu)} \right] \\ &= \mathbb{E}_{s_t \sim \rho^\beta} \left[\bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_t, a=\mu(s_t)} \bigtriangledown_{\theta^\mu}\mu(s|\theta^\mu)|_{s=s_t} \right] \end{aligned}Q-learning采用的是异策略，行动策略表示为 $\beta$，目标策略表示为 $\mu$。 上述目标函数中，最外层是关于状态 $s_t$ 分布的平均，$s_t$ 就是通过执行 $\beta$ 策略采样来的。但是最里层动作-值函数如果展开成贝尔曼方程的形式，下一时刻采用的动作策略则是 $\mu$，如2.2节的第六条公式。 DPG 采用 SGD 随机梯度更新规则。采用批梯度下降是很难收敛的。 3.2 DDPG算法绝大多数优化算法都是假设样本是独立且均匀分布的，但是强化学习采集的样本本身就是一个序列，所以相邻间的样本是有关联的，不满足假设条件。另外，通常是采用mini-batch更新的方式，而不是在线更新。 为了解决样本相互关联的问题，DQN采用了经验回放池，具有固定大小，存储每次探索的 $(s_t, a_t, r_t, s_{t+1})$。当经验回放池满了之后，会丢弃最旧的数据，增加新采样数据。每次更新的时候，actor和critic都会统一从经验回放池中抽取一个mini-batch进行更新。经验回放池可以尽可能地设置得大一点，这样每次抽取的样本关联度会降低。 如果直接按照2.2节的第七条公式去更新网络 $Q(s, a|\theta^Q)$，在大多数环境中都是会发散的。因为该公式的计算目标值 $y_t$ 时也用了网络 $Q(s, a|\theta^Q)$ 的值。很明显，这个网络在还没有收敛的时候，作为目标的一部分进行学习，结果很容易发散。因为目标本身就好像在追求一个不稳定的值。 为了避免目标Q值 $y_t$ 中包含不稳定的网络 $Q(s, a|\theta^Q)$ 输出，采用称为“soft target update” 的更新方法。首先，重新复制一份actor网络，$\mu’(s|\theta^{\mu’})$和critic网络，$Q’(s, a|\theta^{Q’})$，它们用来计算目标Q值。目标网络 $\mu’(s|\theta^{\mu’})$ 和 $Q’(s, a|\theta^{Q’})$ 随着学习网络 $\mu(s|\theta^{\mu})$ 和 $Q(s, a|\theta^Q)$ 更新而更新，但是目标网络的更新幅度远小于学习网络： $\theta’ \leftarrow \tau \theta + (1 - \tau) \theta$， $\tau \ll 1$。这意味着目标网络变化缓慢，提高了目标值计算的稳定性。（不过应该没有彻底解决这一问题，毕竟还是目标网络用于目标值计算时也并非稳定的，只是变化缓慢而已。）但是降低目标网络的更新幅度，可能会让学习网络的更新变慢，也就是牺牲更新速度来提供收敛稳定性。不过文中提到，在实际中，学习的稳定性比学习的速度要重要得多。 另一个问题就是观测值包含不同元素(例如位置信息和速度信息)，这些元素包含不一样的物理量度，变化值范围不同，如果采用这些不同量度的元素，可能会导致网络学习效率变低，也很难在不同环境中泛化。因此需要将所有这些元素重新归一化到相同的范围。采用的方法是Ioffe 和 Szegedy 提出的batch normalization。这种方法会将每个mini-batch中的样本每个维度归一到相同的均值和方差。同时保留这个均值和方差，以用于测试的样本的归一化。对 $\mu$ 网络和 Q 网络的所有隐藏层和状态输入都进行归一化。 为了提高探索效率，采用的行动策略 $\mu’$是对目标策略 $\mu(s_t|\theta_t^\mu)$加上噪声 $\mathcal{N}$：$\mu’(s_t) = \mu(s_t|\theta_t^\mu) + \mathcal{N}$。论文中采用的噪声是Ornstein-Unlenbeck process（O-U过程）。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用HEXO搭建个人博客(Ubuntu 18)]]></title>
    <url>%2F2019%2F09%2F05%2F%E4%BD%BF%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[参考官方文档https://hexo.io/zh-cn/docs 1.安装nvm1wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh 将以下语句添加到.bashrc文件：12export NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm 开始安装1nvm install stable 2.安装HEXO1npm install -g hexo-cli 3.创建blog文件夹1234hexo init blog #通过hexo创建一个blog项目cd blog npm installhexo server #开发服务 4.配置git地址在blog项目根目录下里找到_config.yml文件，找到Deployment，然后按照如下修改：1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master 5.安装 hexo-deployer-git自动部署发布工具1npm install hexo-deployer-git --save 需要在博客目录下安装 6.生成静态文件部署到githubhexo clean &amp;&amp; hexo g &amp;&amp; hexo d 7.打开博客地址：yourname.github.io部署到github后可能得等一会才会生效，少则一分钟，多则半小时。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
