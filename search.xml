<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[学术网站汇总]]></title>
    <url>%2F2020%2F04%2F09%2F%E5%AD%A6%E6%9C%AF%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. 论文网站https://arxiv.org/ ：论文预印本发布网站，可以搜论文。 https://paperswithcode.com/ ：查找论文代码。 https://paperswithcode.com/sota ：查找领域的 state-of-art 论文 https://www.bibsonomy.org/ ：论文 bibtex 引用格式查询，论文发布链接查询 https://openreview.net/ ：查找论文作者的 review]]></content>
      <categories>
        <category>学习工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Batch-Constrained deep Q-learning (BCQ)]]></title>
    <url>%2F2020%2F04%2F07%2FBatch%20Constrained%20deep%20Q-learning%20(BCQ)%2F</url>
    <content type="text"><![CDATA[Fujimoto, S., Meger, D., &amp; Precup, D. (2019). Off-policy deep reinforcement learning without exploration. 36th International Conference on Machine Learning, ICML 2019, 2019-June, 3599–3609. 1. 论文概要本文解决的问题是 Batch reinforcement learning，批强化学习。这种学习方法是从一个固定的数据集中学习出一个策略，不需要和环境发生进一步的交互。当与环境交互起来费时费力时，批强化学习就是关键的解决方案。批强化学习是介于 Off-policy 强化学习和模仿学习之间的一种方法。 模仿学习的数据收集通过是第二个进程来完成，例如人亲自操作或者精细的编程来实现。假设收集过程中采用的策略是得当的，数据集的质量能得到保证，便可以直接应用模仿学习来从数据集中学习策略。但是大多数模仿学习在面对数据集中的不完整轨迹都会学习失败，或者还需要和环境进一步交互。 大多数 off-policy 强化学习都可以称为 growing batch learning。一边使用数据集（或者称为经验池）中的数据进行学习，一边和环境交互产生新的数据加入数据集。但是当数据集中的数据与当前策略的数据分布差距较大时，off-policy 就会学习失败。本文将 off-policy 存在的主要问题归结于 extrapolation error（推断误差），即错误地估计状态动作对的值，本质上来源于经验池中的数据分布和当前策略不一致。比如，如果经验池中没有选择某个动作的样本，off-policy 就不能学到选择了该动作的策略。 为了解决 off-policy 中的推断误差，本文提出了 Batch-Constrained deep Q-learning (BCQ) 算法，旨在于最大化奖励的同时缩小策略和经验池对状态动作对访问的差异。BCQ 算法的核心是利用 state-conditioned generative model （状态条件生成模型）来只产生先前见过的动作。将这个生成模型与 Q 网络结合，来选择与数据集分布最相似的价值最高的动作。本文还证明了这样的 batch-constrained条件，是要从不完整的数据集中学习有限、确定性 MDP 的值的无偏估计的必要条件。 与模仿学习和 off-policy 强化学习不同，BCQ 算法不再需要与环境进行更深的探索，能够从专家数据集或固定的 suboptimal 数据集中学习。 2. 相关知识2.1 Bellman operator $\mathcal{T}^\pi$对于给定的策略 $\pi$，动作值函数可以通过贝尔曼算子来计算： \mathcal{T}^\pi Q(s,a) = \mathbb{E}_{s'}[r + \gamma Q(s',\pi(s'))] \tag{1}对于 $\gamma \in 0,1)$，论文 [[1] 证明了贝尔曼算子是一个压缩映射，并存在唯一的不动点 $Q^\pi(s,a)$ 。 最优动作值函数表示为 $Q^*(s,a) = \max_\pi Q^\pi (s,a)$ 。 2.2 DQN &amp; Q-learning对于连续的状态空间，将动作值函数表示为神经网络 $Q_\theta(s,a)$，并运用 Q-learning 方法对动作值函数进行训练的方法称为 DQN 算法。$Q_\theta(s,a)$ 的训练目标是： y = r + \gamma Q_{\theta'}(s',\pi(s')), \;\; \pi(s') = \arg\max_a Q_{\theta'}(s',a) \tag{2} \label{2}其中 $Q_{\theta’}(s,a)$ 表示目标动作值网络，参数 $\theta’$ 更新采用当前网络参数 $\theta$ 的滑动平均：$\theta’\leftarrow \tau \theta + (1-\tau)\theta$ 。 Q-learning 是一个 off-policy 方法，负责采样的行为策略与学习的目标策略不同。原则上，off-policy 可以采用任意的行为策略。off-policy 方法的损失函数通常是最小化动作值网络估计与从经验回放池 $(s,a,r,s’) \in \mathcal{B}$ 采样的状态动作对的值的差异。 2.3 AC &amp; DDPG对于连续的动作空间，公式 $\eqref{2}$ 比较难以处理。AC 方法将策略也用神经网络表示 $\pi_\phi$ ，并通过另一个值网络的估计进行更新。策略网络称为 actor，值网络称为 critic 。 DDPG 是一种 AC 方法，同时运用 Q-learning 对值网络进行估计，目标函数可以表示为： \phi \leftarrow \arg\max_\phi \mathbb{E}_{s\in \mathcal{B}}[Q_\theta(s,\pi_\phi(s))] \tag{3}$Q_\theta$ 可以采用 DQN 方法进行训练。 3. 推断误差推断误差是指数据集的状态动作访问分布与当前策略的状态动作访问分布不一致。说白了就是不同策略采样的分布不同，在计算动作值函数的时候有关概率肯定有差别。 off-policy 对推断误差的处理是，当前策略（例如greedy）利用的数据全部来源于行动策略（例如epsilon-greedy）时，在贝尔曼方程迭代计算动作值函数的时候应该乘上一连串重要性采样的权重。Q-learning 通过技巧（下一步的 Q 的动作由当前策略（例如greedy）给出）。 推断误差主要来源于以下方面： 数据缺失。如果状态动作对 $(s,a)$ 没有在数据集中，这部分的价值就无法更新，此时误差来源于相似数据的价值和逼近误差。例如，对下一个状态的动作值估计 $Q_\theta(s’,\pi(s’))$ 可能与实际值相差很大，原因是数据集中没有与 $(s’,\pi(s’))$ 相近的数据。 模型偏差。由于数据集是有限的，每个状态动作对的访问也是有限的，因此其分布会带来了偏差，计算贝尔曼方程时，期望是针对数据集 $\mathcal{B}$ 的数据分布，而不是真实的 MDP： \mathcal{T}^\pi Q(s,a) \approx \mathbb{E}_{s'\sim \mathcal{B}} [r+\gamma Q(s',\pi(s'))] \tag{4} 训练不匹配。即便有足够的数据，从数据集采样时是均匀采样，训练的损失函数： y \approx \frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} \| r + \gamma Q_{\theta'}(s',\pi(s')) - Q_\theta(s,a) \|^2 \tag{5} \label{5}当数据集中的数据分布与当前策略的分布不匹配时，对动作值的估计就会有误差。 需要注意的是，即便将公式 $\eqref{5}$ 的权重换成与当前策略相关，如果当前策略最高价值的状态动作对不在数据集中，依然会带来误差。 当前很多 Off-policy 算法都没有考虑推断误差。很多 off-policy 方法能起作用，原因在于他们的数据集采样的都是最近策略的数据，和当前策略差别不大。但是，当面对数据集和当前策略数据分布很不一致的情况，就会表现糟糕。本文做了三个实验，设计了三种不同的数据集来测试 DDPG 面对数据集分布不匹配时的情况。 Final buffer 。训练一个 behavioral DDPG算法100万步（加入了高斯噪声来保证充分探索）并将遇到的 transition 全部储存在经验池，保证足够的覆盖率。 Concurrent 。在训练 behavioral DDPG（加入了高斯噪声来保证充分探索）的同时训练 off-policy 的一个 DDPG。它们俩都用 behavioral DDPG 采样的数据集训练。 Imitation 。一个训练好的 behavioral DDPG 采样 100 万步，不做任何探索。 最后，off-policy DDPG 在这三种数据集中的表现如下图所示，其中 True Value 是通过蒙特卡洛方法计算出来的： 可以看到，off-policy DDPG 表现都远差于 behavioral DDPG。即使对于 concurrent 任务，behavioral DDPG 和 off-policy DDPG 同时训练，两者也有很大的差距。这说明在稳定的状态分布下，初始策略的差异便可导致推断误差。在 final 任务中，off-policy 拥有覆盖面非常大的数据集，但是对值的估计依然不稳定，性能也很差。在 imitation 任务中，即使给出了专家数据集，off-policy 学到的都是非专家动作，导致值估计很快就发散了。 虽然推断误差不一定是正偏的，但当推断误差与强化学习算法中的最大化相结合时，推断误差提供了一个噪音源，可导致持续的高估计偏差。在 on-policy 下，推断误差导致的过高估计，策略就会采集一些 uncertainty 的数据，对这些错误的状态探索之后，值估计就会慢慢修正。但是 off-policy 是基于数据集的，这些误差将会很难被消除。 4. 算法推导当前的 off-policy 方法只管根据估计的动作值来选择动作，而不考虑动作值估计的准不准。如果在估计当前数据集中不存在的动作，将会带来很大的误差。但如果只选择那些在数据集中的动作空间，动作值就会估计得很准。 为了解决推断误差的问题，本文基于一个简单的思想：选择策略的时候要使得状态动作对的分布与 batch 的相似。满足这个要求的策略称为：batch-constrained 策略。batch-constrained 策略有以下三个目标： 最小化所选择的动作与 batch 中的数据的距离。 产生的状态数据是之前已经熟悉的。 最大化值函数。 其中目标 1 是最重要的。如果不限制在相关的 transition 下的话，值函数的估计就会很差，也就不能达到接下来的目标。因此本文在优化值函数的时候，引入了 future certainty 的衡量，同时加入距离的限制。本文的算法中是通过一个 state-conditioned generative model 来实现。该模型用一个网络来在一个小范围内最优地扰动生成的动作，并利用另一个 Q 网络来选择价值最高的动作。最后还利用了两个 Q 网络估计并取它们的 soft minimum 来作为价值估计的目标，目的是惩罚那些不熟悉的状态的值。 为了便于分析，先从有限的 MDP 和表格型环境出发，再引入到连续环境的 BCQ 算法。 4.1 有限 MDP 引入推断误差首先重写以下 Q-learning 的更新公式： Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha (r+\gamma Q(s',\pi(s'))) \tag{6}其中 $\pi(s’) = \arg\max_{a’} Q(s’,a’)$ 。 对于数据集 $\mathcal{B} $ 学习的 Q 函数，定义一个对应的 MDP 为 $M_\mathcal{B}$ 。真实的 MDP 记为 $M$ 。$M_\mathcal{B}$ 和 $M$ 具有相同的动作空间和状态空间，初始状态为 $s_{init}$，初始动作值记为 $Q(s,a)$。$M_\mathcal{B}$ 的转移概率 $p_\mathcal{B}(s’|s,a) = \frac{N(s,a,s’)}{\sum_{\tilde s N(s,a,\tilde s)}}$ ，其中 $N(s,a,s’)$ 表示在数据集 $\mathcal{B}$ 遇见 $N(s,a,s’)$ 的数量。如果 $\sum_{\tilde s} N(s,a,\tilde s) = 0$，则 $p_\mathcal{B}(s_{init}|s,a)=1$，并且 $r(s,a,s_{init})$ 被设置为 $Q(s,a)$ 的初始值。 定理 1： 从数据集 $\mathcal{B}$ 中采样进行 Q-learning 学习，在 $MDP_\mathcal{B}$ 下 Q-learning 最终会收敛到最优动作值函数。 定义 $\epsilon_{MDP}$ 为有限 MDP 下的推断误差，数学意义是根据数据集 $\mathcal{B} $ 计算出的 $Q_\mathcal{B}^\pi$ 和根据真实 MDP 计算出的 $Q^\pi$ 的差值： \epsilon_{MDP}(s,a) = Q^\pi(s,a) - Q_\mathcal{B}^\pi(s,a) \tag{7}$\epsilon_{MDP}(s,a)$ 可以重新写成贝尔曼方程的形式： \begin{aligned} \epsilon_{MDP}(s,a) &= \sum_{s'}(p_M(s'|s,a) - p_\mathcal{B}(s'|s,a)) \left( r(s,a,s')+\gamma\sum_{a'}\pi(a'|s')Q^\pi_{\mathcal{B}}(a',s') \right) \\ &+ \sum_{s'}p_M(s'|s,a) \gamma \sum_{a'} \pi(a'|s') \epsilon_{MDP}(s',a') \end{aligned} \tag{8} \label{8}这意味推断误差可以视为转移概率的差值的函数，其中动作值视为权重。如果选择一个策略使得两个转移概率的差距最小，那么推断误差也能达到最小。为了简便，重写公式 $\eqref{8}$ 如下： \epsilon_{MDP}^\pi = \sum_s \mu_\pi(s) \sum_a \pi(a|s) |\epsilon_{MDP}(s,a)| \tag{9}接下来推导消除推断误差的条件。 引理 1：对于所有的奖励函数，当且仅当对于所有的$s’\in S$ 、 $\mu_\pi(s) &gt; 0$、$\pi(a|s) &gt; 0$ ，满足$p_\mathcal{B}(s’|s,a) = p_M(s’|s,a)$，才能保证 $\epsilon_{MDP}^\pi = 0$ 。 引理 1 告诉我们如果 $M_\mathcal{B}$ 和 $M$ 有相同的转移概率，那么策略的价值就可以被正确地评估。这就意味着如果一个策略只经历在数据集中的 transition，那么价值就能被正确估计。 用数学语言来讲，定义一个 batch-constrained 策略 $\pi \in \Pi_\mathcal{B}$，其中 $\mu_\pi(s) &gt; 0$、$\pi(a|s) &gt; 0$ 并且所有的 $(s,a) \in \mathcal{B}$ 。同时定义一个 coherent 数据集 $\mathcal{B}$，满足所有的 $(s,a,s’) \in \mathcal{B}$ 并且 $s’ \in \mathcal{B}$， 除非 $s’$ 是结束状态。有了这样的数据集，就能保证 batch-constrained 策略的存在，它的价值就能被正确评估。 定理 2：对于确定性的 MDP 和任意的奖励函数，当且仅当策略 $\pi$ 是 batch-constrained 时，$\epsilon_{MDP}^\pi = 0$ 。另外，如果数据集 $\mathcal{B}$ 时 coherent 的，那么这样的策略一定存在，只要初始状态 $s_0 \in \mathcal{B}$ 。 将 batch-constrained 策略 Q-learning 结合形成 batch-constrained Q-learning (BCQL)，其更新公式为： Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha(r+\gamma \max_{a'\; s.t.\; (s',a') \in\mathcal{B}} Q(s',a')) \tag{10}定理 3：给定学习率 $\alpha$ 的 Robbins-Monro 随机收敛条件，以及对环境标准的采样，BCQL 可以收敛到最优动作值函数 $Q^*$ 。 定理 3 说明对于确定性的 MDP，以及 coherent 的数据集，BCQL 能够收敛到最优策略 $\pi^ \in \Pi_\mathcal{B}$，对所有 $\pi \in \Pi_\mathcal{B}$ 和 $(s,a) \in \mathcal{B}$ 满足 $Q^{\pi^} \ge Q^\pi(s,a)$ 。 定理 4：给定确定性 MDP 和 coherent 数据集 $\mathcal{B}$，连同学习率 $\alpha$ 的 Robbins-Monro 随机收敛条件，BCQL 将会收敛到 $Q_\mathcal{B}^\pi(s,a)$，其中 $\pi^*(s) = \arg\max_{a \; s.t. \; (s,a) \in \mathcal{B}} Q^\pi_\mathcal{B}(s,a)$ 是最优 batch-constrained 策略。 定理 4 说明 BCQL 能保证学习的策略优于任何的行动策略。与标准的 Q-learning 不同的是，BCQL 没有对状态动作对访问上的要求，只要求数据集是 coherent 的。 4.2 BCQ 算法将 BCQL 算法拓展到连续环境，本文提出了 BCQ 算法。其中为了满足 Batch-constrained 的条件，BCQ 利用了一个生成模型。对于给定的状态，BCQ 利用生成模型来生成与 batch 相似的动作集合，并通过 Q 网络来选择价值最高的动作。另外，本文还对价值估计过程增加了对未来稀有的状态进行惩罚，与 Clipped Double Q-learning 算法 [2] 类似。最后，BCQ 能学到与数据集的状态动作对访问分布相似的策略。 给定状态 $s$，本文将 $(s,a) $ 和数据集 $\mathcal{B}$ 中的状态动作对的相似度建模成条件概率 $P_\mathcal{B}^G(a|s)$ 。为了估计 $P_\mathcal{B}^G(a|s)$ ，本文采用了参数化的生成模型 $G_\omega(s)$，从中采样动作来逼近 $\arg\max_a P_\mathcal{B}^G(a|s)$ 。 对于生成模型，本文采用了变分自动编码器（conditional variational auto-encoder, VAE）。为了提高生成动作的多样性，本文引入了扰动模型 $\xi_\phi(s,a,\Phi)$，对动作 $a$ 的扰动范围是 $[-\Phi,\Phi]$ 。此时策略可以表示为： \pi(s) = \arg\max_{a_i+\xi_\phi(s,a_i,\Phi)} Q_\theta(s,a_i+\xi_\phi(s,a_i,\Phi)), \;\; \{a_i \sim G_\omega(s)\}_{i=1}^n \tag{11}参数 $n$ 和 $\Phi$ 的选择让 BCQ 算法介于模仿学习和强化学习之间。当 $\Phi=0$ 且 $n=1$ 时，BCQ 就类似于模仿学习；当 $\Phi=a_\max-a_\min$ 且 $n \rightarrow \infty$ 时，BCQ 算法就类似于 Q-learning 算法。 扰动模型 $\xi_\phi$ 的训练和 DDPG 算法的训练目标类似： \phi \leftarrow \mathop{\arg\max}_\phi \sum_{(s,a)\in\mathcal{B}} Q_\theta(s,a+\xi_\phi(s,a,\Phi)) \tag{12}为了对未来一些不常见的状态进行惩罚，本文采用 Clipped Double Q-learning 算法 [2] 对动作值 $Q$ 进行估计，也就是训练两个动作值网络 $\{Q_{\theta_1},Q_{\theta_2}\}$，取它们的最小值作为动作值的估计。本文对 Clipped Double Q-learning 算法进行了一点改进，对两个动作值采用新的结合方式： y = r+\gamma \max_{a_i} \left[ \lambda \min_{j=1,2} Q_{\theta'_j}(s',a_i) + (1-\lambda) \max_{j=1,2}Q_{\theta'_j}(s',a_i) \right] \tag{13}其中超参数 $\lambda$ 是控制对不确定的状态的惩罚力度。当 $\lambda = 1$ 时，公式 $\eqref{13}$ 其实是另一篇论文 Clipped Double Q-learning 的做法 [2] ，也是本文的一作提出来的。 算法和一般的 DQN 相比，BCQ 使用了两个 Q 网络和两个目标 Q 网络，一个扰动网络 $\xi_\phi$ 和对应的目标扰动网络以及一个 VAE 生成模型 $G_\omega(s )$。在训练时，首先从数据集中采样 transition，然后训练 VAE 模型，生成和 batch 中相似的 action，然后从生成器中采样，用扰动网络扰动动作的取值。最后更新 Q 网络和目标 Q 网络。 5. 工程设置参考文献[1] Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Pro- gramming. Athena scientific Belmont, MA, 1996. [2] Fujimoto, S., van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. In Inter- national Conference on Machine Learning, volume 80, pp. 1587–1596. PMLR, 2018.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Q-learning</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trust Region Path Consistency Learning (Trust-PCL)]]></title>
    <url>%2F2020%2F04%2F04%2FTrust%20Region%20Path%20Consistency%20Learning%20(Trust-PCL)%2F</url>
    <content type="text"><![CDATA[Nachum, O., Norouzi, M., Xu, K., &amp; Schuurmans, D. (2018). TruST-PCL: An off-policy trust region method for continuous control. 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, 1–14. 1. 论文概述TRPO 在连续空间的强化学习上获得了成功，提高了基于策略梯度强化学习方法的稳定性和收敛速度。但是，当前的基于置信域优化的方法需要和环境进行大量的 on-policy 交互，样本效率很低。DDPG 是成功运用在连续环境中的 off-policy 强化学习方法，但是 DDPG 牺牲了稳定性来提升了样本效率，同时非常依赖于超参数的选取。 本文为了解决 on-policy 置信域方法的样本效率问题，提出了 Trust-PCL 方法。Trust-PCL 方法是基于 PCL 方法 [1] 的路径一致性原理，即在任意连续的子轨迹中，状态值和熵正则化的最优策略间满足一致性关系。Trust-PCL 方法引入了相对熵正则化器，保证了优化的稳定性。本文得到一个结论是，向最大奖励的强化学习目标引入相对熵正则化器之后，状态值和熵正则化的最优策略间依然对任意的轨迹满足一致性关系。 此外，本文提出了一种简单的方法来确定相对熵调整器的系数，使之不受奖励尺度的影响。 2. 相关知识梯度下降是神经网络优化的主要方法。不过，梯度下降其实也是一种置信域约束优化的形式： \mathscr{minimize} \;\; \ell(\theta+d\theta) \approx \ell(\theta) + \nabla \ell(\theta)^T d\theta \;\;\;\; s.t. \;\; d\theta^Td\theta \le \epsilon \tag{1}其中局部最优化表示为 $d\theta = \eta \nabla \ell(\theta)$，解得 $\eta = \sqrt{\epsilon} / | \nabla \ell(\theta) |$ 。实质上，梯度下降是考虑参数位于欧几里得空间。 但是对于机器学习中的多层网络，欧几里得几何实际上并不是最佳描述参数距离的空间。 通常更有效的是采用黎曼度量，这样能在局部区域中找到更陡峭的梯度方向（简单理解就是，曲线空间的两点，最短距离不是直线，而是曲线）。 如果将损失函数定义为模型参数和最优参数间的布雷格曼散度（Bregman divergence），置信域的形式通常会写成： \mathscr{minimize} \;\; D_F(\theta^*,\theta+d\theta) \;\;\;\; s.t. \;\; D_F(\theta,\theta+d\theta) \le \epsilon \tag{2}$F$ 函数的选择不同，布雷格曼散度的形式就不同。 自然梯度利用费舍尔矩阵 $F(\theta)$ 来定义参数空间。这样损失函数实际上是 KL 散度 $D_{KL}(\theta^*,\theta)$，置信域优化可以写成： \mathscr{minimize} \;\; D_{KL}(\theta^*,\theta+d\theta) \;\;\;\; s.t. \;\; D_{KL}(\theta,\theta+d\theta) \approx d\theta^T F(\theta) d\theta \le \epsilon \tag{3} \label{3}其中梯度方向：$d\theta \equiv -\eta F(\theta)^{-1}\nabla \ell(\theta)$ 。自然梯度对 KL 散度的近似形式为：$D_{KL}(a,b) \approx (a-b)^TF(a)(a-b)$，该近似是二阶泰勒展开。 紧接着，TRPO [2] 对自然梯度的逼近方法进行改善，将自然梯度方法引入到强化学习非线性策略的优化中。另外，最近端优化方法将置信域优化中的硬性约束替换为目标函数中的惩罚项，PPO [3] 算法就是将最近端优化方法应用于强化学习策略的学习中。 论文 [4] 中将熵正则化的奖励目标函数表示成反向的 KL 散度 $D_{KL}(\theta,\theta^*)$ 。也就是说，在这种正则化的奖励目标下，公式 $\eqref{3}$ 应该写成： \mathscr{minimize} \;\; D_{KL}(\theta+d\theta,\theta^*) \;\;\;\; s.t. \;\; D_{KL}(\theta+d\theta,\theta) \approx d\theta^T F(\theta+d\theta) d\theta \le \epsilon \tag{4}但是问题在于此时费舍尔矩阵 $F(\theta+d\theta)$ 的求解依赖于更新后的参数，虽然可以用 $F(\theta)$ 进行近似，但对于较大的 $d\theta$ 而言则不太理想。 本文给出了反向 KL 散度 $D_{KL}(\theta+d\theta,\theta) \le \epsilon$ 的另一种解法，也就成功将熵正则化框架和置信域策略优化结合起来。 下面给出相关算法的符号定义和目标公式。 2.1 最大化期望奖励标准强化学习的目标就是最大化期望未来折扣奖励。在每个状态的基础上，对该目标递归表示为： O_{ER}(s,\pi) = \mathbb{E}_{a,r,s'}[r + \gamma O_{ER}(s',\pi)] \tag{5}当从与环境的交互中对状态进行采样时，与状态无关的目标是每个状态的目标函数的期望： O_{ER}(\pi) = \mathbb{E}_s[O_{ER}(s,\pi)] \tag{6}2.2 Path Consistency LearningPCL 是本文算法的前身，也是一个引入熵正则化框架的强化学习方法。带熵正则化的强化学习目标为： O_{ENT}(s,\pi) = O_{ER}(s,\pi) + \tau \mathbb{H}(s,\pi) \tag{7} \label{7}折扣熵 $\mathbb{H}(s,\pi)$ 的递归表示形式为： \mathbb{H}(s,\pi) = \mathbb{E}_{a,s'} [-\log \pi(a|s)+\gamma \mathbb{H}(s',\pi)] \tag{8} \label{8}结合公式 $\eqref{7}$ 和公式 $\eqref{8}$，带熵正则化的强化学习目标 $O_{ENT}(s,\pi)$ 可以重新表示为递归形式： O_{ENT}(s,\pi) = \mathbb{E}_{a,r,s'}[r - \tau \log \pi(a|s) + \gamma O_{ENT}(s',\pi)] \tag{9} \label{9}PCL 的论文 [1] 中得到了对于任意路径（轨迹）的最优状态值和最优策略的一致性关系： V^*(s_0) = \mathbb{E}_{r_i,s_i} \left[ \gamma^d V^*(s_d) + \sum_{i=0}^{d-1} \gamma^i (r_i-\tau \log \pi^*(a_i|s_i)) \right] \tag{10}其中 $\pi^$ 是最大化目标 $O_{ENT}(s)$ 的最优策略，最优状态值函数 $V^(s) = O_{ENT}(s,\pi^*)$ 。PCL 的目标就是同时优化 $\pi_\theta$ 和 $V_\phi$ 的参数，减小等式两边的误差。 2.3 Trust Region Policy Optimization采用标准的策略梯度算法对标准强化学习目标 $O_{ER}$ 进行优化是很不稳定的，TRPO [2] 通过置信域优化的方法来限制策略更新的幅度，表示为一个带约束的优化问题： \mathop{\mathscr{maximize}}_\pi O_{ER}(\pi) \;\;\;\; s.t. \;\;\;\; \mathbb{E}_{s \sim \tilde{\pi},\rho}[KL(\tilde \pi(\cdot|s) \| \pi(\cdot|s) )] \le \epsilon \tag{11}3. 算法推导为了更稳定地对参数空间进行训练，本文对带熵正则化的强化学习目标 $O_{ENT}(s,\pi)$ 进行扩展，引入与更新前的策略（也可以称为目标策略）相关的折扣相对熵置信域： \mathop{\mathscr{maximize}}_\pi \mathbb{E}_s[O_{ENT}(\pi)] \;\;\;\; s.t. \;\;\;\; \mathbb{E}_s[\mathbb{G}(s,\pi,\tilde \pi)] \le \epsilon \tag{12}其中折扣相对熵 $\mathbb{G}(s,\pi,\tilde \pi)$ 的递归表达式为： \mathbb{G}({s,\pi, \tilde \pi}) = \mathbb{E}_{a,s'} [\log \pi(a|s) - \log \tilde \pi(a|s) + \gamma \mathbb{G}(s',\pi,\tilde \pi)] \tag{13} \label{13}尽管之前的强化学习方法已经单独与熵正则化和相对熵结合，但是同时让强化学习同时与熵正则化和相对熵结合还是第一次，也是本文一个亮点。熵正则化可以提高探索率，而引入相对熵置信域能保障训练的稳定性和更快的训练速度。 对公式 $\eqref{13}$ 用拉格朗日乘法表示，现在带约束的最优化问题转变为不带约束的最大化问题： O_{RELENT}(s,\pi) = O_{ENT}(s,\pi) - \lambda \mathbb{G}(s,\pi,\tilde \pi) \tag{14} \label{14}与状态无关的目标通过对每个状态的目标函数取期望得到： O_{RELENT}(\pi) = \mathbb{E}_s [O_{RELENT}(s,\pi)] \tag{15}3.1 引入相对熵的 PCL引入相对熵后的目标 $O_{RELENT}$ 和熵正则化目标 $O_{ENT}$ 其实有相似的结构，可以将 $O_{RELENT}$ 也表示成熵正则化目标的形式，只不过此时奖励函数的形式发生了一点变化。 O_{RELENT}(s,\pi) = \tilde O_{ER}(s,\pi) + (\tau + \lambda) \mathbb{H}(s,\pi) \tag{16}其中 $\tilde O_{ER}(s,\pi)$ 表示标准的强化学习目标（头上带有波浪线表示发生了一点转换）。但是奖励函数的形式发生了一点变换：$\tilde r(s,a) = r(s,a) + \lambda \log \tilde \pi(a|s)$ 。 由于 $O_{RELENT}(\pi)$ 的形式与 $O_{ENT}(\pi)$ 相似，可以直接引用 PCL 论文 [1] 中的推导结果。最优策略 $\pi^*$ 的表达式如下： \pi^*(a_t|s_t) = \exp \left\{ \frac{\mathbb{E}_{\tilde r_t \sim \tilde r(s_t,a_t), s_{t+1}} [\tilde r_t + \gamma V^*(s_{t+1})] - V^*(s_t)}{\tau+\lambda} \right\} \tag{17} \label{17}softmax 最优值函数表示为： V^*(s_t) = (\tau+\lambda) \log \int_\mathcal{A} \exp \left\{ \frac{\mathbb{E}_{\tilde r_t \sim \tilde r(s_t,a_t), s_{t+1}} [\tilde r_t + \gamma V^*(s_{t+1})]}{\tau+\lambda} \right\} da \tag{18}对公式 $\eqref{17}$ 重新整理，可以得到最优策略 $\pi^$ 和最优值函数 $V^$ 的一致性关系： \begin{aligned} V^*(s_t) &= \mathbb{E}_{\tilde r_t \sim \tilde r(s_t,a_t), s_{t+1}} [\tilde r_t - (\tau+\lambda) \log \pi^*(a_t|s_t) + \gamma V^*(s_{t+1})] \\ &= \mathbb{E}_{r_t,s_{t+1}} [r_t - (\tau+\lambda) \log \pi^*(a_t|s_t) + \lambda \log \tilde \pi(a_t|s_t) + \gamma V^*(s_{t+1})] \end{aligned} \tag{19} \label{19}公式 $\eqref{19}$ 是 one-step 的一致性关系，因为 PCL 对任意路径都成立，所以可以扩展成 multi-step 的形式： V^*(s_t) = \mathbb{E}_{r_{t+i},s_{t+i}} \left[\gamma^d V^*(s_{t+d}) + \sum_{i=0}^{d-1} \gamma^i (r_{t+i} - (\tau+\lambda) \log \pi^*(a_{t+i}|s_{t+i}) + \lambda \log \tilde \pi(a_{t+i}|s_{t+i})) \right] \tag{20}3.2 Trust-PCL和 PCL 的目标函数一样，为了训练参数化策略 $\pi_\theta$ 和参数化状态函数 $V_\phi$，构造对于采样轨迹 $s_{t:t+d} \equiv (s_t,a_t,r_t,\cdots,s_{t+d-1},a_{t+d-1},r_{t+d-1},s_{t+d})$ 的一致性误差函数为： C(s_{t:t+d},\theta,\phi) = -V_\phi(s_t) + \gamma^d V_\phi(s_{t+d}) + \sum_{i=0}^{d-1} \gamma^i \left(r_{t+i} - (\tau+\lambda) \log \pi_\theta(a_{t+i}|s_{t+i}) + \lambda \log \pi_{\tilde \theta}(a_{t+i}|s_{t+i}) \right) \tag{21}对于一个 mini-batch 轨迹集合 $S = \{ s_{0:T_k}^{(k)} \}_{k=1}^B$，损失函数为： \mathcal{L}(S,\theta,\phi) = \sum_{k=1}^B \sum_{t=0}^{T_k-1} C(s_{t:t+d}^{(k)},\theta,\phi)^2 \tag{22}训练时，采用的是 off-policy 方法，即从经验池中采样轨迹集合 $S$。每个样本加入经验池时都带有自己的优先级，该优先级与 PCL 论文中的定义相似： P(s_{0:T}) = 0.1/B + 0.9 \exp(\beta \sum_{i=0}^{T-1} r(s_i,a_i)) / Z \tag{23}其中 $B$ 表示经验池的大小，$Z$ 表示归一化因子。$\beta$ 是可调节的超参数。 对于更新前的策略（或称为目标策略） $\pi_{\tilde \theta}$ 的参数更新，采用的更新方式为 $\tilde \theta = \alpha \tilde \theta + (1-\alpha) \theta$ 。 3.3 拉格朗日乘数 $\lambda$如果将相对熵项视为惩罚项而不是约束项，如公式 $\eqref{14}$ 所示，那么拉格朗日乘数 $\lambda$ 将成为一个超参数，这个超参数的调节有很大的困难，实际上在训练过程中，超参数是需要随训练进程而变化的。因此将相对熵视为约束项可能是更好的选择，TRPO 中也是视为约束项。 本文提出了一种新的方法，将超参数从 $\lambda$ 重定向为超参数 $\epsilon$ 。给定由超参数 $\epsilon$ 定义的约束项，这个方法可以得到近似惩罚项 $\lambda(\epsilon)$ ，此时 $\lambda$ 不再是超参数，而是关于 $\epsilon$ 的函数，或者说一个自动调节的超参数。这也是本文的亮点之一。 为了便于分析，本文进行了一些限制，例如令 $\gamma = 1$，$\tau = 0$。另一个限制就是从单独的初状态 $s_0$ 开始求解 KL 散度与真实的 KL 散度相差不大。实际实验中不会进行限制，但是结果表现得一样好。 在这种限制下，对于完整的一个 episode $s_{0:T}=(s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1},s_T)$，由公式 $\eqref{17}$ 可以得到： \pi^*(s_{0:T}) \propto \tilde \pi(s_{0:T}) \exp \left\{ \frac{R(s_{0:T})}{\lambda} \right\} \tag{24} \label{24}其中 $\pi(s_{0:T}) = \prod_{i=0}^{T-1} \pi(a_i|s_i)$ ，$R(s_{0:T}) = \sum_{i=0}^{T-1} r_i$ 。$\pi^*$ 的归一化因子为： Z = \mathbb{E}_{s_{0:T}\sim \tilde \pi} \left[ \exp \left\{ \frac{R(s_{0:T})}{\lambda} \right\} \right] \tag{25} \label{25}公式 $\eqref{24}$ 和 $\eqref{25}$ 的推导其实并不怎么严格，很大程度依赖于假设。接下来利用这两条公式得到 $\pi^*$ 和 $\tilde \pi$ 的近似 KL 散度。 \begin{aligned} KL(\pi^*\| \tilde \pi) &= \mathbb{E}_{s_{0:T}\sim \pi^*} \left[ \log \left( \frac{\pi^*(s_{0:T})}{\tilde \pi(s_{0:T})} \right) \right] \\ &= \mathbb{E}_{s_{0:T}\sim \pi^*} \left[ \frac{R(s_{0:T})}{\lambda} - \log Z \right] \\ &= -\log Z + \mathbb{E}_{s_{0:T} \sim \tilde \pi} \left[ \frac{R(s_{0:T})}{\lambda} \cdot \frac{\pi^*(s_{0:T})}{\tilde \pi(s_{0:T})} \right] \\ &= -\log Z + \mathbb{E}_{s_0:T\sim \tilde \pi} \left[ \frac{R(s_{0:T})}{\lambda} \exp\{ R(s_{0:T}/\lambda - \log Z) \} \right] \end{aligned} \tag{26} \label{26}公式 $\eqref{24}$ 中的期望只与 $\tilde \pi$ 有关，因此可以通过对策略 $\tilde \pi$ 进行采样计算。有了 $\tilde \pi$ 采样的轨迹，给定的超参数 $\epsilon$ ，为了得到最优的 $\lambda$，只需要执行线性搜索，使得 $KL(\pi^*|\pi)$ 尽量靠近 $\epsilon$ 即可。 但是还有一个问题，就是 $\epsilon$ 是否应该随着训练的进程改变。考虑到随着训练的进行，一个 episode 的长度可能会变长，此时 $KL(\pi^*|\tilde \pi)$ 也会变大，相反也一样。为了同等地改变相对熵的约束范围，利用多个 episode 采样轨迹的集合 $S= \{s_{0:T_k}^{(k)} \}^N_{k=1}$， $\epsilon$ 可以修订为：$\frac{\epsilon}{N} \sum_{k=1}^N T_k$ 。$T_k$ 就表示第 $k$ 个 episode 的长度。不过为了避免和环境进行过多的交互，本文对这个集合 $S$ 取最后 100 个 episode 来计算平均 episode 长度。 3.4 伪代码 参考文献参考文献[1] Nachum, O., Norouzi, M., Xu, K., &amp; Schuurmans, D. (2017). Bridging the gap between value and policy based reinforcement learning. Advances in Neural Information Processing Systems, 2017-Decem(Nips), 2776–2786. [2] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015. [3] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. [4] Norouzi, Mohammad, et al. “Reward augmented maximum likelihood for neural structured prediction.” Advances In Neural Information Processing Systems. 2016.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC</tag>
        <tag>Trust Region</tag>
        <tag>PCL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Path Consistency Learning (PCL)]]></title>
    <url>%2F2020%2F03%2F27%2FPath%20Consistency%20Learning%20(PCL)%2F</url>
    <content type="text"><![CDATA[Nachum, O., Norouzi, M., Xu, K., &amp; Schuurmans, D. (2017). Bridging the gap between value and policy based reinforcement learning. Advances in Neural Information Processing Systems, 2017-Decem(Nips), 2776–2786. 1. 论文概述本文基于 softmax temporal value consistency 和熵正则化下的策略最优性，建立一个新的强化学习中值和策略的联系。具体指的是，本文展示了 softmax consistent action values 对应于最优熵正则化策略概率的推论。根据这个推论，本文提出了一个新的强化学习算法，Path Consistency Learning (PCL)，旨在于最小化动作样本轨迹的 soft consistency error，且无论动作轨迹是 on-policy 还是 off-policy 产生。 基于策略梯度的方法主要问题是样本效率，因为策略梯度是从 rollout 中估计的，。尽管可以使用一些几何上的方法，比如 Natural gradient，Relative entropy policy，Trust region 等方法，但方差控制问题依然很大。AC 一类方法可以通过 critic 代替 rollout，牺牲一些偏差来降低方差，但即便是这样，因为很多 AC 方法采用的是 on-policy，依然存在样本效率低的问题。另外基于值的方法，比如 Q-learning 算法，可以从环境采样的动作轨迹进行学习，这一类 off-policy 方法的样本效率比 on-policy 要高。但是这类算法的主要问题是不能和估计函数稳定地交互，调节超参数比较困难。 理想上，我们想将 on-policy 的无偏性和稳定性与 off-policy 的样本高效性结合起来。这个思路造就了一系列 off-policy AC 方法，比如 Q-Prop，DDPG，A3C 等方法。尽管它们相对于 on-policy AC 方法有所提高，但是他们仍旧没有解决在函数逼近下与 off-policy 学习相关的理论困难。 本文探索了熵正则化框架下策略优化和 softmax value consistency 的关系，本文称为 path consistency，路径一致性。利用这个观察结论，本文设计了一款性能稳定的 off-policy AC 学习方法，并说明如何将 actor 和 critic 统一在一个模型中，而不是用两个独立的模型表示。 2. 相关知识为了便于推导，本文对状态转移进行一些限定，假设状态转移函数是确定的，不是随机的。每步的奖励和下个状态可以分别表示为 $r_t = r(s_t,a_t)$ 和 $s_{t+1} = f(s_t,a_t)$ ；实际上对于随机的状态转移函数也成立，本文的附录 C 中有证明。以下公式的某些符号可能与之前论文中所用的符号不太一致。 没有熵正则化的目标函数如下，也就是贝尔曼方程： O_{ER}(s,\pi) = \sum_a \pi(a|s) [r(s,a) + \gamma O_{ER}(s',\pi)], \;\; where \;\; s' = f(s,a) \tag{1} \label{1}$O_{ER}$ 就是期望折扣奖励值。 定义 $V^\circ$ 是在状态 $s$ 下，所有策略最大的 $O_{ER}(s,\pi)$ 值，也就是：$V^\circ = \max_\pi O_{ER}(s,\pi)$。定义 $\pi^\circ$ 为满足 $V^\circ$ 的最优策略，即：$\pi^\circ = \arg\max_\pi O_{ER}(s,\pi)$ 。这个最优策略是关于动作的 one-hot 分布，因此可以得到： V^\circ = O_{ER}(s,\pi^\circ) = \max_a (r(s,a) + \gamma V^\circ(s')) \tag{2} \label{2}公式 $\eqref{2}$ 称为 hard-max Bellman temporal consistency。这里的 hard-max 说的是，下一步让目标函数最大化的动作选择只能选择一个，非黑即白。同样也可以写出更常见的 $Q^\circ$ 版本的公式： Q^\circ(s,a) = r(s,a) + \gamma \max_{a'} Q^\circ(s',a') \tag{3}以上说明了 hard-max 版本的 temporal consistency，接下来就到了 softmax 版本的诞生了。这和熵正则化的引入有关。考虑带有熵正则化的目标函数如下： O_{ENT}(s,\pi) = O_{ER}(s,\pi) + \tau \mathbb{H}(s,\pi) \tag{4} \label{4}其中 $\tau \ge 0$ 代表温度项，控制折扣熵正则化的程度。 定义折扣熵的递归公式如下： \mathbb{H}(s,\pi) = \sum_a \pi(a|s) [-\log \pi(a|s) + \gamma \mathbb{H}(s',\pi)] \tag{5} \label{5}其中公式 $\eqref{5}$ 的第一项就是熵的定义，第二项是递归项，递归项主要是为了方便写成带熵正则化的贝尔曼方程。联合公式 $\eqref{1} $，$\eqref{4}$ 和 $\eqref{5}$ ，可以得到带熵正则话的贝尔曼方程如下： O_{ENT}(s,\pi) = \sum_a \pi(a|s) [r(s,a) - \tau \log \pi(a|s) + \gamma O_{ENT}(s',\pi)] \tag{6} \label{6}接下来令 $V^(s) = \max_\pi O_{ENT}(s,\pi)$ 表示在状态 $s$ 下的最优值函数。同时令 $\pi^(a|s)$ 为在状态 $s$ 下获取到最大 $O_{ENT}(s,\pi)$ 值的最优策略。因为引入了熵，此时最佳策略不再是关于动作的 one-hot 分布， 毕竟熵项趋于使用不确定性更大的策略。目标函数中引入熵使得策略变得更加不确定，这就是从 hard-max 转变为 soft-max 名称的由来。 将 softmax 最优策略 $\pi^(a|s)$ 用 softmax 最优值函数 $V^(s)$ 表示如下： \pi^*(a|s) \propto \exp \{ (r(s,a)+\gamma V^*(s'))/\tau \} \tag{7} \label{7}公式 $\eqref{7}$ 是玻尔兹曼分布的形式，其完整的表达式为： \pi^*(a|s) = \frac{\exp \{ (r(s,a)+\gamma V^*(s'))/\tau \}}{\sum_a \exp \{ (r(s,a)+\gamma V^*(s'))/\tau \}} \tag{8} \label{8}分子称为配分函数。公式 $\eqref{8}$ 的验证可以参考 soft Q-learning 论文附录 A.1 中的公式 $(19)$，主要是通过构造 $\pi$ 和 $\pi^$ 的负 KL 散度来证明只有 $\pi = \pi^$ 时，目标函数取得最大值。 将公式 $\eqref{8}$ 代入公式 $\eqref{6}$ ，可以得到 softmax 最优值函数的表达式： V^*(s) = O_{ENT}(s,\pi^*) = \tau \log \sum_a \exp \{ (r(s,a) + \gamma V^*(s')) / \tau \} \tag{9} \label{9}这里的 softmax 表示 log-sum-exp 形式，不是神经网络中的 softmax。soft Q-learning 论文中也称为 soft 最优状态值函数。当 $\tau \rightarrow 0$ 时，公式 $\eqref{9} $ 恢复为公式 $\eqref{2} $ 的形式。 同样，可以写出 softmax 最优动作值函数的形式： Q^*(s,a) = r(s,a) + \gamma V^*(s') = r(s,a) + \gamma \tau \log \sum_{a'} \exp(Q^*(s',a')/\tau) \tag{10} \label{10}3. 算法推导接下来是本文的重点推导，根据以下的结论得出 PCL 算法。 注意到公式 $\eqref{8}$ 可以写成： \pi^*(a|s) = \frac{\exp \{ (r(s,a)+\gamma V^*(s'))/\tau \}}{\exp\{ V^*(s)/ \tau \}} \tag{11} \label{11}对公式 $\eqref{11}$ 两边同时取对数，可以得到定理 1。 定理 1：对于 $\tau &gt; 0$，最大化 $O_{ENT}$ 的最优策略 $\pi^$ 和最优值函数 $V^(s) = \max_\pi O_{ENT}(s,\pi)$ 对于任意的 $s$ 和 $a$ $(s’=f(s,a))$ 都满足以下的时序一致性关系： V^*(s) - \gamma V^*(s') = r(s,a) - \tau \log \pi^*(a|s) \tag{12} \label{12}注意定理 1 适用于环境的状态转移是确定的情况，即 $s’$ 对于确定的 $s$ 和 $a$ 是唯一的。更通用的形式在本文的附录 C 中有证明。联合公式 $ \eqref{10}$ 和 公式 $\eqref{12}$，可以得到 $\pi^$ 和 $Q^$ 的关系： \pi^*(a|s) = \exp\{ (Q^*(s,a) - V^*(s))/\tau \} \tag{13} \label{13}定理 1 可以从 one-step 扩充到 multi-step，很容易得到以下 multi-step 时序一致性关系的引理： 引理 2：对于 $\tau &gt; 0$，$\pi^$ 和 $V^$ 对于任意的 $s_1$ 和动作序列 $a_1,\cdots,a_{t+1}$ $(s_{i+1} = f(s_i,a))$ 都满足以下的扩展时序一致性关系： V^*(s_1) - \gamma^{t-1} V^*(s_t) = \sum_{i=1}^{t-1} \gamma^{i-1} [r(s_i,a_i) - \tau \log \pi^*(a_i|s_i)] \tag{14} \label{14}定理 1 反过来也成立。 定理 3：如果一个策略 $\pi(a|s)$ 和状态值函数 $V(s)$ 对于所有的 $s$ 和 $a$ $(s’=f(s,a))$ 都满足公式 $\eqref{12}$，那么 $\pi = \pi^$，$V=V^$ 。 定理 3 告诉我们，通过最小化公式 $\eqref{12}$ 和公式 $\eqref{14}$ 两边的差异，就可以学习参数化的策略和值函数估计。 4. 具体算法将参数化策略表示为 $\pi_\theta$，参数化值函数表示为 $V_\phi$，根据公式 $\eqref{14}$，对于一个长度为 d 的子轨迹：$s_{i:i+d} \equiv (s_i,a_i,\cdots,s_{i+d-1},a_{i+d-1},s_{i+d})$，可以定义满足 soft 时序一致性的目标函数如下： C(s_{i:i+d},\theta,\phi) = -V_\phi(s_i) + \gamma^d V_\phi(s_{i+d}) + \sum_{j=0}^{d-1} \gamma^j [r(s_{i+j},a_{i+j}) - \tau \log \pi_\theta(a_{i+j}|s_{i+j})] \tag{15} \label{15}学习的目标是找到 $\pi_\theta$ 和 $V_\phi$ 使得 $C(s_{i:i+d},\theta,\phi)$ 尽可能接近 0 。 这个算法就称为 Path Consistency Learning (PCL)，损失函数为： O_{PCL}(\theta,\phi) = \sum_{s_{i:i+d} \in E} \frac{1}{2} C(s_{i:i+d},\theta,\phi)^2 \tag{16} \label{16}其中 $E$ 代表子轨迹集合。对公式 $\eqref{16}$ 进行求导可以得到： \Delta \theta = \eta_\pi C(s_{i:i+d},\theta,\phi) \sum_{j=0}^{d-1} \gamma^j \nabla_\theta \log \pi_\theta (a_{i+j}|s_{i+j}) \tag{17} \label{17} \Delta \phi = \eta_v C(s_{i:i+d},\theta,\phi) \left( \nabla_\phi V_\phi(s_i) - \gamma^d \nabla_\phi V_\phi(s_i+d) \right) \tag{18} \label{18}其中 $\eta_\pi$ 和 $\eta_v$ 表示各自的学习率。由于一致性关系适用于任何轨迹，PCL 算法利用公式 $\eqref{17}$ 和公式 $\eqref{18}$ 既可以对 on-policy 采样的轨迹进行学习，也可以对从经验池采样的轨迹进行学习。 最终 PCL 采用 on-policy 和 off-policy 结合，伪代码如下： 注意存入经验池的不是单独的状态和动作对，而是一段子轨迹 $s_{0:T} \sim \pi_\theta(s_{0:})$ 。另外每个轨迹加入经验池时都有自己的优先级，主要与奖励值有关，本文设计的子轨迹优先级表达式如下： P(s_{0:T}) = 0.1/B + 0.9 \exp(\alpha \sum_{i=0}^{T-1} r(s_i,a_i)) / Z \tag{19}其中 $B$ 表示经验池的大小，$Z$ 表示归一化因子。$\alpha$ 是可调节的超参数。 4.1 Unified PCL根据公式 $\eqref{9}$、$\eqref{10}$ 和 $\eqref{13}$，可以策略和值函数估计都用动作值函数估计 $Q_\rho$ 来表示： V_\rho(s) = \tau \log \sum_a \exp \{ Q_\rho(s,a)/\tau \} \tag{20} \pi_\rho(a|s) = \exp\{ (Q_\rho(s,a) - V_\rho(s))/\tau \} \tag{21}这样做的意义在于提出一种新型的 AC 框架，让 actor 和 critic 统一在一个模型中。对参数 $\rho$ 进行求导可以得到： \begin{aligned} \Delta_\rho &= \eta_\pi C(s_{i:i+d},\rho) \sum_{j=0}^{d-1} \gamma^j \nabla_\rho \log \pi_\rho (a_{i+j}|s_{i+j}) \\ &+ \eta_v C(s_{i:i+d},\rho) \left( \nabla_\rho V_\rho(s_i) - \gamma^d \nabla_\rho V_\rho(s_i+d) \right) \end{aligned} \tag{22} \label{22}4.2 PCL 和 AC、Q-learning 的联系对于 A2C (advantage actor critic) 框架，给定一个长度为 $d$ 的轨迹，优势函数可以表示为： A(s_{i:i+d},\phi) = -V_\phi(s_i) + \gamma^d V_\phi(s_{i+d}) + \sum_{j=0}^{d-1} \gamma^j r(s_{i+j},a_{i+j}) \tag{23}策略网路和状态网络的参数梯度表达式为： \Delta \theta = \eta_\pi \mathbb{E}_{s_{i:i+d}|\theta} [A(s_{i:i+d},\phi) \nabla_\theta \log \pi_\theta(a_i|s_i)] \tag{24} \label{24} \Delta \phi = \eta_v \mathbb{E}_{s_{i:i+d}|\theta} [A(s_{i:i+d},\phi) \nabla_\phi V_\phi(s_i)] \tag{25} \label{25}其中 $\mathbb{E}_{s_{i:i+d}|\theta}$ 表示从当前策略 $\pi_\theta$ 采样的样本期望。 注意到公式 $\eqref{24}$ 、$\eqref{25}$ 和公式 $\eqref{17}$、$\eqref{18}$ 十分相似。实际上，当令 PCL 的参数 $\tau \rightarrow 0$ 时，PCL 就是一个 A2C 算法的变体。因此，PCL 算法可以视为 A2C 算法的一般化。并且 PCL 的使用范围更广，从 A2C 的 on-policy 扩充为 off-policy。另外需要注意的是，在 A2C 算法中，$V_\phi$ 的训练目标是 $V^{\pi_\theta}$，而在 PCL 中，不需要重新计算 $V^{\pi_\theta}$ 。并且，PCL 可以将 actor 和 critic 统一为一个模型，不再需要单独的 critic 模型。 另外如果令 $d=1$，PCL 其实变成了 soft Q-learning 。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Q-learning</tag>
        <tag>RL</tag>
        <tag>AC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Soft Q-learning (SQL)]]></title>
    <url>%2F2020%2F03%2F17%2FSoft%20Q-learning%20(SQL)%2F</url>
    <content type="text"><![CDATA[Haarnoja, T., Tang, H., Abbeel, P., &amp; Levine, S. (2017). Reinforcement learning with deep energy-based policies. 34th International Conference on Machine Learning, ICML 2017, 3, 2171–2186. 1. 论文概述本文将最大化熵方法运用在连续状态和动作空间的强化学习方法中，设计了名为 soft Q-learning 的算法。这种算法的优点是提高探索效率，并且便于让智能体在不同任务中共享学习到的技能。 强化学习可以使用确定性策略和随机策略。随机策略可以提高探索率，但是这种探索率通常是通过引入噪声或者初始化一个熵很高的随机策略。随机策略在某些场景下有些潜在优点，比如随机策略可以实现很高的探索率和组合性（组合性是指与其他算法结合的难易程度）。另外在面对不确定的动态属性，随机策略能表现出很好的鲁棒性，并且提高计算效率和收敛。 当我们将最优控制和概率推理联系起来的时候，随机策略就是一个最优解决方案。直观上，将控制问题视为推理的过程，目的不是为了产生一个确定性的 the lowest cost 行为，而是产生一系列的 low-cost 行为，并且最大化对应策略的熵。假设我们能学习到了完成任务的所有途径，那么完成任务的最优途径实际上只是学习到的所有途径中的一个具体化。 因此这种最大化熵的随机策略可以视为更具体的行为和复杂的任务的一个初始化。例如用这种方法训练一个 robot 向前走的 model，然后这个 model 作为下次训练 robot 跳跃、奔跑的初始化参数。随机策略也可以作为多模态奖励场景下的探索机制，并且在处理干扰时鲁棒性更强。 但是将最大化熵与随机策略结合并应用在连续的状态和动作空间，以及任意类型的策略分布上是具有挑战性的。本文的贡献是将随机策略和最大化熵结合，提出一个容易训练并且高效的算法：soft Q-learning 算法。 2. 相关知识2.1 Energy based model在这里补充一下基于能量的模型的概念。基于能量模型可以表示为： p_\theta(x) = \frac{1}{\int \exp(f_\theta(x))dx} \exp(f_\theta(x)) = \frac{1}{Z(\theta)} \exp(f_\theta(x))\tag{1}\label{1}其中 $Z(\theta) = \int \exp(f_\theta(x)) dx$ 是归一化常数，也可以叫做 partition function 。 2.1 Maximum Entropy Reinforcement Learning标准的强化学习算法优化目标为： \pi^*_{std} = \arg \max_\pi \sum_t \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} [r(s_t,a_t)]\tag{2}最大熵强化学习算法优化目标为： \pi^*_{MaxEnt} = \arg \max_\pi \sum_t \mathbb{E}_{(s_t,a_t) \sim \rho_\pi}[r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))]\tag{3}\label{3}其中 $\alpha$ 是衡量 reward 和 entropy 之间重要性的超参数。与 Boltzmann exploration [1] 和 PGQ [2] 算法不同的是，公式 $\eqref{3}$ 不是最大化当前时间步骤的熵，而是使得策略 $\pi$ 整个轨迹的熵最大化。 2.2 Soft Value Functions and Energy-Based Models传统的强化学习方法策略通常是一个单峰的分布 (unimodal policy distribution)，比如高斯策略，如下图左图所示。而我们想要探索整个动作分布，可以对动作值函数取幂，就形成了多峰策略分布 (multimodal policy distribution)，如下图右图所示。 \multimodal policy distribution.PNG) 本文使用了一种一般 energy-based 的策略定义形式： \pi(a_t|s_t) \propto \exp(-\mathcal{E}(s_t,a_t))\tag{4}其中 $\mathcal{E}$ 是 energy function，可以用神经网络表示。本文将能量模型 $\mathcal{E}(s_t,a_t)$ 和 soft 状态函数、动作值函数结合起来。如下面定理 1。 定理 1：定义 soft 动作值函数为： Q^*_{soft}(s_t,a_t) = r_t + \mathbb{E}_{(s_{t+1},\dots) \sim \rho_\pi} \left[ \sum_{l=1}^\infty \gamma^l(r_{t+l} + \alpha \mathcal{H}(\pi^*_{MaxEnt}(\cdot|s_{t+l}))) \right] \tag{5}定义 soft 状态值函数为： V^*_{soft}(s_t) = \alpha \log \int_\mathcal{A} \exp \left( \frac{1}{\alpha} Q^*_{soft}(s_t,a') \right) \; da'\tag{6}\label{6}那么得到公式 $\eqref{3}$ 的最优策略为： \pi^*_{MaxEnt}(a_t|s_t) = \exp \left( \frac{1}{\alpha}\left(Q^*_{soft}(s_t,a_t) - V^*_{soft}(s_t) \right) \right)\tag{7}\label{7} 公式 $\eqref{6}$ 中的 soft 状态值函数是 LogSumExp 形式，是一种 smooth maximum。公式 $\eqref{7}$ 中的 $\frac{1}{\alpha} Q_{soft}(s_t,a_t)$ 称为 negative energy，$\frac{1}{\alpha}V_{soft}(s_t)$ 称为 log-partition function。本文的附录 A.1 证明了公式 $\eqref{7}$ 表示的新策略可以提高熵和动作值函数的和，也就是可以实现 Policy improvement 。 本文还推导了 soft 贝尔曼方程的表达式： 定理 2： soft 贝尔曼方程的表达式为： Q^*_{soft}(s_t,a_t) = r_t + \gamma \mathbb{E}_{s_{t+1}\sim p_s} [V^*_{soft}(s_{t+1})]\tag{8}\label{8} 本文的附录 A.2 证明了公式 $\eqref{8}$ 成立。 3. 具体算法3.1 Soft Q-Iteration通过压缩映射可以证明： Q_{soft}(s_t,a_t) \leftarrow r_t + \gamma \mathbb{E}_{s_{t+1}\sim p_s}[V_{soft}(s_{t+1})], \forall s_t,a_t \tag{9} \label{9} V_{soft}(s_t) \leftarrow \alpha \log \int_\mathcal{A} \exp \left( \frac{1}{\alpha}Q_{soft}(s_t,a') \right) da', \forall s_t \tag{10} \label{10}会收敛到 $Q^_{soft}$ 和 $V^_{soft}$ 。本文的附录 A.2 有证明。 但是还有几个点需要取考虑。比如如何将 soft 贝尔曼方程运用在连续和大规模的状态、动作空间，如何对基于能量模型的策略进行采样。 3.2 Soft Q-learning由于公式 $\eqref{10}$ 中含有积分项，处理起来比较困难，作者使用随机优化方法对上述公式进行优化。首先对 soft Q 函数进行函数逼近，表示为 $Q^\theta_{soft}(s_t,a_t)$。 通过重要性采样得到公式 $\eqref{10}$ 的期望形式： V^\theta_{soft}(s_t) = \alpha \log \mathbb{E}_{q_{a'}} \left[ \frac{\exp(\frac{1}{\alpha} Q^\theta_{soft}(s_t,a'))}{q_{a'}(a')} \right] \tag{11} \label{11}其中 $q_{a’}$ 可以是动作空间的任意分布。 通过对 soft Q 构造最小平方误差目标函数： J_Q(\theta) = \mathbb{E}_{s_t \sim q_{s_t}, a_t \sim q_{a_t}} \left[ \frac{1}{2} \left( \hat{Q}_{soft}^\bar{\theta}(s_t,a_t) - Q^\theta_{soft}(s_t,a_t) \right)^2 \right] \tag{12} \label{12}其中 $\hat{Q}_{soft}^\bar{\theta}(s_t,a_t) = r_t + \gamma \mathbb{E}_{s_{t+1} \sim p_{s}}[V_{soft}^\bar{\theta}(s_{t+1})]$ 是目标 Q 值，可以通过公式 $\eqref{11}$ 得到的 $V_{soft}^\bar{\theta}(s_{t+1})$ 进行计算，$\bar{\theta}$ 表示目标网络的参数。 到了这里，我们可以利用采样的状态和动作进行随机梯度下降来解决公式 $\eqref{12} $ 的最小化问题。 对于 $q_{s_t}$ 和 $q_{a_t}$，可以使用策略 $\pi(a_t|s_t) \propto \exp(\frac{1}{\alpha} Q_{soft}^\theta(s_t,a_t))$ 的样本，也可以使用均匀分布进行采样。但是对于连续的状态、动作空间，使用均匀采样会产生高维度的数据，因此最好的选择是使用当前策略。 问题是如何从当前策略 $\pi(a_t|s_t) \propto \exp(\frac{1}{\alpha} Q_{soft}^\theta(s_t,a_t))$ 进行采样呢？公式 $\eqref{7}$ 只是一个一般形式，从公式 $\eqref{7}$ 采样是不可能的，因此需要一个 approximate sampling 的过程。 3.3 Approximate Sampling and SVGD当前从基于能量模型的策略中采样主要有两种方法：一种是基于马尔科夫链的蒙特卡罗（MCMC）采样，另一种是构造采样网络，训练成输出与目标分布相似的样本。基于 MCMC 的方法不适用于一边执行策略，一边采样，因此本文利用基于 Stein variational gradient (SVGD) 的采样网络 [3] 和 amortized SVGD [4] 。 使用 amortized SVGD 有三个好处： 提供了一个随机的采样网络。 可以收敛到精确的模型后验分布。 与 AC 算法相似，可以同 AC 算法联系起来，这就有了之后的 SAC 算法。 学习的随机采样网络可以表示为 $a_t = f^\phi (\xi;s_t)$，其中 $\phi$ 表示网络的参数，$\xi$ 表示噪声，可以是高斯分布或者其它随机分布。为了将采样网络训练成与 $Q^\theta_{soft}$ 对应的策略模型，我们定义该策略模型为 $\pi^\phi(a_t|s_t)$，并且目标是近似基于能量模型的策略，即公式 $\eqref{7}$ 。利用 KL 散度来衡量两个分布的近似程度，因此目标函数的表达式如下： J_\pi(\phi;s_t) = D_{KL} \left( \pi^\phi(\cdot|s_t) \| \exp(\frac{1}{\alpha}(Q^\theta_{soft}(s_t,\cdot) - V^\theta_{soft})) \right) \tag{13} \label{13}SVGD 提供了一个最贪婪的梯度方向： \Delta f^\phi(\cdot;s_t) = \mathbb{E}_{a_t \sim \pi^\phi} \left[ \mathcal{\kappa(a_t,f^\phi(\cdot;s_t))}\nabla_{a'} Q^\theta_{soft}(s_t,a')\big|_{a'=a_t} + \alpha \nabla_{a'}\kappa(a',f^\phi(\cdot;s_t))\big|_{a'=a_t} \right] \tag{14} \label{14}其中 $\kappa$ 表示核函数，通常使用高斯核函数。 实际上 $\Delta f^\phi$ 只是 $\kappa$ 的再生核希尔伯特空间中的最优梯度方向，严格来说并不是公式 $\eqref{13} $ 的梯度。但是论文 [4] 中证明了 $\frac{\partial J_\pi}{\partial a_t} \propto \Delta f^\phi$ 。因此可以利用链式法则将 SVGD 应用到策略网络的梯度求解中： \frac{\part J_\pi(\phi;s_t)}{\part \phi} \propto \mathbb{E}_{\xi} \left[ \Delta f^\phi(\xi;s_t) \frac{\part f^\phi(\xi;s_t)}{\part \phi} \right] \tag{15} \label{15}结合以上推导，可以得到 SQL 算法的伪代码如下： \SQL.PNG) 但是还有两个地方需要注意，一个是采样策略 $q_{a’}$ 怎么确定，二是公式 $\eqref{15}$ 具体应该怎么算。 3.4 确定 $q_{a’}$在前面的 3.2 节已经说明 $q_{a’}$ 最好是使用策略 $\pi(a_t|s_t) \propto \exp(\frac{1}{\alpha} Q_{soft}^\theta(s_t,a_t))$ 的样本，因此可以直接从采样网络 $a’=f^\phi(\xi’;s)$ 进行采样。不过采样网络在初期训练时，并不能产生能很好估计 $V^\theta_{soft}$ 的样本，因此可以选择在初期训练时选择均匀采样策略，在后期训练时采用当前采样网路。 3.5 计算 $\frac{\part J_\pi(\phi;s_t)}{\part \phi}$注意以下解释使用的符号 $i$ 和 $j$ 与伪代码中不对应。 结合公式 $\eqref{14}$ 和公式 $\eqref{15}$，可以知道 SVGD 需要计算两次期望。为了计算公式 $\eqref{14}$ 的期望，需要进行一次采样求平均：$a_t^{(i)} = f^\phi(\xi^{(i)})$ 。同样为了计算公式 $\eqref{15}$ 中的期望，也要进行一次采样求平均：$\tilde{a}_t^{(j)} = f^\phi(\tilde{\xi}^{(j)})$ 。这两次采样的结果可能相同也可能不同，但是是分别对应两次期望的采样。 将公式 $\eqref{14}$ 代入公式 $\eqref{15}$，可以得到： \hat \nabla_\phi J_\pi(\phi;s_t) = \frac{1}{KM} \sum_{j=1}^{K} \sum_{i=1}^{M} \left( \kappa(a_t^{(i)},\tilde{a}_t^{(j)}) \nabla_{a'} Q_{soft}^\theta(s_t,a') \big|_{a'=a_t^{(i)}} + \nabla_{a'} \kappa(a',\tilde{a}_t^{(j)}) \big|_{a'=a_t^{(i)}} \right) \nabla_\phi f^\phi(\tilde{\xi}^{(j)};s_t) \tag{16}伪代码中的 $\hat \nabla_\phi J_\pi$ 是 $\hat \nabla_\phi J_\pi(\phi;s_t)$ 对于 mini-batch 的平均。 另外 $\nabla_{a’} Q^\theta_{soft}(s_t,a’)$ 的求解实际上不是对公式 $\eqref{9} $ 中的 $Q_{soft}(s_t,a’)$ 进行理论求导， 这极其复杂并且无法计算。实际上 $Q^\theta_{soft}(s_t,a’)$ 是 $Q_{soft}(s_t,a’)$ 的神经网络逼近形式，很明显 $\nabla_{a’} Q^\theta_{soft}(s_t,a’)$ 的求解可以在神经网络中运用链式法则，对输入 $a’$ 进行求导。 4.工程设置学习率：Q 网络的学习率为 0.001，策略采样网络的学习率为 0.0001 。使用 ADAM 优化器。 经验回放池大小：一百万。样本超过一万才进行训练。 Mini-batch 大小：64 。 每次迭代的时间步长：10000 。 网络结构： Q 网络和策略采样网络都采用两层隐藏层，每层 200 格隐藏单元，激活函数采用 ReLU 。 添加噪声：OU 噪声。 核函数 $\kappa$ ：$\kappa(a,a’) = \exp(-\frac{1}{h} | a - a’ |^2_2)$, $h = \frac{d}{2 \log (M+1)}$ ，其中 $d$ 表示采样动作 $a_t^{(i)}$ 的成对距离的中值。 熵权重系数 $\alpha$ 不同环境设置不同，但是都会在短时间内降火，设置是在 200 个 epochs 中 log 线性速度降火到 0.001 。 参考文献[1] Sallans, B. and Hinton, G. E. Reinforcement learning with factored states and actions. Journal ofMachine Learning Research, 5(Aug):1063–1088, 2004. [2] O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. arXiv preprint arXiv:1611.01626, 2016. [3] Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016. [4] Wang, D. and Liu, Q. Learning to draw samples: With application to amortized mle for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>Q-learning</tag>
        <tag>energy-based</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Twin Delayed Deep Deterministic policy gradient]]></title>
    <url>%2F2020%2F03%2F09%2FTwin-Delayed-Deep-Deterministic-policy-gradient%2F</url>
    <content type="text"><![CDATA[Fujimoto, S., Van Hoof, H., &amp; Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. 35th International Conference on Machine Learning, ICML 2018, 4, 2587–2601. 1. TD3 概念Q-learning 和 actor-critic 都存在状态值过估计以及收敛于次优策略的问题。本文依照 double Q-learning 的思想提出了 TD3 算法，研究了过估计误差和目标网络间的联系，以及通过延迟策略的更新来降低每次更新的误差，提高策略的性能。 离散动作空间的过估计问题已经进行了研究，提出了 double Q-learning 算法。但是对于连续动作空间的 actor-critic 框架，过估计问题和误差的累积问题同样存在。过估计问题的来源主要是对带有噪声的状态值进行最大化，导致了最终目标产生过估计。另外，基于时间差分算法的强化学习算法，会利用一个本身不正确的状态估计值来进行下一次更新，导致误差累积。过估计和误差累积通常会导致一个本来很差的状态被估计成很高的状态值，产生次优策略。 本文主要有三个贡献： 首先，本文展示了采用目标网络是降低误差累积和降低误差的重要方法。 其次，本文提出了策略延迟更新的策略，即在状态估计收敛后再更新策略。 最后，本文提出了一种新颖的正则方法。 最终，本文将以上的方法与 DDPG 融合起来，产生的算法命名为 TD3 (Twin Delayed Deep Deterministic policy gradient algorithm)。 2. 相关知识强化学习的目标： R_t = \sum_{i=t}^{T} \gamma^{i-t} r(s_i,a_i) \tag{1}DDPG 的目标求导： \nabla_\phi J(\phi) = \mathbb{E}_{s \sim p_\pi} [\nabla_a Q^\pi(s,a)|_{a=\pi(s)} \nabla_\phi \pi_\phi(s)] \tag{2}Q-learning 的时间差分更新公式： Q^\pi(s,a) = r + \gamma \mathbb{E}_{s',a'}[Q^\pi(s',a')], \;\; a' \sim \pi(s') \tag{3} \label{3}DQN 的目标函数： y = r + \gamma Q_{\theta'}(s',a'), \;\; a' \sim \pi_{\phi'}(s') \tag{4}其中 $\pi_{\phi’}$ 是由目标网络计算出来的，目标网络的参数更新方式为： \theta' \leftarrow \tau \theta + (1-\tau) \theta' \tag{5}3. 过估计偏差在离散空间的 Q-learning 算法，过估计偏差的来源比较明显。首先对于一个贪婪的目标函数：$y=r+\gamma \max_{a’}Q(s’,a’)$，如果该函数对误差 $\epsilon$ 比较敏感，则在 $Q(s’,a’)$ 上求最大值通常会大于实际的最大值：$\mathbb{E}_\epsilon[\max_{a’}(Q(s’,a’)+\epsilon)] \ge \max_{a’} Q(s’,a’)$ 。即使误差是零均值的，也会导致过高的动作值估计，并且通过贝尔曼方程传播，如公式$ \eqref{3}$，从而导致误差累积。 在连续空间的 AC 框架中，过估计偏差并不是那么明显，但是它的确存在。 3.1 AC 中的过估计偏差AC 框架中的策略更新依赖于 critic 对状态值的估计。但是状态值的估计过程中会存在过估计偏差，导致产生次优策略 现在假设两种策略函数参数 $\phi_{approx}$ 和 $\phi_{true}$，对应的策略表示为 $\pi_{approx}$ 和 $\pi_{true}$ 。其中 $\phi_{approx}$ 的更新依赖于动作值估计函数 $Q_\theta(s,a)$，而 $\phi_{true}$ 的更新依赖于真实的动作值估计函数 $Q^\pi(s,a)$，但这是未知的。导数如下： \begin{aligned} \phi_{approx} &= \phi + \frac{\alpha}{Z_1} \mathbb{E}_{s \sim p_\pi} [\nabla_\phi \pi_\phi(s) \nabla_a Q_\theta(s,a) |_{a=\pi_\phi(s)}] \\ \phi_{true} &= \phi + \frac{\alpha}{Z_2} \mathbb{E}_{s \sim p_\pi} [\nabla_\phi \pi_\phi(s) \nabla_a Q^\pi(s,a) |_{a=\pi_\phi(s)}] \end{aligned} \tag{6}其中 $Z_1$ 和 $Z_2$ 是规范化参数，即 $Z^{-1} | \mathbb{E}[\cdot] |=1$ 。通过规范化参数，噪声的存在就可以反映为学习步长的变化。 由于梯度的方向只是局部的最大化方向，存在噪声 $\alpha \le \epsilon_1$（噪声足够小），那么 $\pi_{approx}$ 的近似动作值 $Q_\theta$ 就会大于 $\pi_{true }$ 。这可以理解为，$\pi_{approx}$ 和 $\pi_{ture}$ 都在近似动作值函数 $Q_\theta$ 图像的山腰，但是 $\pi_{approx}$ 前进的步数大于 $\pi_{ture}$，用公式表示如下： \mathbb{E}[Q_\theta(s,\pi_{approx}(s))] \ge \mathbb{E}[Q_\theta(s,\pi_{true}(s))] \tag{7} \label{7}同理，存在噪声 $\alpha \le \epsilon_2$（噪声足够小），那么 $\pi_{approx}$ 的真实动作值 $Q^\pi$ 就会小于 $\pi_{true }$ 。这可以理解为，$\pi_{approx}$ 和 $\pi_{ture}$ 都在真实动作值函数 $Q^\pi$ 图像的山顶，但是 $\pi_{approx}$ 前进的步数大于 $\pi_{ture}$，用公式表示如下： \mathbb{E}[Q^\pi(s,\pi_{true}(s))] \ge \mathbb{E}[Q^\pi(s,\pi_{approx}(s))] \tag{8} \label{8}同时，近似动作值函数对于 $\pi_{ture}$ 而言通常都是过估计的，也就是 $\mathbb{E}[Q_\theta(s,\pi_{ture}(s))] \ge \mathbb{E}[Q^\pi(s,\pi_{ture}(s))]$，结合公式 $\eqref{7}$ 和公式 $\eqref{8}$ ，可以得到： \mathbb{E}[Q_\theta(s,\pi_{approx}(s))] \ge \mathbb{E}[Q^\pi(s,\pi_{approx}(s))] \tag{9} \label{9}由公式 $\eqref{9}$ 可以知道，近似动作值函数对于学习的策略存在过估计，这种过估计偏差虽然在一个更新步骤中很小，但是经过非常多的更新步骤积累起来的估计偏差将会变得非常大。而过估计的动作值函数将会产生次优策略，从而导致新一轮的偏差出现。 本文为了降低 critic 的过估计偏差，提出了 Clipped Double Q-learning 方法。 3.2 Clipped Double Q-learning在 Double DQN 方法中，作者利用了一个目标网络对时间差分更新公式中的动作值进行估计，但是在采取行动策略时，用的是行为网络的贪婪策略。 但是本文作者发现，这种方法对于 AC 框架的作用效果不大，目标网络和行为网络对状态值的估计相似，不能很好地解决过估计问题。本文作者提出，采用一对 actors $(\pi_{\phi_1},\pi_{\phi_2})$ 和一对 critics $(Q_{\theta_1},Q_{\theta_2})$，$\pi_{\phi_1}$ 的更新依赖于 $Q_{\theta_1}$，$\pi_{\phi_2}$ 的更新依赖于 $Q_{\theta_2}$，但是在时间差分更新公式中，目标 $y_1$ 依赖于 $Q_{\theta_2}$，目标 $y_2$ 依赖于 $Q_{\theta_1}$： y_1 = r + \gamma Q_{\theta_2'}(s',\pi_{\phi_1}(s')) \\ y_2 = r + \gamma Q_{\theta_1'}(s',\pi_{\phi_2}(s')) \tag{10} \label{10}在公式 $\eqref{10}$ 中，由于$Q_{\theta_1}$ 和 $Q_{\theta_2}$ 并不是特别独立（参数更新依赖于对方以及使用同一个经验回放池），还是会存在过估计的问题，本文进而提出 Clipped Double Q-learning 方法： y_1 = r + \gamma \min_{i=1,2} Q_{\theta_2'}(s',\pi_{\phi_1}(s')) \tag{11} \label{11}公式 $\eqref{11}$ 可能会存在欠估计（underestimation）问题，但是欠估计的偏差不会在策略更新中传播（因为策略不会选一个欠优的动作）。对于 $Q_{\theta_2}$ 的更新，可以简单令 $y_2=y_1$ 。 4. 解决高方差除了过估计偏差的影响，高方差也会给策略梯度的更新带来影响。 在使用时间差分公式进行更新时，实际上贝尔曼方程并没有完全满足，每次更新都会与真正的贝尔曼方程存在残差，称为 TD误差 $\delta(s,a)$： Q_\theta(s,a) = r + \gamma \mathbb{E}[Q_\theta(s',a')] - \delta(s,a) \tag{12} \label{12}对公式 $\eqref{12}$ 进行改写： \begin{aligned} Q_\theta(s_t,a_t) &= r_t + \gamma \mathbb{E}[Q_\theta(s_{t+1},a_{t+1})] - \delta_t \\ &= r_t + \gamma \mathbb{E}[r_{t+1} + \gamma \mathbb{E}[Q_\theta(s_{t+2},a_{t+2}) - \delta_{t+1}]] - \delta_t \\ &= \mathbb{E}_{s_i \sim p_\pi, a_i \sim \pi} \left[ \sum_{i=t}^T \gamma^{i-t}(r_i - \delta_i) \right] \end{aligned} \tag{13} \label{13}从公式 $\eqref{13}$ 可以看出动作值估计 $Q_\theta$ 的方差不仅依赖于期望回报值，也依赖于 TD 误差总和。如果 $\gamma$ 比较大，那么 TD 总误差的方差是很大的。 4.1 目标网络和延迟更新目标网络是强化学习实现稳定更新的重要方法。目标网络给强化学习提供了一个稳定的目标。如果没有一个稳定的目标，每次更新过程中都会产生残差，并且积累，最终产生较大的方差。 目标网络参数的更新可以采用软更新公式： \theta' \leftarrow \tau \theta + (1-\tau)\theta' \tag{14}目标网络在经历多次更新才能收敛，并且策略在高误差的状态估计下更新会产生发散的动作。本文认为策略网络的更新应该比状态网络的更新频率更低。也就是说在进行策略更新之前，应该先降低状态估计的误差。 因此延迟更新是让策略更新在每隔 $d$ 个状态更新之后才进行。 4.2 对目标策略的正则化如果策略采用的是确定性策略，存在的问题是确定性策略经常会受到状态估计的误差的影响，导致过拟合。本文通过对目标策略添加受限的噪声来进行正则化。 y = r + \gamma Q_{\theta'}(s',\pi_{\phi'}(s') + \epsilon), \\ \epsilon \sim clip(\mathcal{N}(0,\sigma),-c,c) \tag{15} \label{15}注意公式 $\eqref{15}$ 中目标策略所用的噪声 $\epsilon$ 是和探索策略所用的噪声 $\epsilon$ 区分开的。 5. 算法伪代码]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC</tag>
        <tag>DPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Soft actor-critic]]></title>
    <url>%2F2020%2F03%2F08%2FSoft-actor-critic%2F</url>
    <content type="text"><![CDATA[Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. 35th International Conference on Machine Learning, ICML 2018, 5, 2976–2989. 1. 论文概要无模型的强化学习算法存在采样复杂度高和收敛性弱的问题。即使很简单的任务，所需要的样本数量都达到上百万。另外对于不同问题，这些强化学习算法的超参数需要精细调整。本文提出一个 soft actor-critic 强化学习方法，是就基于最大化熵的 off-policy 强化学习方法。在这个框架中，actor 旨在于最大化奖励回报和熵。本文提出的算法在稳定性和性能方面都胜过目前的强化学习算法。 之所以 TRPO、PPO 和 A3C 这些算法的采样复杂度高，是因为他们都需要在每个梯度更新步骤采集新的样本。而 off-policy 方法一般用在 Q-learning 一类强化学习方法上，与传统的策略梯度方法结合会导致算法稳定性和收敛性下降。另外，最大化熵方法通常能提高探索效率和鲁棒性，但还没有与 off-policy 和 policy-gradient 等强化学习模型结合起来。 本文的主要贡献点就是将 off-policy 和最大化熵方法结合到 ac 框架中，用于连续空间的控制任务，并将算法命名为 SAC （soft actor-critic）。之前的很多 AC 框框架只采用了 on-policy 方法，并只是将熵当作正则项。 2. 相关知识传统的强化学习目标是 $\sum_t \mathbb{E}_{(s_t,a_t)\sim \rho_\pi}[r(s_t,a_t)]$，在此基础上添加了策略的熵作为目标项： J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} [r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))] \tag{1} \label{1}超参数 $\alpha$ 控制着熵这一项的重要程度，也就是控制着策略的随机程度。对于无限时长的任务，可以加入折扣因子： J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} \left[ \sum_{l=t}^\infty \gamma^{l-t} \mathbb{E}_{s_l \sim p, a_l \sim \pi} [r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) | s_t=s_l, a_t=a_l] \right] \tag{2}3. 算法推导3.1 Soft Policy Iteration本文先将最大熵方法和策略迭代方法结合得出收敛性结论（针对 tabular-setting）。 首先定义 soft 策略迭代中的 soft 策略评估过程，假设 $\mathcal{T}^\pi$ 为修改后的贝尔曼操作子，定义 soft 策略评估过程如下： \begin{aligned} \mathcal{T}^\pi Q(s_t,a_t) &\triangleq r(s_t,a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p}[V(s_{t+1})] \\ where \;\; V(s_t) &= \mathbb{E}_{a_t \sim \pi} [Q(s_t,a_t) - \log \pi(a_t|s_t)] \end{aligned} \tag{3} \label{3}以上公式的 $V(s_t)$ 称为 soft 状态函数，其中融合了 $\pi$ 的熵。根据公式 $\eqref{3}$ 可以证明以下引理。 引理 1：考虑公式 $\eqref{3}$ 中的 $\mathcal{T}^\pi$，并给出一个映射：$Q^0:\mathcal{S}\times \mathcal{A} \rightarrow \mathbb{R}, |\mathcal{A}|&lt; \infty$，定义 $Q^{k+1}=\mathcal{T}^\pi Q^k$。当 $k \rightarrow \infty$ 时，$Q^k$ 将会收敛为 $\pi$ 的 soft Q-value 。 引理 1 的结果表示公式 $\eqref{3}$ 定义的贝尔曼方程可以收敛。引理 1 在本文的附录 B.1 中有证明。 接下来定义 soft 策略迭代中的 soft 策略提升过程： \pi_{new} = \mathop{\arg \min}_{\pi' \in \Pi} D_{KL} \left( \pi'(\cdot|s_t) \Bigg\| \frac{exp(Q^{\pi_{old}}(s_t,\cdot))}{Z^{\pi_{old}}(s_t)} \right) \tag{4} \label{4}公式 $\eqref{4}$ 中的 $\pi’ \in \Pi$ 表示将策略限制在一个范围中，者可以对应为某些分布家族的参数化。为了实现限制 $\pi \in \Pi$，本文采用 KL 散度进行投影的方式，也就是公式 $\eqref{4}$ 中出现的 $D_{KL}$ 。另外 $Z^{\pi_{old}}(s_t)$ 是配分函数，用来归一化分布，实际上是不可计算的，但是对新策略的参数求导没有贡献，因此可以忽略。根据公式 $\eqref{4}$ 可以证明以下引理。 引理 2：令 $\pi_{old} \in \Pi$，并令 $\pi_{new}$ 为公式 $\eqref{4}$ 的结果，那么 $Q^{\pi_{new}}(s_t,a_t) \ge Q^{\pi_{old}}(s_t,a_t), \forall (s_t,a_t) \in \mathcal{S} \times \mathcal{A}, |\mathcal{A}| &lt;\infty$ 引理 2 的结果表示公式 $\eqref{4}$ 定义的策略提升过程能满足新策略的性能不下降。证明过程在本文的附录 B.2 。 根据引理 1 和引理 2 的结论，本文证明了以下定理（证明过程在本文的附录 B.3）。 定理 1：重复运用 soft 策略评估和 soft 策略提升，最终可以收敛到最优策略 $\pi^*$，使得 $Q^{\pi^*}(s_t,a_t) \ge Q^\pi(s_t,a_t),\forall \pi \in \Pi \;\; and (s_t,a_t) \in \mathcal{S} \times \mathcal{A},|\mathcal{A}|&lt;\infty$ 。 3.2 Soft Actor-Critic以上的 soft policy iteration 方法只是针对离散的状态和动作空间。为了应对大规模的连续状态和动作空间，需要对状态函数和策略进行函数逼近，也就是参数化。 本文总共对三个函数进行了参数化：soft 状态函数 $V_\psi(s_t)$、soft Q 函数 $Q_\theta(s_t,a_t)$，策略函数 $\pi_\phi(a_t|s_t)$ 。对应的参数分别是 $\psi$、$\theta$ 和 $\phi$ 。 根据公式 $\eqref{3}$ ，sot 状态函数是可以通过 soft Q 函数和策略函数来计算的，但是引入一个单独的函数估计可以提高训练的稳定性，以及便于与其他网络同时训练。 soft 状态函数的训练目标是： J_V(\psi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ \frac{1}{2} \left( V_\psi(s_t) - \mathbb{E}_{a_t \sim \pi_\phi}[Q_\theta(s_t,a_t) - \log \pi_\phi(a_t|s_t)] \right)^2 \right] \tag{5} \label{5}这里的 $\mathcal{D}$ 表示经验回放池。注意公式 $\eqref{5}$ 中的 $a_t$ 不是从经验回放池中抽取的动作，而是从当前策略采样的动作。 公式 $\eqref{5}$ 求导可得： \hat{\nabla_\psi} J_V(\psi) = \nabla_\psi V_\psi(s_t)(V_\psi(s_t) - Q_\theta(s_t,a_t) + \log \pi_\phi(a_t|s_t)) \tag{6} \label{6}soft Q 函数的训练目标是： J_Q(\theta) = \mathbb{E}_{(s_t,a_t) \sim \mathcal{D}} \left[ \frac{1}{2}\left( Q_\theta(s_t,a_t) - \hat{Q}(s_t,a_t) \right)^2 \right] \tag{7} \label{7}其中： \hat{Q}(s_t,a_t) = r(s_t,a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p}[V_{\bar{\psi}}(s_{t+1})] \tag{8}公式 $\eqref{7}$ 求导可得： \hat{\nabla_\theta}J_Q(\theta) = \nabla_\theta Q_\theta(s_t,a_t)(Q_\theta(s_t,a_t) - r(s_t,a_t) - \gamma V_{\bar{\psi}}(s_{t+1})) \tag{9}其中 $V_\bar{\psi}$ 表示的是独立的目标网络，参数 $\bar{\psi}$ 的更新是通过 $\psi$ 进行 soft update，也就是滑动加权平均。 最后是策略的训练目标： J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ D_{KL} \left( \pi_\phi(\cdot|s_t) \Bigg\| \frac{exp(Q_\theta(s_t,\cdot))}{Z_\theta(s_t)} \right) \right] \tag{10}为了对策略进行参数化，本文利用神经网络加噪声的形式表示：$a_t = f_\phi(\epsilon_t;s_t)$，其中 $\epsilon_t$ 表示输入噪声向量，可以从某些固定分布中采样。重新策略的训练目标为： J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D},\epsilon \sim \mathcal{N}} [\log \pi_\phi(f_\phi(\epsilon_t;s_t)|s_t) - Q_\theta(s_t,f_\phi(\epsilon_t;s_t))] \tag{11} \label{11}联合公式 $\eqref{3}$ 对公式 $\eqref{11}$ 进行求导得： \hat{\nabla_\phi} J_\pi(\phi) = \nabla_\phi \log \pi_\phi(a_t|s_t) + (\nabla_{a_t} \log \pi_\phi(a_t|s_t) - \nabla_{a_t} Q(s_t,a_t)) \nabla_\phi f_\phi(\epsilon_t;s_t) \tag{12}最终 SAC 的伪代码如下： 4. 消融实验4.1 Stochastic vs. deterministicdeterministic SAC 不加入熵作为目标，同时策略函数输出的策略是固定的，没有加入噪声。对比结果如下： 4.2 reward scalingreward scaling 可以认为是调整公式 $ \eqref{1}$ 中的超参数 $\alpha$，只不过是通过放大奖励项的倍数，$\alpha$ 的值则设为 1。对比结果如下： 4.3 target smoothing coefficient就是指目标状态网络的软更新超参数 $\tau$ 的取值，对比结果如下： 4.4 hard update vs. soft update目标网络的更新还可以采用硬更新方法，就是每隔一段训练周期，就将行为网络的参数赋值给目标网络。对比结果如下： 5. 工程设置]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Imitation Learning]]></title>
    <url>%2F2020%2F03%2F04%2FGenerative-Adversarial-Imitation-Learning%2F</url>
    <content type="text"><![CDATA[1. GAIL 概念介绍假如需要从一个示例的专家数据中学习出一个策略，而且学习过程中没有专家的指导和信号预测。一种方是行为克隆方法，主要是通过监督学习模型从专家数据中学习出策略，但是这种方法需要大量数据。另一种方法是通过逆强化学习从专家数据中学习代价函数，该代价函数从专家的角度而言是唯一最优的，但是这种方法不直接而且比较慢，通常需要强化学习在内循环。为什么不可以直接从专家数据中学习出一个策略呢？ 本文提出一种可以直接从专家数据中学习出策略的算法，称为 GAIL（Generative Adversarial Imitation Learning），生成对抗模仿学习。GAIL 实际上也运用了 GAN 的训练方法，来拟合状态数据和动作数据的分布，从而定义专家的行为。 2. 相关知识2.1 符号定义$\overline{\mathbb{R}}$ 定义为扩展的实数集：$\mathbb{R} \cup \{\infty\}$ 。 定义状态空间为 $\mathcal{S}$，动作空间为 $\mathcal{A}$，$\Pi$ 表示在状态空间 $\mathcal{S} $ 执行动作 $\mathcal{A} $ 的平稳随机策略集合。 状态转移概率定义为 $P(s’|s,a)$ 。 起始状态的分布写为 $p_0(s)$ 。 对于给定的策略 $\pi \in \Pi$，定义性能期望为：$\mathbb{E}_\pi[c(s,a)] \triangleq \mathbb{E}[\sum_{t=0}^\infty \gamma^t c(s_t,a_t)]$ 。 对于采样的轨迹样本，性能期望的符号表示为 $\hat{\mathbb{E}_\tau}$，专家策略表示为 $\pi_E$ 。 2.2 逆强化学习本文后面采用了 maximum causal entropy (因果熵) IRL [1,2] ，它构造了一个优化问题： \mathop{maximize}_{c \in \mathcal{C}} \left( \min_{\pi \in \Pi} - H(\pi) + \mathbb{E}_\pi[c(s,a)] \right) - \mathbb{E}_{\pi_E}[c(s,a)] \tag{1}其中 $c$ 表示 cost function，$\mathcal{C}$ 表示函数族，$H(\pi) \triangleq \mathbb{E}_\pi[-\log \pi(a|s)]$ 表示 $\gamma$ 折扣因果熵。 $\pi_E$ 实际上只是执行策略 $\pi_E$ 得到的一系列采样轨迹。因此 $\mathbb{E}_{\pi_E}$ 需要从这些样本中估计期望值。 最大化因果熵逆强化学习方法的目的是找到一个代价函数 $c \in \mathcal{C}$，使得对于专家策略，代价最低，而对于其他策略，代价很高。找到代价函数 $c$ 之后，从而通过强化学习过程估计专家策略： RL(c) = \mathop{\arg \min}_{\pi \in \Pi} -H(\pi)+\mathbb{E}_\pi[c(s,a)] \tag{2}3. 算法推导首先研究 IRL 的表达能力。IRL 建立在庞大的代价函数族 $\mathbb{R}^{\mathcal{S} \times \mathcal{A}} = \{c:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\}$ 上。由于拥有庞大的函数族，，对于固定的数据集很容易造成过拟合。因此本文引入一个凸的正则项 $\psi: \mathbb{R}^{\mathcal{S} \times \mathcal{A}} \rightarrow \overline{\mathbb{R}}$ 。加入正则项的最大因果熵 IRL 目标可以表示为： IRL_\psi(\pi_E) = \mathop{\arg \max}_{c \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}}-\psi(c) + \left( \min_{\pi \in \Pi} - H(\pi) + \mathbb{E}_\pi[c(s,a)] \right) - \mathbb{E}_{\pi_E}[c(s,a)] \tag{3}令 $\tilde{c} \in IRL_\psi(\pi_E)$ ，现在我们感兴趣的是由 $RL(\tilde{c})$ 得出的策略的性能。 为了评估 $RL(\tilde{c})$，将原先优化问题转化为凸问题。首先定义策略 $\pi \in \Pi$ 的占有率（occupancy measure） $\rho_\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ 为 $\rho_\pi(s,a) = \pi(a|s) \sum_{t=0}^\infty \gamma^t P(s_t=s|\pi)$ 。 占有率可以理解为状态动作对在策略 $\pi$ 执行下的概率分布。此时对于 $c(s,a)$，期望值的计算可以写为：$\mathbb{E}_\pi[c(s,a)]=\sum_{s,a} \rho_\pi(s,a)c(s,a)$ 。论文 [3] 中证明了占有率集合 $\mathcal{D} \triangleq \{\rho_\pi: \pi \in \Pi \}$ ，可以写成仿射约束的可行集：$\mathcal{D}=\left\{ \rho:\rho \ge 0 \;\; and \;\; \sum_a \rho(s,a)=\rho_0(s) + \gamma \sum_{s’,a} P(s’|s,a)\rho(s’,a) \;\; \forall s \in \mathcal{S} \right\}$ 。（PS：仿射变换就是线性变化加上一个平移）注意，$\Pi$ 和 $\mathcal{D}$ 是一一映射的。 命题 3.1：如果 $\rho \in \mathcal{D}$，那么 $\rho$ 是策略 $\pi_\rho \triangleq \rho(s,a)/\sum_{a’} \rho(s,a’)$ 的占有率，$\pi_{\rho}$ 也是唯一的占有率为 $\rho$ 的策略。 命题 3.2：$RL \circ IRL_\psi(\pi_E) = \arg \min_{\pi \in \Pi} -H(\pi)+\psi^*(\rho_\pi-\rho_{\pi_E})$ 为了评估 $RL(\tilde{c})$，需要一个数学工具：共扼函数。对于函数 $f:\mathbb{R}^{\mathcal{S} \times \mathcal{A}} \rightarrow \overline{\mathbb{R}}$，它的共扼函数 $f^:\mathbb{R}^{\mathcal{S} \times \mathcal{A}} \rightarrow \overline{\mathbb{R}}$ 表示为$f^(x)=\sup_{y\in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}} x^Ty -f(y) $ 。共扼函数一定是凸函数。 命题 3.1 出自论文 [4] 。命题 3.2 在本文的附录 A 有证明。 命题 3.2 告诉我们带有 $\psi$ 正则项的 IRL 在含蓄地寻找一个策略，其占有率接近专家策略，通过共扼函数 $\psi^*$ 来度量。本文接下来将会阐述采用不同的 $\psi$ 设置，将会导致不同的模仿学习方法。 3.1 constant function $\psi$最特殊的情况是 $\psi$ 是一个常数（也就是不采用正则项）。 引理 3.1：令 $\bar{H}(\rho) = -\sum_{s,a}\rho(s,a) \log \left( \rho(s,a)/\sum_{a’}\rho(s,a’) \right)$ 。 那么 $\bar{H}$ 是严格的凹函数，并且对于所有的 $\pi \in \Pi$ 和 $\rho \in \mathcal{D}$，都有 $H(\pi)=\bar{H}(\rho_\pi)$ 和 $\bar{H}(\rho)=H(\pi_\rho)$ 。 引理 1 在本文的附录 A 有证明。引理 3.1 告诉我们可以随意在策略和占有率两者间切换，因此得到下面引理。 引理 3.2：如果令 $L(\pi,c)=-H(\pi)+\mathbb{E}_\pi[c(s,a)]$，令 $\bar{L}(\rho,c)=-\bar{H}(\rho)+\sum_{s,a}\rho(s,a)c(s,a)$ 。那么对于所有的代价函数 $c$，都有 $L(\pi,c) = \bar{L}(\rho_\pi,c) \;\; \forall \pi \in \Pi$ ，并且都有 $\bar{L}(\rho,c)=L(\pi_\rho,c) \;\; \forall \rho \in \mathcal{D}$ 。 根据上面两条引理，可以得出以下推论： 推论 3.2.1：​如果 $\psi$ 是一个常数，并且 $\tilde{c} \in IRL_\psi(\pi_E)$，$\tilde{\pi} \in RL(\tilde{c})$，那么 $\rho_{\tilde{\pi}}=\rho_{\pi_E}$ 。 以上推论告诉我们，如果不采用正则项，那么 $RL(\tilde{c})$ 得到的策略将完全与专家策略 $\pi_E$ 匹配。 推论 3.2.1 证明过程如下（涉及到最优化中的原问题和对偶问题）： 定义 $\bar{L}(\rho,c) = -\bar{H}(\rho)+\sum_{s,a}c(s,a)(\rho(s,a)-\rho_E(s,a))$ 。给定 $\psi$ 是常数。 根据引理 3.2，可得： \begin{aligned} \tilde{c} & \in IRL_\psi (\pi_E) = \mathop{\arg \max}_{c \in \mathbb{R}^{\mathcal{S}\times \mathcal{A}}} \min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_\pi[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)] + const \\ & = \mathop{\arg \max}_{c \in \mathbb{R}^{\mathcal{S}\times \mathcal{A}}} \min_{\rho \in \mathcal{D}} -\bar{H}(\rho) + \sum_{s,a} \rho(s,a)c(s,a) - \sum_{s,a} \rho_E(s,a)c(s,a) = \mathop{\arg \max}_{c \in \mathbb{R}^{\mathcal{S}\times \mathcal{A}}} \min_{\rho \in \mathcal{D}} \bar{L}(\rho,c) \end{aligned} \tag{4} \label{4}以上问题的对偶问题如下： \mathop{minimize}_{\rho \in \mathcal{D}} -\bar{H}(\rho) \;\; subject \;\; to \;\; \rho(s,a) = \rho_E(s,a) \;\; \forall s \in \mathcal{S}, a\in \mathcal{A} \tag{5} \label{5}因此 $\tilde{c}$ 是等式 $\eqref{5}$ 的对偶最优解。 引理 3.1 保证了 $-\bar{H}$ 是严格的凸函数，因此最优解是唯一的。原始问题的最优解可以通过对偶问题的最优解得到，即： \tilde{\rho} = \mathop{\arg \min}_{\rho \in \mathcal{D}} \bar{L}(\rho, \tilde{c}) = \mathop{\arg \min}_{\rho \in \mathcal{D}} -\bar{H}(\rho) + \sum_{s,a} \tilde{c}(s,a)\rho(s,a)=\rho_E \tag{6}如果 $\tilde{\pi} \in RL(\tilde{c})$，根据引理 3.2，$\tilde{\pi}$ 的占有率 $\rho_{\tilde{\pi}} = \tilde{\rho} = \rho_E$ 。 从推论的结果，本文得出以下两条结论： IRL 是占有率匹配问题的一个对偶形式。 从占有率匹配对偶问题中导出的最优策略就是原始问题的最优策略。 3.2 occupancy measure matching在上一节中，阐述了如果正则项是常数，则原问题相当于与专家数据进行状态动作数据占有率匹配的问题，也就是说占有率越与专家数据的占有率匹配，则推导出的策略越接近专家策略。但在实际运用中，占有率匹配算法实际是不可行的。因为专家轨迹数据的分布仅仅是一个有限样本的集合，在大规模环境中，很多数据不在专家数据集中，因此导致这些数据的占有率在专家数据集中为零。这回导致让学习出策略避开专家数据集中没有的状态动作对。 另外还有一个问题，占有率匹配的约束条件数量与定义域 $\mathcal{S} \times \mathcal{A}$ 的大小相同，这就导致在函数逼近的参数优化过程中产生大量的运算。 为了在大规模环境中运用模仿学习算法，松弛公式 $\eqref{5}$ 的形式，写成： \mathop{minimize}_\pi \; d_\psi(\rho_\pi,\rho_E) - H(\pi) \tag{7} \label{7}其中 $d_\psi(\rho_\pi,\rho_E)$ 定义为 $\rho_\pi$ 和 $\rho_E$ 两个占有率的正则项，即 $d_\psi(\rho_\pi,\rho_E) \triangleq \psi^*(\rho_\pi-\rho_E)$ ，与命题 3.2 的形式类似。 3.2.1 apprenticeship learning学徒学习（apprenticeship learning）算法也是公式 $\eqref{7}$ 的一种特殊形式。令代价函数集合 $\mathcal{C} \subset \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$，学徒学习算法的目标为： \mathop{minimize}_{\pi} \; \max_{c \in \mathcal{C}} \mathbb{E_\pi}[c(s,a)]-\mathbb{E}_{\pi_E}[c(s,a)] \tag{8} \label{8}接下来说明公式 $\eqref{8}$ 为什么属于是公式 $\eqref{7}$ 的形式。 定义一个指示函数 $\delta_\mathcal{C}: \mathbb{R}^{\mathcal{S}\times\mathcal{A}} \rightarrow \overline{\mathbb{R}}$，表达式为： \delta_\mathcal{C} = \left\{ \begin{aligned} &0 &c \in \mathcal{C} \\ &+\infty &otherwise \end{aligned} \right.则公式 $\eqref{8}$ 可以写为如下形式： \max_{c \in \mathcal{C}} \mathbb{E_\pi}[c(s,a)]-\mathbb{E}_{\pi_E}[c(s,a)] = \max_{c \in \mathbb{R}^{\mathcal{S}\times \mathcal{A}}} -\delta_\mathcal{C} + \sum_{s,a}(\rho_\pi(s,a)-\rho_{\pi_E}(s,a))c(s,a) = \delta^*(\rho_\pi - \rho_{\pi_E}) \tag{9}因此学徒学习算法实际上采用的代价函数正则项为 $\psi = \delta_\mathcal{C}$，也就是将代价函数的范围限制在集合 $\mathcal{C}$ 中。带有熵正则项的学徒学习算法的目标可以写为： \mathop{minimize}_{\pi} \; -H(\pi) + \max_{c \in \mathcal{C}} \mathbb{E_\pi}[c(s,a)]-\mathbb{E}_{\pi_E}[c(s,a)] \tag{10} \label{10}对于集合 $\mathcal{C}$ 的形式，经典的学徒学习算法主要是将 $\mathcal{C}$ 限制子一个凸集中，该集合由一系列基函数进行线性组合得来：$f(s,a)=[f_1(s,a),\dots,f_d(s,a)]$。文中给出两种凸集：$\mathcal{C}_{linear}$ [5] 和 $\mathcal{C}_{convex}$[4] 。 \begin{aligned} \mathcal{C}_{linear} &= \{ \sum_i w_i f_i : \| w \|_2 \le 1 \} \\ \mathcal{C}_{convex} &= \{ \sum_i w_i f_i : \sum_iw_i = 1, w_i \ge 0 \;\;\forall_i \} \end{aligned} \tag{11} \label{11}3.2.2 学徒学习的优点学徒学习采用了限制的代价函数范围 $\mathcal{C}$，因此能够将这样的 $\mathcal{C}$ 与策略函数逼近结合起来，运用到大规模的状态空间和动作空间中。比如论文 [6] 就将策略梯度公式和学徒学习的目标公式 $\eqref{8}$ 结合起来： \nabla_\theta \max_{c\in \mathcal{C}} \mathbb{E}_{\pi_\theta}[c(s,a)]-\mathbb{E}_{\pi_E}[c(s,a)] = \nabla_\theta \mathbb{E}_{\pi_\theta}[c^*(s,a)] = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)Q_{c^*}(s,a)] \\ \mathscr{where} \;\; c^* = \mathop{\arg \max}_{c \in \mathcal{C}} \mathbb{E}_{\pi_\theta}[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)], \;\; Q_{c^*}(\bar{s},\bar{a})=\mathbb{E}_{\pi_\theta}[c^*(\bar{s},\bar{a})|s_0=\bar{s},a_0=\bar{a}] \tag{12}3.2.3 学徒学习的缺点如果对 $\mathcal{C}$ 的限制过于严格，很有可能找不到和专家策略一样优秀的策略。因此对于基函数 $f_1,\dots,f_d$ 的设计需要十分精致。 4. GAIL 算法正如前面所说的，如果正则项 $\psi$ 是常数，则对应的模仿学习算法可以与专家策略的占有率完全匹配，但是对于大规模的环境并不适用；如果正则项 $\psi$ 是线性函数类，如公式 $\eqref{11} $ ，则对应的模仿学习算法也许不能与专家策略的占有率完全匹配，但是可以适用于大规模的环境。 本文将提出新的正则器，结合以上两种正则器的优点： \psi_{GA}(c) \triangleq \left\{ \begin{aligned} &\mathbb{E}_{\pi_E}[g(c(s,a))] &if \;\; c < 0 \\ &+\infty &otherwise \end{aligned}, \right. \;\; where \;\; g(x) = \left\{ \begin{aligned} &-x-\log(1-e^x) &if \;\; x < 0 \\ &+\infty &otherwise \end{aligned} \right. \tag{13} \label{13}注意到以上的代价函数的上界为零。公式 $\eqref{13} $ 的意义在于，代价函数对于专家轨迹的值如果是一个小的负数，则正则器给予一个很低的惩罚；如果是一个很大的值（即为零），则正则器给予一个很高的惩罚。 之所以采用这样的正则器，是因为从公式 $ \eqref{13}$ 中可以得到以下推论（证明过程在文中的附录）： \psi^*_{GA}(\rho_\pi-\rho_{\pi_E}) = \max_{D \in (0,1)^{\mathcal{S}\times \mathcal{A}}} \mathbb{E}_\pi[\log(D(s,a))] + \mathbb{E}_{\pi_E}[\log(1-D(s,a))] \tag{14}其中判别器类别的范围是 $D:\mathcal{S \times \mathcal{A}} \rightarrow (0,1)$。现在我们的优化目标是： \mathop{minimize}_\pi \psi^*_{GA}(\rho_\pi - \rho_{\pi_E}) - \lambda H(\pi) \tag{15} \label{15}公式 $\eqref{15}$ 将模仿学习算法和生成对抗网络建立了连接，$D$ 就是生成对抗模型中的判别器。$D$ 的任务就是负责识别生成器 $G$ 生成的数据分布和真实的数据分布的差异，当 $D$ 不再能辨别出差异时，$G$ 的任务就完成了。在公式 $\eqref{15}$ 中，$\rho_\pi$ 就相当于 $G$ 生成的数据分布，而 $\rho_E$ 就相当于真实数据分布。 首先引入 $\pi$ 和 $D$ 的函数逼近形式。定义一个策略网络 $\pi_\theta$ 和判别器网络 $D_w:\mathcal{S}\times \mathcal{A} \rightarrow (0,1)$。然后交替运用 Adam优化 $w$ 和运用TRPO 优化 $\theta$ 。 伪代码中对熵的求导如下： \nabla_\theta H(\pi_\theta) = \nabla_\theta \mathbb{E}_{\pi_\theta}[-\log \pi_\theta(a|s)] = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q_{\log}(s,a)], \\ where \;\; Q_{\log}(\bar{s},\bar{a}) = \mathbb{E}_{\pi_\theta}[-\log \pi_\theta(a|s) | s_0=\bar{s},a_0=\bar{a}] \tag{16}参考文献[1] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, AAAI’08, 2008. [2] B. D. Ziebart, J. A. Bagnell, and A. K. Dey. Modeling interaction via the principle of maximum causal entropy. In ICML, pages 1255–1262, 2010. [3] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &amp; Sons, 2014. [4] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proceedings ofthe 25th International Conference on Machine Learning, pages 1032–1039, 2008. [5] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings ofthe 21st International Conference on Machine Learning, 2004. [6] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In Proceedings ofthe 33rd International Conference on Machine Learning, 2016.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>生成对抗</tag>
        <tag>模仿学习</tag>
        <tag>学徒学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hindsight Experience Replay]]></title>
    <url>%2F2020%2F02%2F29%2FHindsight-Experience-Replay%2F</url>
    <content type="text"><![CDATA[1. HER 概念对于机器人的强化学习算法，存在的挑战就是奖励函数的设置，不但需要考虑如何反映任务的完成水平，同时还需要指导策略的优化。有时，奖励函数的设置还需涉及到具体领域的专业知识。如果不对这些复杂的工程问题进行考虑，而简单用奖励函数表示任务成功或失败，就会造成奖励系数问题。就算仔细设计了奖励函数，可扩展性和鲁棒性也不高。 为了解决奖励稀疏的问题，本文提出了 HER 算法。HER 算法可以针对稀疏和二进制奖励的情况，提高样本效率，进而避免涉及复杂的奖励函数。可以将 HER 算法与其他 off-policy 强化学习算法结合。二进制奖励是指奖励只有两种值，出现在任务成功或任务失败的时候。 HER 算法具备推理能力，能够实现系统中任意状态为目标的任务。HER 参考论文 [1] 的通用策略设计，对于输入，不仅输入当前状态，而且需要目标状态。 HER 背后的核心思想是，在每个 episode 中都回放不同的目标（即当前 episode 需要实现的目标），而不止是最终要实现的目标。 2. 相关知识2.1 RL 基础最优动作值函数（对于最优策略 $\pi^$，满足 $Q^{\pi^}(s,a) \ge Q^\pi(s,a)$，$\forall s \in S, \forall a \in A$）： Q^{*} (s,a) = \mathbb{E}_{s'\sim p(\cdot|s,a)} \left[ r(s,a) + \gamma \mathop{\max}_{a'\in A} Q^*(s',a') \right] \tag{1}2.2 DQNDQN 存在两种策略：行为策略和目标策略。行为策略执行的是 $\epsilon$-greedy 策略，目标策略执行的是 greedy 策略。 每次训练，使用行为策略产生轨迹 $(s_t,a_t,r_t,s_{t+1})$，并存储到回放池中。训练的目标函数是： \mathcal{L} = \mathbb{E}(Q(s_t,a_t) - y_t)^2 \\ y_t = r_t + \gamma \max_{a'\in A} Q(s_{t+1}, a') \tag{2}其中 $Q(s,a)$ 表示动作值函数的函数逼近。样本从回放池中采样。 2.3 DDPGDDPG 是一种采用 AC 框架的算法。A 表示策略网络：$\pi: S \rightarrow A$，C 表示动作值网络：$Q: S \times A \rightarrow \mathbb{R}$ 。 同样也采用行为策略在每个 episode 中产生轨迹。行为策略的表达式为 $\pi_b(s) = \pi(s) + \mathcal{N}(0,1)$ ，表示一个确定策略，，加上一个随机分布（分布类型可以自定义）。critic 的目标与 DQN 的训练目标相似，只不过目标策略不是 greedy 策略，而是确定策略 $\pi(s)$ 。critic 的目标为：$y_t = r_t + \gamma Q(s_{t+1},\pi(s_{t+1}))$ 。actor 的目标函数为 $\mathcal{L}_a = -\mathbb{E}_s Q(s,\pi(s))$ 。样本同样从回放池中采样。 2.4 UVFA全称为 Universal Value Function Approximators，出自论文 [1] 。UVFA 是 DQN 的一个扩展，设立了不止一个目标去优化。假设其中一个目标 $g \in \mathcal{G}$，对应的奖励函数写为：$r_g: S \times A \rightarrow \mathbb{R}$ 。 每个 episode 开始时，都根据某种分布采样一个 state-goal 分布，记为 $p(s_0,g)$ 。这个目标 $g$ 将在这个 episode 中保持不变。接下来每个时间步长，智能体的输入不仅是状态 $s_t$，同时还有目标 $g$，因此策略的形式可以表示为：$\pi: S \times \mathcal{G} \rightarrow A$，奖励函数的形式为 $r_t = r_g(s_t,a_t)$ 。 动作值函数同样也不止依赖于状态动作对，也依赖于目标 $g$ ，表示为 $Q^\pi(s_t,a_t,g) = \mathbb{E}[R_t|s_t,a_t,g]$ 。 3. 具体算法定义一个谓语：$f_g:S \rightarrow \{0,1\}$ 。目标表示为能够实现满足 $f_g(s) = 1$ 的任意状态。 例如，如果指定状态空间的某一状态为目标，则 $f_g(s) = [s=g]$ 。如果指定状态空间某一维度的值为目标（假设 $S=\mathbb{R}^2$），则 $f_g((x,y)) = [x = g]$ 。 另外假设，对于状态空间的任意状态，我们都能找到一个目标，使得该状态满足要求。用数学符号定义为：存在一个映射 ：$m: S \rightarrow \mathcal{G} \;\; s.t. \;\; \forall_{s \in S} f_{m(s)}(s)=1$ 。最简单的，如果该映射是个很恒等映射，即 $\mathcal{G} = S$，$f_g(s) = [s=g]$， 显然该假设成立。 再举一个例子，如果状态空间是二维的，目标空间是一维的，则该假设也能够成立，例如其中一种情况为 $m((x,y))=x$ 。 HER 背后的思想是，在经历过一些 episode 之后，即经验回放池中存储着一系列经验和每个 episode 的目标（目标空间的一个子集）。HER 需要决定的是每次回放哪些目标的轨迹。文中称之为回放策略 $\mathbb{S}$。 HER 可视为一种循序渐进的教学，先从简单的容易实现的目标回放，再到困难的最终的目标回放。 算法伪代码如下图所示： 伪代码中最重要的就是采样策略 $\mathbb{S}$ 。 4. 工程设置环境：7 个自由度的机械臂 任务设定：(1) 用机械臂将物体推向目标处。(2) 用机械臂给滑块施力，让它滑到目标处。(3) 用机械臂夹起物体并放置到空间某处。 状态空间：机械臂的关节角度和速度（包含线性速度、角速度），关节位置，物体的转动和速度。 目标：目标设定为靠近某个状态下物体的位置，表示为 $f_g(s)=[|g-s_{object}| \le \epsilon]$ ，其中 $s_{object}$ 表示状态 $s$ 下物体的位置。 奖励：$r(a,s,g) = -[f_g(s’)=0]$，其中 $s’$ 表示在 状态 $s$ 下执行动作 $a$ 后的下一个状态。这表示如果 $s’$ 不满足目标 $g$ 的要求，则奖励设置为 -1，否则设置为 1 。 动作：四维输出，前面三维是抓手的绝对位置，最后一维是抓手的两支手指的距离。注意机械臂的控制是通过运动学进行的，根据抓手期待的位置去控制。 目标回放策略 $\mathbb{S}$：文中总共设计了四种策略。 final：每个 episode 的最后一个状态作为目标。 future：从当前的 episode 中收集可能在未来遇见的状态作为目标。 episode：从当前的 episode 中随机采样状态作为目标。 random：从所有遇见的状态中随机采样状态作为目标。 参考文献[1] Schaul, T., Horgan, D., Gregor, K., &amp; Silver, D. (2015). Universal value function approximators. 32nd International Conference on Machine Learning, ICML 2015, 2, 1312–1320. International Machine Learning Society (IMLS).]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>HER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement learning with unsupervised auxiliary tasks]]></title>
    <url>%2F2020%2F02%2F25%2FReinforcement-learning-with-unsupervised-auxiliary-tasks%2F</url>
    <content type="text"><![CDATA[1. 摘要传统的强化学习目标是最大化累积奖励函数。本文提出一种强化学习方法，目标改为同时最大化多个伪奖励函数，相当于增加了辅助目标。 奖励信号有时是难以观测的，稀少的。即使奖励信号很频繁，运动传感器数据也包含大量其他可能存在的学习目标。本文的目的是预测和控制运动传感器数据的特征，将它们当作强化学习的伪奖励信号进行训练（换句话来说，就是通过这些伪奖励信号的训练，改变网络的特征提取）。直觉上，如果智能体能够预测和控制它未来的经历，很容易实现一个长远的复杂的目标。 本文通过多个不同伪奖励信号对策略和状态函数进行学习，并进行一些辅助预测使得智能体侧重于任务的关键环节。本文采用了一种经验重放机制，主要重放包含奖励事件的一系列经验。为了让伪奖励信号和辅助预测影响到基础决策，本文共享一个 CNN 和 LSTM 网络。本文并没有直接利用这些伪奖励信号所对应的策略，而是作为辅助目标来学习一个更有效的表示（representation learning）。（换句话来说，这些伪奖励信号学习出来的策略并没什么卵用，只是他们学习的过程会影响到底层网络的特征表示的学习） 本文对提出的算法起了一个名字：UNREAL（UNsupervised Reinforcement and Auxiliary learning），以下简称 UNREAL 算法。这个 unsupervised 强调的是所有辅助任务的训练都不需要借助环境的额外信号来监督。（但是后面用到了过去的经验来进行监督学习。） 2. Auxiliary tasksUNREAL 方法定义了辅助任务，并结合到强化学习的框架中。UNREAL 的架构图如下： 2.1 Auxiliary control tasks辅助控制任务在文中的定义为伪奖励函数。定义辅助控制任务 $c$ 为奖励函数 $r^{(c)} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$，其中状态空间 $\mathcal{S}$ 包括观测历史、奖励以及智能体自身的状态。 假设给定辅助控制任务集合为 $\mathcal{C}$，令 $\pi^{c}$ 为辅助控制任务的策略，$\pi$ 为基础任务的策略，则整体目标定义为最大化基础任务和所有辅助任务的性能： \mathop{\arg\max}_\theta \mathbb{E}_\pi [R_{1:\infty}] + \lambda_c \sum_{c \in \mathcal{C}} \mathbb{E}_{\pi_c} [R_{1:\infty}^{(c)}] \tag{1}其中 $R_{1:\infty}$ 表示策略的折扣累计回报，$\theta$ 是策略 $\pi$ 和所有辅助策略 $\pi^{(c)}$ 的参数集合。通过共享 $\pi$ 和 $\pi^{(c)}$ 的部分参数，算法必须平衡好全局奖励 $r_t$ 和辅助任务的奖励。 为了有效地同时对多个辅助控制任务进行训练，本文提出应该采用 off-policy 强化学习方法。本文采用的是 value-based 的强化学习方法：n-step Q-learning 。对于每个辅助控制任务，定义目标函数如下： \mathcal{L}_Q^{(c)} = \mathbb{E} \left[ \left(R_{t:t+n}^{(c)} + \gamma^n \max_{a'} Q^{(c)}(s',a',\theta^-) - Q^{(c)}(s,a,\theta) \right)^2 \right] \tag{2} \label{2}本文定义的辅助奖励函数主要由以下两种类型： Pixel changes 。视觉感知流发生变化通常对应某些重要事件的发生。本文通过训练独立的策略来最大化图像输入流每个单元（大小为$4 \times 4$）的像素变化，作为一个辅助控制任务。（像素控制辅助任务） Network features 。由于神经网络可以提取环境的高级特征，这对于智能体的控制而言是有用信息。因此本文神经网络的每个隐藏单元的激活状态作为辅助奖励，尽可能在某一指定的隐藏层中激活每个单元。（特征控制辅助任务） 在架构图中可以看到基础策略 $\pi$ 和像素控制辅助策略共享了底层的卷积网络和 LSTM 网络。在图 (b) 中可以看到，辅助网络的输出是 $Q^{aux}$，大小为 $N_{act} \times n \times n$ 的张量，$Q^{aux}(a,i,j)$ 表示对输入图像的第 $(i,j)$ 个单元，采用动作 $a$ 之后，像素的变化期望值。辅助网络是一个反卷积网络。 对于特征控制，针对第二层卷积层（$32 \times 9 \times 9$ 的特征图谱），同样采用反卷积网络作为辅助网络，输出 $Q^{aux}$ 。 其实文中最后只用了像素控制辅助任务。 2.2 Auxiliary reward tasks为了学习到使奖励最大化的策略，智能体需要有能力识别产生高回报的状态。但是很多环境的奖励信号都是稀疏的，这意味着训练可以识别高回报状态的特征提取器需要很长时间。为了解决奖励稀疏的问题，同时不给策略和状态的学习引入偏差，本文提出奖励预测的概念。 奖励预测也是一种辅助任务。给定历史状态的上下文（一个序列的连续观测值），奖励预测辅助任务可以给出即时奖励的预测。和状态值估计不同，奖励预测需要估计的是回报值，并只将估计的回报值用于智能体的特征塑形（feature shaping），这样就不会给数据分布带来偏差，即不会给策略和状态函数带来偏差。特征塑形是指。。。。 为了训练一个奖励预测器，从策略 $\pi$ 的经验池中抽取状态序列 $S_\tau = (s_{\tau-k},s_{\tau-k+1},\cdots,s_{\tau-1})$ （预测目标就是这个序列的真实奖励值）。抽样时，尽量使得零奖励和非零奖励的概率大致相等。为了对奖励预测器进行训练，定义目标损失函数 $\mathcal{L}_{RP}$，采用的是多类别交叉熵分类损失函数，类别是 (zero, positive, or negative reward)。采用均方根误差函数其实也可以。 训练网络是策略和状态网络的 CNN 层加上一个简单的前向反馈网络。如图 (c) 所示。输入是状态序列 $S_\tau$（从图中来看，主要是连续的三帧图像）。 通过奖励预测这种方式影响的特征表示会被基础策略的 LSTM 网络共享，因此能够让策略学习得更有效率。 2.3 经验回放为了执行辅助奖励预测，本文将经验回放池简单地分成两个子集，一个是有奖励的子集，另一个是无奖励的子集。两个子集的回放概率相等。这意味着本身稀少的带有奖励的状态被过采样，可以被视为一种优先经验回放机制。 本文同时采用经验回放池来执行状态函数回放。状态函数回放的意义是指输入近期的历史连续状态序列，通过网络输出额外的状态函数值估计。这与基础策略的状态值函数区分开，基础的状态值函数的输入的当前的状态，而状态回放函数输入是近期的历史连续状态序列。如图 (d) 所示。这种经验回放不需要进行划分。 经验回放同时也用于其他辅助任务的训练，例如 2.1 节中的 $Q^{aux}$。辅助任务的经验回放通常是采样最近的经验。 3. UNREAL 算法UNREAL 算法的主体是 A3C 算法，借助于并行异步强化学习框架，提高样本效率和稳定性。同时采用 RNN 网络对完整的历史经验进行编码，可以更好地学习到环境的特征。 UNREAL 的损失函数可以表示为四种损失函数的简单结合形式： \mathcal{L}_{UNREAL}(\theta) = \mathcal{L}_{A3C} + \lambda_{VR}\mathcal{L}_{VR} + \lambda_{PC}\sum_c\mathcal{L}_{PC}^{(c)} + \lambda_{RP}\mathcal{L}_{RP} \tag{3}$\mathcal{L}_{A3C}$ 表示 A3C 损失，$\mathcal{L}_{VR}$ 表示状态回放函数损失， $\mathcal{L}_{PC}$ 表示辅助控制函数损失损失，$\mathcal{L}_{RP}$ 表示辅助奖励预测损失。$\lambda_{VR}$、$\lambda_{PC}$ 、$\lambda_{RC}$ 是相应的权重。 $\mathcal{L}_{A3C}$ 使用的是 on-policy；$\mathcal{L}_{VR}$ 需要使用经验回放池近期的历史状态序列和 A3C 估计的状态值；$\mathcal{L}_{PC}$ 通过经验回放以及 n-step Q-learning 方法训练；$\mathcal{L}_{RP}$ 需要使用重新划分的经验回放池。 4. 工程设置 输入：连续四帧的 $84 \times 84$ RGB 图像 卷积层：前面两层都是卷积层。第一层有 16 个 $8 \times 8$，步长为 4 的滤波器。第二层有 32 个 $4 \times 4$ ，步长为 2 的滤波器。 全连接层：总共有 256 个单元。 激活函数：以上三层均采用 ReLU 激活函数。 LSTM 层：带 forget gates，256 个单元。输入为 CNN 编码的观测值，以及先前的动作和当前的奖励。 策略函数和状态函数：都是 LSTM 层的线性映射。 episode length：20 个时间步长。 像素控制辅助任务： 将图像中间的 $80 \times 80$ 的区域分成 $20 \times 20$ 的方格，每个格子为一个 $4 \times 4$ 的单元。辅助任务的即时奖励定义为每个单元的平均像素和上一帧对应单元的平均像素的绝对值。 $Q^{aux}$ 产自 LSTM 网络的输出和反卷积网络。首先将 LSTM 的输出通过全连接网络，ReLU 激活函数，映射为 $32 \times 7 \times 7$ 的特征图。然后利用反卷积层，带有 $N_{act}$ 个，大小为 $4 \times 4$，步长为 2 的滤波器，将 $32 \times 7 \times 7$ 的特征图映射为独立的状态值张量和优势值张量（采用 dueling DQN 中的技术），最后合并为 $N_{act} \times 20 \times 20$ 的 $Q^{aux}$ 。 特征控制辅助任务： 对于第二层卷积层的输出（$32 \times 9 \times 9 $），同样也采用反卷积网络映射为 $Q^{aux}$ 。 奖励预测任务输入为三个连续的观测值。经过agent 的 CNN 编码输出后，这三个输出值连接一个 128 个单元的全连接网络，采用 ReLU 激活函数，最后是 softmax 层，输出三个概率值（正奖励、负奖励、零奖励的概率）。$\lambda_{RP}$ 设置为 1。 状态回放函数采用长度为 20 的历史连续状态序列。 经验回放池存储最近 2k 个经验（观测值、动作、奖励值）。 采用 32 个并行线程，RMSprop 优化算法，学习率从 0.0001 和 0.005 之间 log 均匀采样。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>unsupervised auxiliary tasks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACKTR论文笔记]]></title>
    <url>%2F2020%2F02%2F23%2FACKTR%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. ACKTR 概念本文 [1] 涉及到一个新术语：Kronecker 因子的近似曲率，以下简称：K-FAC。本文结合三个东西，一个是 K-FAC，一个是置信域优化，另一个是 AC。作者称其为第一个可扩展的 AC 框架的置信域优化方法，可以理解为精度和速度可以调节。 置信域优化方法是为了应对 SGD 及相关一阶优化方法的探索效率不足的问题，这往往导致训练时间过长。另外 A3C 也采用多线程的异步训练方法来应对探索效率问题，但是异步训练对样本的利用效率太低。样本的利用效率往往也十分关键，因为在现实世界，机器人与环境交互的数据量远远不满足训练数据需要的规模和增长速率，即使在仿真环境中，仿真器的运行时间也可以超过计算时间。 在这篇文章之前，采用过高级优化方法的技术有自然策略梯度方法以及 TRPO 方法。这两种方法的计算复杂度都很高。有学者提出 K-FAC 方法 [2, 3]，这是对自然梯度方法的一种可扩展的近似，已经用来加速各种大型网络的监督训练。 2. 相关知识2.1 策略梯度和AC 方法策略梯度的一般形式可以表示为： \nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_\pi[\sum_{t=0}^\infty \Psi^t \nabla_\theta \log \pi_\theta(a_t|s_t)] \tag{1}其中 $\Psi^t$ 通常采用优势函数 $A^\pi(s_t,a_t)$。本文对优势函数的估计采用 A3C 中的 k-step 估计方法： A^\pi(s_t,a_t) = \sum_{i=0}^{k-1} \left( \gamma^i r(s_{t+i}, a_{t+i})+\gamma^k V_\Phi^\pi(s_{t+k}) \right) - V_\Phi^\pi(s_t) \tag{2}其中 $V_\Phi^\pi$ 表示价值网络，用来逼近策略的回报总和：$V_\Phi^\pi = \mathbb{E}_\pi[R_t]$ 。对价值网络的训练方法采用最小化均方根误差：$\frac{1}{2}| \hat{R}_t - V_\Phi^\pi(s_t) |$ 。 2.2 采用 K-FAC 逼近自然梯度为了更新非凸目标函数 $\mathcal{J}(\theta)$，最速下降方法通过更新步长 $\Delta \theta$ 来最小化 $\mathcal{J}(\theta+\Delta\theta)$，同时满足约束条件 $| \Delta\theta |_B &lt; 1$。$| \cdot |_B$ 定义为 $| x |_B = (x^T B x)^{\frac{1}{2}}$，其中 B 是半正定矩阵。解以上的待约束优化问题，可以得到 $\Delta \theta \propto -B^{-1} \nabla_\theta \mathcal{J}$，并且 $\nabla_\theta \mathcal{J}$ 就是标准的梯度。假设 $B=I$，那么就是标准的梯度下降方法，$| x |_I$ 就是欧几里德范式（二范式）。论文中说，欧几里德范数的变化取决于参数 $\theta$，但这对于随机参数化的模型来说不是一个好的选择，这会影响到优化轨迹。 自然梯度与随机梯度方法的差别在于，它利用费舍尔信息矩阵 $F$ （KL 散度的本地二次型近似）来构造范式，而不是标准矩阵 $I$。这种范式独立于模型参数 $\theta$，属于一种概率分布。自然梯度可以提供更加稳定和有效的梯度更新。但是对于现在的深度神经网络，参数高达百万级别，计算费舍尔矩阵和它的逆基本上是不现实的。 K-FAC 方法利用 Kronecker-factored 来近似费舍尔信息矩阵，从而能够实现自然梯度更新。 令 $p(y|x)$ 表示神经网络输出的概率分布，$L = \log p(y|x)$ 表示对数似然函数。令 $W \in \mathbb{R}^{C_{out} \times C_{in}}$ 表示神经网络第 $\ell$ 层的参数矩阵，其中 $C_{out}$ 和 $C_{in}$ 表示该层神经网络的输出和输入神经元。令输入的激活向量表示为 $a \in \mathbb{R}^{C_{in}}$ ，输出到下一层的未激活的向量表示为 $s = W a$ 。 根据求导的链式法则，可以知道 $\nabla_W L = (\nabla_s L) a^T$，因此对于 $\ell$ 层参数的费舍尔矩阵 $F_\ell$ 可以近似比表示为 $\hat{F}_\ell$： \begin{aligned} F_\ell &= \mathbb{E}[\mathscr{vec}\{\nabla_W L\} \mathscr{vec}\{\nabla_W L\}^T] = \mathbb{E}[aa^T \otimes \nabla_s L(\nabla_s L)^T] \\ &\approx \mathbb{E}[aa^T] \otimes \mathbb{E}[\nabla_s L(\nabla_s L)^T] := A \otimes S := \hat{F}_\ell \end{aligned} \tag{3}上式中 $\otimes$ 表示矩阵 Kronecker（克罗内克）积，也是本方法名字的由来。令 $A$ 表示 $\mathbb{E}[aa^T]$，$S$ 表示 $\mathbb{E}[\nabla_s L(\nabla_s L)^T]$。这个近似可以解释为：假设输入的激活向量的二阶统计与后向传播的导数无关。 根据矩阵克罗内克积的性质，有：$(P \otimes Q) ^{-1} = P^{-1} \otimes Q^{-1}$，$(P \otimes Q) \mathscr{vec}(T) = PTQ^{T}$ 。因此对于自然梯度更新： \mathscr{vec}(\Delta W) = \hat{F}_\ell^{-1}\{ \nabla_W \mathcal{J} \} = \mathscr{vec}(A^{-1} \nabla_W \mathcal{J} S^{-1}) \tag{4} \label{4}公式 $\eqref{4}$ 的计算复杂度与 $|W|$ 的规模相当，而直接计算费舍尔矩阵向量乘积和求逆复杂度是 $|W|^2$ 的规模。 3. 算法框架3.1 K-FAC + AC本文利用 K-FAC 方法逼近自然梯度，然后将自然梯度更新用于 AC 框架（既用于 actor，也用于 critic 的更新）。 强化学习目标函数（即 actor 的目标函数）的费舍尔矩阵可以定义为： F = \mathbb{E}_{p(\tau)} [\nabla_\theta log \pi(a_t|s_t) \nabla_\theta \log \pi(a_t|s_t)^T] \tag{5}其中 $p(\tau)$ 表示轨迹的分布，完整表达式为 $p(\tau) = p(s_0) \prod_{t=0}^T \pi(a_t|s_t)p(s_{t+1}|s_t,a_t)$ 。实际上都是通过在训练过程采样轨迹求平均的方法来求以上的期望，通过表达式求期望太难了。 critic 的目标函数是均方根误差函数，可视为最小二乘逼近问题。对于最小二乘逼近问题，常见的二阶优化方法是 Gauss-Newton 方法，Gauss-Newton 对函数曲率的逼近定义为 $G := \mathbb{E}[J^TJ]$，其中 $J$ 是参数的雅克比矩阵。 假设观测模型为高斯观测模型，则 Gauss-Newton 矩阵等价于费舍尔矩阵。因此也可以将 K-FAC 方法计算 Gauss-Newton 矩阵。为了建立高斯观测模型，本文将 critic 的输出 $v$ 定义为高斯分布：$p(v|s_t) \sim \mathcal{N}(v;V(s_t), \sigma^2)$ 。另外，为了简便，将 $\sigma$ 设置为 1，等价于 vanilla Gauss-Newton 方法。此时 critic 的费舍尔矩阵就是 Gauss-Newton 矩阵。 为了可以将 K-FAC 方法同时运用于 actor 和 critic 的更新中，可以让 actor 和 critic 共享同一个网络，除了输出层不一样。可以定义一个策略分布（actor 的输出）和状态分布（critic 的输出）的联合分布：$p(a,v|s)=\pi(a|s)p(v|s)$，然后构造关于 $p(a,v|s)$ 费舍尔矩阵：$\mathbb{E}_{p(\tau)} [\nabla \log p(a,v|s)\nabla \log p(a,v|s)^T]$ 。 本文还对目标函数运用 Tikhonov 正则项 [4]。 3.2 置信域优化自然梯度在更新时和随机梯度类似：$\theta \leftarrow \theta - \eta F^{-1} \nabla_\theta L$，这种更新方式有时会导致较大的更新步长。论文 [5] 中将置信域优化和 K-FAC 结合起来，以置信域的方式进行更新。置信域步长：$\min(\eta_{\max}, \sqrt{\frac{2\delta}{\Delta\theta \hat{F} \Delta\theta}})$，其中学习率 $\eta_{\max}$ 和置信域范围参数 $\delta$ 都是超参数。 4. 工程设置对比算法：A2C、TRPO，采用相同的模型结构。 训练时长：1e7 以上的时间步长，1 个时间步长等于 4 帧。 Adaptive Gauss-Newton：对 critic 输出分布的方差用贝尔曼误差的方差进行估计。贝尔曼误差反映的是当前的状态价值与更新后的状态价值差的绝对值。 4.1 离散控制网络结构：第一层卷积层采用 32 个 $8 \times 8$ ，步长为 4 的滤波器。第二层卷积层采用 64 个 $4 \times 4$ ，步长为 2 的滤波器。第三层卷积层采用 32 个 $3 \times 3$ ，步长为 1 的滤波器。第四层全连接层采用 512 个神经元。softmax 输出层输出动作，线性输出层输出状态值。 actor 和 critic 共享网络结构（除了输出层）。 最大学习率 $\eta_{\max}$ 在 {0.7,0.2,0.07,0.02} 中选取，置信域半径 $\delta$ 设置为 0.001。 4.2 连续控制对于低维度状态空间作为输入时，采用两个独立的网络来估计 actor 和 critic 。两层全连接网络，每层 64 个隐藏单元。actor 网络的激活函数采用 Tanh，critic 网络的激活函数采用 ELU。输出层不采用激活函数。actor 网络输出高斯策略分布，标准差作为最后一层的偏置加入，与状态输入无关。 对于高维度 $42 \times 42$ RGB 图像状态空间作为输入时，前面两层卷积层采用 32 个 $3 \times 3$，步长为 2 的滤波器，最后一层全连接层采用 256 个神经元。actor 和 critic 采用独立网络。actor 网络的激活函数采用 ReLU，critic 网络的激活函数采用 ELU。 最大学习率 $\eta_{\max}$ 在 {0.3,0.03,0.003} 中选取，置信域半径 $\delta$ 设置为 0.001。 actor 网络和 critic 网络都采用正交初始化。 参考文献[1] Wu, Y., Grosse, R., &amp; Ba, J. (n.d.). Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation. 1–14. [2] Martens J , Grosse R . Optimizing Neural Networks with Kronecker-factored Approximate Curvature[J]. 2015. [3] R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolutional layers. In ICML, 2016 [4] Martens J , Grosse R . Optimizing Neural Networks with Kronecker-factored Approximate Curvature[J]. 2015. [5] J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using Kronecker- factored approximations. In ICLR, 2017.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACER论文笔记]]></title>
    <url>%2F2020%2F02%2F12%2FACER%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. ACER 概念ACER 就是将 Actor-Critic 和 experience replay 结合起来的强化学习方法。本文 [1] 提出了几种新的技术：带偏差纠正的截断重要性采样、随机 dueling 网络结构、一种新的置信域策略优化方法。 提出 ACER 的概念源于智能体与环境交互的代价高昂，需要减少仿真次数，提高数据的样本利用率。ACER 的出现旨在于设计一个稳定的、样本利用率高的 actor-critic 方法。 2. 相关知识假设可微分策略符号为 $\pi_\theta(a_t|x_t)$，其中 $x_t$ 是状态输入，策略梯度可以表示为： g = \mathbb{E}_{x_0:\infty, a_0:\infty} \left[ \sum_{t\ge0} A^\pi(x_t,a_t)\nabla_\theta \log\pi_\theta(a_t|x_t) \right] \tag{1}论文 [2] 的 Proposition 1 提出，可以将 $A^\pi(x_t,a_t)$ 替换为 $Q^\pi(x_t,a_t)$、折扣回报 $R_t$ 或者时间差分 $r_t+\gamma V^\pi(x_t+1)-V^\pi(x_t)$，不会引入偏差，只是四种式子的方差不一样。 通常直接计算折扣回报 $R_t$ 会引入较大的方差，但是只有较小的偏差。而使用函数逼近状态函数则会引入较大偏差，但是只有较小的方差。通过结合 $R_t$ 和函数逼近两种方法，即保证较小偏差也维持固定方差是 ACER 背后的设计思想。 为了对偏差和方差的权衡，A3C 论文中提出了 $k$-step 回报的方法，通过结合 $k$ 步的奖励以及函数逼近来对方差和偏差进行折衷： \hat{g}^{A3C} = \sum_{t\ge0} \left( \left( \sum_{i=0}^{k-1}\gamma^ir_{t+i} \right) + \gamma^k V_{\theta_v}^\pi(x_{t+k}) - V_{\theta_v}^\pi(x_t) \right) \nabla_\theta \log\pi_\theta(a_t|x_t) \tag{2}重要性采样是离线强化学习方法中常见的技术，用于控制方差和训练的稳定性。假设当前从经验回放池中采样的一段轨迹为 $\{x_0,a_0,r_0,\mu(\cdot|x_0),\cdots,x_k,a_k,r_k,\mu(\cdot|x_k)\}$，其中 $\mu$ 是行为策略，则使用重要性采样因子的策略梯度为： \hat{g}^{imp} = \left( \prod_{t=0}^k \rho_t \right) \sum_{t=0}^k \left( \sum_{i=0}^k \gamma^i r_{t+i} \right) \nabla_\theta \log \pi_\theta(a_t|x_t) \tag{3}$\rho_t$ 就是重要性因子： \rho_t = \frac{\pi(a_t|x_t)}{\mu(a_t|x_t)} \tag{4}3. 具体算法3.1 离散 ACER重要性采样是离线强化学习方法中常见的技术，用于控制方差和训练的稳定性。假设当前从经验回放池中采样的一段轨迹为 $\{x_0,a_0,r_0,\mu(\cdot|x_0),\cdots,x_k,a_k,r_k,\mu(\cdot|x_k)\}$，其中 $\mu$ 是行为策略，则使用重要性采样因子的策略梯度为： \hat{g}^{imp} = \left( \prod_{t=0}^k \rho_t \right) \sum_{t=0}^k \left( \sum_{i=0}^k \gamma^i r_{t+i} \right) \nabla_\theta \log \pi_\theta(a_t|x_t)\tag{5}$\rho_t$ 就是重要性因子： \rho_t = \frac{\pi(a_t|x_t)}{\mu(a_t|x_t)}\tag{6}上面提到的重要性采样虽然不会带来偏差，但是会引来很大的方差，因为式子中包含无界限的重要性因子 $\rho_t$ 的连乘操作。可以对重要性采样因子进行截断，这是一种解决办法。Degris 提出了另一种利用边缘状态函数的方法。 3.1.1 Marginal value functionDegris [3] 提出利用极限分布上的边缘状态函数来逼近策略梯度： g^{marg} = \mathbb{E}_{x_t \sim \beta, a_t \sim \mu} [\rho_t \nabla_\theta \log \pi_\theta(a_t|x_t)Q^\pi(x_t,a_t)] \tag{7} \label{7}$\beta(x)$ 是行动策略 $\mu$ 的极限分布：$\beta(x) = lim_{t \rightarrow \infty} P(x_t=x|x_0,\mu)$，上式中没有重要性因子的连乘，只需要对边缘重要性因子 $\rho_t$ 进行估计，因此可以降低方差。 3.1.2 Retrace为了估计 $Q^\pi(x_t,a_t)$，本文使用了 Retrace($\lambda$) [4] 技术。假定一段由行为策略 $\mu$ 采样的轨迹，Retrace($\lambda$) 估计方法可以表示为： Q^{ret}(x_t, a_t) = r_t + \gamma \overline{\rho}_{t+1}[Q^{ret}(x_{t+1},a_{t+1})-Q(x_{t+1},a_{t+1})] + \gamma V(x_{t+1}) \tag{8}上式的 $\lambda = 1$。其中 $\hat{\rho}_t$ 是截断重要性因子，$\hat{\rho}_t = min\{c,\rho_t\}$，$\rho_t = \frac{\pi(a_t|x_t)}{\mu(a_t|x_t)}$，Q 是当前对 $Q^\pi$ 的估计。在论文 [4] 中一经证明 Retrace 技术拥有较低的方差，并且对于 tabular 强化学习，任意的行为策略都能使目标策略的状态函数收敛。 为了对 $Q$ 进行估计，采用双头输出的卷积网络，一头输出动作状态的估计值 $Q_{\theta_v}(x_t,a_t)$，另一头输出策略 $\pi_\theta(a_t|x_t)$ 。为了获得$g^{marg}$，ACER 采用 $Q^{ret}$ 来估计 $Q^\pi$ 。为了对 $Q_{\theta_v}(x_t,a_t)$ 进行学习，本文利用 $Q^{ret}$ 为目标函数，建立均方差损失函数来对参数 $\theta_v$ 进行学习： (Q^{ret}(x_t,a_t)-Q_{\theta_v}(x_t,a_t))\nabla_{\theta_v} Q_{\theta_v}(x_t,a_t) \tag{9}采用 $Q^{ret}$ 有两个好处：一是降低策略梯度的偏差，而是加快 critic 的学习。 3.1.3 bias correction公式 ($\ref{7}$) 中的边缘重要性因子可能估计比较大，导致训练不稳定。为了更好地降低策略梯度的方差，本文提出一种截断的重要性因子，并引入一个修正项，如下公式所示（公式 ($\ref{7}$) 中的 $\mathbb{E}_{x_t \sim \beta, a_t \sim \mu}$ 简写为 $\mathbb{E}_{x_t,a_t}$）： \begin{aligned} g^{marg} &= \mathbb{E}_{x_t,a_t} [\rho_t \nabla_\theta \log \pi_\theta(a_t|x_t)Q^\pi(x_t,a_t)] \\ &= \mathbb{E}_{x_t} \left[ \mathbb{E}_{a_t}[\overline{\rho}_t \nabla_\theta \log \pi_\theta(a_t|x_t)Q^\pi(x_t,a_t)] + \mathbb{E}_{a \sim \pi} \left( \left[ \frac{\rho_t(a)-c}{\rho_t(a)} \right]_+ \nabla_\theta \log \pi_\theta(a|x_t)Q^\pi(x_t,a) \right) \right] \end{aligned} \tag{10} \label{10}其中截断重要性因子 $\overline{\rho}_t = \min\{c,\rho_t\}$，$\rho_t=\frac{\pi(a_t|x_t)}{\mu(a_t|x_t)}$。另外关于动作 $a \sim \pi$ 有 $\rho_t(a)=\frac{\pi(a|x_t)}{\mu(a|x_t)}$，并且 $[x]_+$ 表示只有当 $x&gt;0$ 时，$[x]_+ = x$，否则等于 0。 公式 ($\ref{10}$) 的第一项表示策略梯度估计的方差被限制了，第二项表示修正项，保证估计是无偏的。注意到第二项中只有当 $\rho_t(a) &gt; c$ 时偏正项起作用。假设我们选取较大的 $c$ 值，只有当公式 ($\ref{7}$) 中的策略梯度估计方差非常大时，修正项才会起作用。从公式 ($\ref{10}$) 中可以知道第一项截断权重的最大值为 $c$，第二项修正权重的最大值为 1。 本文将修正项中的 $Q^\pi(x_t,a)$ 用神经网络的输出值 $Q_{\theta_v}(x_t,a_t)$ 进行逼近，而第一项依然用 $Q^{ret}$，最终公式 ($\ref{10}$) 变成以下形式，也就是本文提出的带偏差纠正的截断重要性采样技术： \hat{g}^{marg} = \mathbb{E}_{x_t} \left[ \mathbb{E}_{a_t}[\overline{\rho}_t \nabla_\theta \log \pi_\theta(a_t|x_t)Q^{ret}(x_t,a_t)] + \mathbb{E}_{a \sim \pi} \left( \left[ \frac{\rho_t(a)-c}{\rho_t(a)} \right]_+ \nabla_\theta \log \pi_\theta(a|x_t)Q_{\theta_v}(x_t,a) \right) \right] \tag{11} \label{11}公式 ($\ref{11}$) 带有 Markov 过程的平稳分布的期望，可以通过行为策略 $\mu$ 的采样轨迹来取均值逼近。因此 ACER 的策略梯度逼近公式为： \hat{g}_t^{acer} = \overline{\rho}_t \nabla_\theta \log \pi_\theta(a_t|x_t)[Q^{ret}(x_t,a_t)-V_{\theta_v}(x_t)] \\ \;\;\;\;\;\;\;\;\;\;\; + \mathbb{E}_{a \sim \pi} \left( \left[ \frac{\rho_t(a)-c}{\rho_t(a)} \right]_+ \nabla_\theta \log \pi_\theta(a|x_t) [Q_{\theta_v}(x_t,a)-V_{\theta_v}(x_t)]\right) \tag{12} \label{12}注意到，当 $c=\infty$ 时，公式 ($\ref{12}$) 就相当于只有第一项，当 $c = 0$ 时，公式 ($\ref{12}$) 就相当于只有第二项。 3.1.4 置信域策略优化actor-critic 框架的方法经常会出现较大的方差，本文结合 TRPO 中的置信域优化的思想，将 actor-critic 框架和置信域优化结合起来，限制每次更新的幅度。 TRPO 中需要反复求解费舍尔向量乘积，需要非常大的计算量。本文不用像 TRPO 那样约束新策略与旧策略相近，而是通过维护一个平均策略网络，表示旧策略的滑动平均，并且强制更新后的新策略不能偏离这个均值太远。 首先将策略网络分成两个部分，一个是分布类型 $f$，另一个是深度神经网络输出，代表这个分布的统计特性 $\phi_\theta(x)$。也就是说策略可以表示成：$\pi(\cdot|x)=f(\cdot|\phi_\theta(x))$。为了对平均策略网络 $\phi_{\theta_a}$ 进行更新，本文采用软更新的方法，即 $\theta_a = \alpha \theta_a + (1-\alpha)\theta$ 。 接下来，将公式 ($\ref{12}$) 用 $\phi_\theta$ 表示： \hat{g}_t^{acer} = \overline{\rho}_t \nabla_{\phi_\theta(x_t)} \log f(a_t|\phi_\theta(x))[Q^{ret}(x_t,a_t)-V_{\theta_v}(x_t)] \\ \;\;\;\;\;\;\;\;\;\;\; + \mathbb{E}_{a \sim \pi} \left( \left[ \frac{\rho_t(a)-c}{\rho_t(a)} \right]_+ \nabla_{\phi_\theta(x_t)} \log f(a|\phi_\theta(x)) [Q_{\theta_v}(x_t,a)-V_{\theta_v}(x_t)]\right) \tag{13} \label{13}为了进行置信域优化，总共分成两个步骤。 解决带 KL 散度约束的优化问题 结合平均策略网络和策略网络，构造二者的 KL 散度约束，如下公式所示： \begin{aligned} & \mathop{minimize}_z \;\; \frac{1}{2} \| \hat{g}_t^{acer} - z \|_2 ^ 2 \\ & subject \; to \;\; \nabla_{\phi_\theta(x_t)} D_{KL}[f(\cdot|\phi_{\theta_a}(x_t)) \| f(\cdot|\phi_\theta(x_t))]^T z \le \delta \end{aligned} \tag{14} \label{14}令 $k = \nabla_{\phi_\theta(x_t)}D_{KL}[f(\cdot|\phi_{\theta_a}(x_t)) | f(\cdot|\phi_\theta(x_t))]$ ，根据 KKT 条件和拉格朗日乘法，很容易解得 $z$ 的最优值为： z^* = \hat{g}_t^{acer} - max \left\{ 0,\frac{k^T \hat{g}_t^{acer} - \delta}{\|k\|_2^2} \right\}k \tag{15} \label{15}公式 ($\ref{14}$) 的约束可以理解为策略梯度的变化量不能超过范围 $\delta$，$k$ 表示策略梯度变化率，$z$ 表示策略梯度。 如果约束满足，则策略梯度没有产生任何改变。否则策略梯度沿着 $k$ 的方向降低变化。 后向传播 $z^$ 是关于 $\phi_{\theta}$ 的最优策略梯度，根据链式传播原则，神经网络中的参数可以通过后向传播更新：$\frac{\partial \phi_\theta(x)}{\partial \theta} z^$ 。 3.1.5 伪代码 从主算法中可以看出，先采用 ACER on-policy 算法，然后再根据回放比进行 $n$ 次 ACER off-policy 算法。 注意算法 2 中有一处的赋值 $\mu(\cdot|x_i) \leftarrow f(\cdot|\phi_{\theta’}(x_i))$，应该是作者在ACER on-policy 算法中故意让行为策略等于目标策略。要么就是笔误，右侧可能是 $f(\cdot|\phi_{\theta_a}(x_i))$ 。不过无所谓，问题不大。 3.2 连续 ACER本文在连续的动作空间对 ACER 做了两次修改，分别在策略估计和置信域优化两个方面。 3.2.1 策略估计修改Retrace 技术只学习了 $Q_{\theta_v}$，没有学习 $V_{\theta_v}$。作者觉得二者合一对状态的估计更加准确，方差更小。 本文设计了一种称为 Stochastic Dueling Networks 的网络结构，简称 SDNs，可以同时估计 $V^\pi$ 和 $Q^\pi$ 。网络的输出 $\tilde{Q}_{\theta_v}$ 是对 $Q^\pi$ 的随机估计值，另一个输出 $V_{\theta_v}$ 是对 $V^\pi$ 的确定估计值。$\tilde{Q}_{\theta_v}$ 的计算可以通过 $V_{\theta_v}$ 表示： \tilde{Q}_{\theta_v}(x_t,a_t) \sim V_{\theta_v}(x_t) + A_{\theta_v}(x_t,a_t) - \frac{1}{n}\sum_{i=1}^n A_{\theta_v}(x_t,u_i) \\ u_i \sim \pi_\theta(\cdot|x_t) \tag{16} \label{16} 同时还可以依据 $\tilde{Q}_{\theta_v}$ 和目标 $Q^{target}$ 间的差距来更新 $V_{\theta_v}$ 和 $V^{target}$。也就是说在某种意义上 $\tilde{Q}_{\theta_v}$ 和 $V_{\theta_v}$ 的更新是一致的。这种一致性可以通过以下两个等式来说明： \begin{aligned} &Assume \;\; \mathbb{E}_{u_{1:n}\sim \pi(\cdot|x_t)} \left( \tilde{Q}_{\theta_v}(x_t,a_t) \right) = Q^\pi(x_t,a_t) \\ &Then \;\;\;\;\;\;\, V_{\theta_v}(x_t) = \mathbb{E}_{a \sim \pi(\cdot|x_t)} \left[ \mathbb{E}_{u_{1:n}\sim \pi(\cdot|x_t)} \left( \tilde{Q}_{\theta_v}(x_t,a) \right) \right] = \mathbb{E}_{a \sim \pi(\cdot|x_t)} \left[ Q^\pi(x_t,a) \right] = V^\pi(x_t) \end{aligned} \tag{17} \label{17}本文通过推导，利用带修正项的重要性截断技术的形式，推导出一种新的 $V^{target}$ 表达形式（推导过程见文章附录 D）： V^{target}(x_t) = min \left\{1, \frac{\pi(a_t|x_t)}{\mu(a_t|x_t)} \right\} \left( Q^{ret}(x_t,a_t)-Q_{\theta_v}(x_t,a_t) \right) + V_{\theta_v}(x_t) \tag{18}最后本文在更新 $Q^{ret}$ 的参数时采用的截断重要性因子 $\overline{\rho}_t$ 也加了一点改变： \overline{\rho}_t = min \left\{ 1, \left( \frac{\pi(a_t|x_t)}{\mu(a_t|x_t)} \right)^{\frac{1}{d}} \right\} \tag{19}其中 $d$ 是动作空间的维度。这个改变只是工程性的，并没有理论的意义。 3.2.2 置信域优化修改连续情况下的策略梯度更新公式与离散情况下的策略梯度更新公式相似，除了将公式 ($\ref{12}$) 中的 $Q^{ret}$ 替换为 $Q^{opc}$ 。$Q^{opc}$ 的计算公式与 $Q^{ret}$ 非常相似，除了截断重要性因子被替换为 1 [5]。连续情况下的策略梯度公式如下所示： \begin{aligned} \hat{g}_t^{acer} &= \mathbb{E}_{x_t} \left[ \mathbb{E}_{a_t} \left[ \overline{\rho}_t \nabla_{\phi_\theta(x_t)} \log f(a_t|\phi_\theta(x))(Q^{ret}(x_t,a_t)-V_{\theta_v}(x_t)) \right] \\ + \mathbb{E}_{a \sim \pi} \left( \left[ \frac{\rho_t(a)-c}{\rho_t(a)} \right]_+ \nabla_{\phi_\theta(x_t)} \log f(a|\phi_\theta(x)) [Q_{\theta_v}(x_t,a)-V_{\theta_v}(x_t)]\right) \right] \end{aligned} \tag{20} \label{20} Q^{opc}(x_t, a_t) = r_t + \gamma [Q^{opc}(x_{t+1},a_{t+1})-Q(x_{t+1},a_{t+1})] + \gamma V(x_{t+1}) \tag{21}公式 ($\ref{20}$) 的期望形式可以用蒙特卡罗方法代替，最终写成： \begin{aligned} \hat{g}_t^{acer} &= \overline{\rho}_t \nabla_{\phi_\theta(x_t)} \log f(a_t|\phi_\theta(x))(Q^{ret}(x_t,a_t)-V_{\theta_v}(x_t)) \\ &+ \left[ \frac{\rho_t(a_t')-c}{\rho_t(a_t')} \right]_+ \nabla_{\phi_\theta(x_t)} \log f(a_t'|\phi_\theta(x)) [Q_{\theta_v}(x_t,a_t')-V_{\theta_v}(x_t)] \end{aligned} \tag{22} \label{22}$Q^{opc}$ 出自另一篇研究异策略纠正的论文 [5]，可以发现它的形式与 Retrace 技术中的 $Q^{ret}$ 非常像，除了没有截断重要性因子。论文 [5] 中分析，由于没有截断重要性因子，训练过程 $Q^{opc}$ 比 $Q^{ret}$ 不稳定，并且不适用于策略评价（policy evaluation）过程。但是恰恰由于没有截断重要性因子，$Q^{opc}$ 会更好地保留回报（return）的信息，从而在策略梯度中更好地估计 $Q^\pi$，加快训练过程。用一句话概括就是，$Q^{opc}$ 不适用于策略评价（policy evaluation）过程，但适用于策略提升（policy improvement）过程。 3.2.3 伪代码 伪代码中有几个细节需要注意： 首先这是一个 off-policy 的方法，不像离散版本中既结合了 on-plicy，也结合了 off-policy。 其次，$a_i$ 和 $a_i’$ 代表的意义不同，准确来说，代表的采样时刻不同。$a_i$ 是存储在回放池中的，也就是在仿真时刻所执行的动作。而 $a_i’$ 是在策略更新的时刻，根据当前的目标策略 $f(\cdot|\phi_{\theta’}(x_t))$ 所采样的。 最后就是注意 $\rho_i$，$\rho_i’$ 和 $c_i$ 三种重要性因子的使用。 $\rho_i’$ 是用于策略梯度的偏差修正。 4. 工程设置4.1 ATARI 环境网络结构：第一层为 32 个 $8 \times 8$ 步长为 4 的滤波器，第二层为 64 个 $4 \times 4$ 步长为 2 的滤波器，第三层为 64 个 $3 \times 3$ 步长为 1 的滤波器，最后一层为 512 个单元的全连接层。每个隐藏层都采用非线性激活函数。网络输出为 softmax 策略和 Q 值。 超参数：熵正则化项权重为 0.001，奖励折扣因子为 $\gamma=0.99$，每隔 20 个步长更新一次，即 $k=20$。截断重要性权重超参数 $c=10$，置信域优化约束上限 $\delta=1$，软更新系数 $\alpha=0.99$。 4.2 Mujoco 环境网络结构：和 ATARI 环境相同的网络结构，网络输出为策略和状态值。另外还维护一个小型的网络，输出为随机的优势值 $A$，更新也是利用目标 $Q^{target}$ 和 $V^{target}$ 的更新公式。 超参数：SDNs 中的超参数（公式 ($\ref{16} $) ）中的 $n$ 取值为 5。奖励折扣因子为 $\gamma=0.995$，每隔 50 个步长更新一次，即 $k=50$。截断重要性权重超参数 $c=5$，软更新系数 $\alpha=0.99$。策略分布采用高斯分布，固定标准差为 0.3。学习率从范围 $[10^{-4},10^{-3.3}]$ 中 log-uniformly 采样。置信域优化约束上限 $\delta$ 从范围 $[0.1,2]$ 中均匀采样。总共采样出 30 组超参数设置。 参考文献[1] Wang, Z., Mnih, V., Bapst, V., Munos, R., Heess, N., Kavukcuoglu, K., &amp; De Freitas, N. (2019). Sample efficient actor-critic with experience replay. 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, (2016). [2] Schulman, J., Moritz, P., Levine, S., Jordan, M., &amp; Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. 1–14. Retrieved from http://arxiv.org/abs/1506.02438 [3] Degris, T., White, M., &amp; Sutton, R. S. (2012). Off-policy actor-critic. Proceedings of the 29th International Conference on Machine Learning, ICML 2012, 1, 457–464. [4] Munos, R., Stepleton, T., Harutyunyan, A., &amp; Bellemare, M. G. (2016). Safe and efficient off-policy reinforcement learning. Advances in Neural Information Processing Systems, (Nips), 1054–1062. [5] Anna Harutyunyan, Marc G Bellemare, Tom Stepleton, and Remi Munos. Q (λ) with off-policy corrections. arXiv preprint arXiv:1602.04951, 2016.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC</tag>
        <tag>ACER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A3C论文笔记]]></title>
    <url>%2F2020%2F02%2F11%2FA3C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. A3C 概念A3C 大部分时候泛指的是一个轻型的深度强化学习框架，用于深度神经网络控制器的异步优化。完整名称是：Asynchronous Advantage Actor-Critic（A3C），就是将异步训练框架和 Advantage Actor-Critic 方法结合的产物。 过去很多观点认为简单的在线强化学习算法与神经网络的结合通常不稳定，因为在线强化学习连续观察的数据都是相互关联的，因此很多学者利用经验回放池的方法来将连续观察的数据关联性打破（例如 DQN 和 TRPO 算法都利用了经验回放池）。但是这种方法的限制在于只能运用到离线的强化学习方法中。经验回放池有以下缺点： 使用了更多的内存和计算力。 需要离线强化学习算法，并且能够利用旧策略产生的数据完成更新。 为了解决经验回放池的限制，本文提出异步训练框架的概念，在多个并行的环境中训练多个智能体。该框架可以提升大部分强化学习的应用范围，无论是在线强化学习方法还是离线强化学习方法都可以适用这个框架。这种框架可以大大减少所需要的计算资源，不再依赖于 GPU 集群，实现在单台多核的计算机上更快训练。 2. 相关知识2.1 value-based model-free基于价值的无模型强化学习方法最典型的就是 Q-learning 方法，旨在于逼近最优动作值函数：$Q^*(s,a) \approx Q(s, a, \theta)$。目标更新公式可以表示为： L_i(\theta_i) = \mathbb{E} \left( r + \gamma \mathop{max}_{a'} Q(s', a';\theta_{i-1})-Q(s,a;\theta_i) \right)^2其中 $s’$ 表示 $s$ 的下一状态。这种更新方法称为“one-step”方法，因为只利用了一步回报 $r + \gamma max_{a’}Q(s’,a’;\theta_i)$。这种更新方法可能比较缓慢，因为只有一个状态动作对 $(s,a)$ 是通过奖励 $r$ 直接影响。可以采用 “n-step”方法，将 $r + \gamma max_{a’}Q(s’,a’;\theta_i)$ 替换为 $r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1}r_{t+n-1}+max_a \gamma^n Q(s_{t+n}, a)$。 2.2 policy-based model-free基于策略的无模型强化学习方法最典型的就是 REINFORCE 系列方法。标准的 REINFORCE 方法通过更新 $\nabla_{\theta}log\pi(a_t|s_t;\theta)$ 来实现 $\nabla_{\theta} \mathbb{E}[R_t]$ 的无偏估计。为了降低估计的方差，同时保持无偏性，通常减去状态函数 $b_t(s_t)$，因此更新方式变成：$\nabla_{\theta}log\pi(a_t|s_t;\theta)(R_t-b_t(s_t))$。 当利用一个估计的状态函数进行逼近 $b_t(s_t) \approx V^\pi(s_t)$，此时 $R_t - b_t$ 可以视为动作 $a_t$ 在状态 $s_t$ 中的优势函数：$A(a_t,s_t) = Q(a_t,s_t)-V(s_t)$。这种方法可以被视为是 actor-critic 架构，本文也称为 advantage actor-critic。 3. 算法框架使用异步训练框架的目的是让深度神经网络策略的训练变得稳定，但是不需要借助于经验回放池。 首先将每个异步的学习环境放在不同的进程中。 同时对于不同的学习者，可以采用不同的探索策略来扩大差异。 最后采用全局的变化来对参数进行更新。 接下来展示将异步训练框架和 one-step Q-learning 结合起来的算法： 注意每个线程都有自己的环境副本，并在每一步都计算损失的梯度。同时在线网络参数 $\theta$ 和目标网络参数 $\theta^-$ 是所有线程共享的。另外并不是每一步都利用梯度去更新参数，而是先计算累计梯度，只有当过了 $I_{AsyncUpdate}$ 个时间步数才利用 $d\theta$ 更新 $\theta$ 。这样可以避免一个线程的更新迅速覆盖另一个线程的更新，提供计算效率和数据效率间的一个平衡。 接下来展示将异步训练框架和 n-step Q-learning 结合起来的算法： 这种 n-step 方法是往前计算 n 个回报，与常见的后向观点相反（例如 eligibility traces 技术，翻译为“资格迹”）。为了进行一次更新，需要利用探索策略往前进行 $t_{max}$ 步，然后对于这段探索过程中遇见的每个“状态动作对”，都计算梯度。很明显，倒数第一步所用的是 “one-step”更新，倒数第二步所用的是“two-step”更新，而最后一步则是 “n-step”更新。将这些梯度全部累计起来，在更新参数的时候一次性更新。 注意到 n-step 方法没有参数延迟更新时间 $I_{AsyncUpdate}$ ，这可能是因为 n-step 本身就提高了数据利用效率，不必再追求计算效率和数据效率之间的平衡。 最后展示的是将异步训练框架和 advantage actor-critic 结合起来的方法（也就是 A3C 方法）： A3C 方法维护两个网络：$\pi(a_t|s_t;\theta)$ 和 $V(s_t;\theta_v)$。与 n-step Q-learning 类似，也是往前进行 $t_{max}$ 步，然后再对参数进行更新。更新方式可以表示为：$\nabla_{\theta’} log \pi(a_t|s_t;\theta’)A(s_t,a_t;\theta,\theta_v)$，其中 $A(s_t,a_t;\theta,\theta_v)$ 就是对优势函数的估计值：$\sum_{i=0}^{k-1}\gamma^i r_{t+i} + \gamma^k V(s_{t+k};\theta_v) - V(s_t;\theta_v)$，其中 $k$ 值随不同的状态动作对而不同，上限值就是 $t_{max}$ 。 虽然 $\pi$ 和 $V$ 网络所用的参数表示不同，但实际运用时，通常会共享部分参数。例如 $pi(a_t|s_t;\theta)$ 输出层使用 softmax 层，$V(s_t;\theta_v)$ 输出层使用全连接层，但是其他非输出层的参数则共享。 另外在目标策略的函数中加入熵正则化项，可以避免过早收敛到非最优策略，尤其对于需要分层的行为。可以写成：$\nabla_{\theta’} log \pi(a_t|s_t;\theta’)(R_t-V(s_t;\theta_v))+\beta \nabla_{\theta’} H(\pi(s_t;\theta’))$，其中 $H$ 表示策略的熵，超参数 $\beta$ 控制熵正则项的强度。 4. 工程上的设置4.1 Atari Game通过搜索方法在六个 Atari 游戏中调整超参数，然后在全部游戏的训练中固定同一套超参数。网络结构和 DQN 的相似，不过在最后一层隐藏层再连接 256 个 LSTM 单元。 4.2 MuJoco Physics Simulator输入可以分成两种：一种是机器人自身状态，一种是 RGB 像素。 机器人自身状态包括：关节位置，速度，目标位置。 没有在策略更新或状态更新中使用 bootstrpping 技术，而是将一个 episode 作为一次更新。 [1] Mnih, V., Badia, A. P., Mirza, L., Graves, A., Harley, T., Lillicrap, T. P., … Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. 33rd International Conference on Machine Learning, ICML 2016, 4, 2850–2869.]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>AC 框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TRPO & PPO 论文笔记（下）]]></title>
    <url>%2F2019%2F11%2F15%2FTRPO-PPO-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[二、 PPO：近端策略优化1. PPO 概述标准的策略梯度方法是在每个数据样本都进行一次梯度更新，PPO 方法可以进行 mini-batch 更新。比起 TRPO，PPO 继承了它部分优点，但是更容易实现，更通用，和更简单的采样方法。TRPO 方法使用了二阶近似，PPO 旨在于用一阶近似来达到 TRPO 类似的效果，同时提高数据的利用效率。通过交替利用策略采样和对采样的数据进行多个 epoch的优化，来提高数据的利用效率。 在连续的任务上，“概率比截断”版本的 PPO 方法表现得最好。在离散的动作空间任务上，PPO 方法和 ACER 方法效果类似，但更容易实现。 2. 相关知识2.1 策略梯度策略梯度算法中策略梯度的公式推导结果为： \hat{g}=\hat{\mathbb{E}}_t [\nabla_\theta \log \pi_\theta(a_t|s_t)\hat{A}_t] \tag{2.1}其中 $\pi_\theta$ 表示随机策略，$\hat{A}_t$ 是时刻 $t$ 对优势函数的估计值。$\hat{\mathbb{E}}_t$ 表示一个 batch 的样本进行经验估计。 以上的策略梯度对应的目标函数是： L^{PG}(\theta)=\hat{\mathbb{E}}_t [\log \pi_\theta(a_t|s_t)\hat{A}_t] \tag{2.2}虽然可以对以上的目标函数进行多次 epoch 优化，但是这样做实际上是经验主义上的，并且找不到充分的理由，而且还会对较大规模的梯度更新造成破坏。 2.2 TRPOTRPO 中的代理目标函数为： \mathop{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right] \tag{2.3} subject \,\, to \,\, \hat{\mathbb{E}}_t \left[ KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{2.4} \le \delta然后对目标函数进行进行一阶近似，约束进行二阶近似，然后结合共轭梯度进行求解。 实际上为 TRPO 提供保证的理论是采用惩罚项，而非约束项（这项理论应用在 Natural PG 中，TRPO 是对这项理论改造后的结果），也就是优化无约束问题，代理目标函数为： \mathop{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{2.5} \label{2.5}其中 $\beta$ 可以看作惩罚因子，是一个超参数。实际上这种代理目标函数背后的理论是优化策略 $\pi$ 的下界，让每次策略更新后的新策略都能得到提升。但是 $\beta$ 的取值非常难以确定，即使在很简单的问题上，都没办法简单地找到 $\beta$ 值，甚至在同一个问题中，$\beta$ 的取值随着学习过程而改变。这就导致 TRPO 采用了硬约束的方式，而不是惩罚项。 所以现在的问题就是，如果我们想在学习过程中找到性能单调递增的策略，光靠简单地选择一个惩罚项系数和 SGD 是不够的，但是使用 TRPO 又太复杂，需要涉及二阶近似，计算量太大。这个时候 PPO 就出现了。 3. 截断代理目标函数令 $r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$，很明显，$r(\theta_{old})=1$，TRPO 的代理目标可以重写成： L^{CPI}(\theta)=\hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right]= \hat{\mathbb{E}}_t \left[ r_t(\theta)\hat{A}_t \right] \tag{3.1} \label{3.1}上标 $CPI$ 表示 conservative policy iteration，是一篇著名文章中提出来的策略迭代方法 [1]，以上的目标函数就是这篇文章所提出来的。注意，此时目标函数没有加上约束，可能会导致很大的梯度更新，导致策略梯度失去意义。 现在考虑当 $r_t(\theta)$ 偏移 $1$ 的时候进行惩罚，引入截断代理目标函数。将等式 $\eqref{3.1}$ 改写为： L^{CLIP}(\theta)=\hat{\mathbb{E}}_t \left[ min(r_t(\theta)\hat{A_t}, clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t) \right] \tag{3.2}其中 $\epsilon$ 是一个超参数。可以发现 $min$ 函数中的第一项就是 $L^{CPI}$，第二项对 $r_t(\theta)$ 进行裁剪，超出 $[1-\epsilon,1+\epsilon]$ 的部分直接抹去，最后对两项取最小值，确保 $L^{CLIP}$ 取得结果是 $L^{CPI}$ 的下界。 注意，不是什么时候都将 $r_t$ 的值限制在 $[1-\epsilon,1+\epsilon]$，只有当目标函数获得提升时，我们将会对 r 进行限制；当目标函数变差时，我们不会对 r 进行处理。 至于 $r_t$ 什么时候被限制在 $1-\epsilon$，什么时候被限制在 $1 + \epsilon$，这取决于 $\hat{A}_t$的符号。 如果 $\hat{A}_t$ 大于零，则 $r_t$ 将限制在 $&lt; 1 + \epsilon$ 范围内，注意此时没有将 $r_t$ 限制在 $&gt; 1-\epsilon$ 的范围。 如果 $\hat{A}_t$ 小于零，则 $r_t$ 将限制在 $&gt; 1 - \epsilon$ 范围内，注意此时没有将 $r_t$ 限制在 $&lt; 1+\epsilon$ 的范围。 具体如图 3-1 所示。 图 3-1 对 r 裁剪的两种情况 图 3-2 的结果也说明了 $L^{CLIP}$ 取得结果是 $L^{CPI}$ 的下界。图中的横轴表示沿着策略梯度方向插值的参数 $\theta$，横坐标为 $1$ 时是初始参数，往左是沿着梯度下降，往右是沿着梯度上升。可以发现沿着梯度上升时， $L^{CLIP}$ 永远在 $L^{CPI}$ 的下方。 图 3-2 几种代理目标函数的取值 4. 自适应 KL 惩罚系数除了第 3 节所说的截断代理目标函数的方法，本文还提出利用一个对 KL 的自适应惩罚项系数来构建代理目标，将新旧策略的 KL 散度值限定在一个目标 KL 散度值 $d_{targ}$ 附近。文中说这种方法的效果不如截断代理目标函数的方法好，不过可以作为补充和 baseline。 实现过程如下： 首先利用 SGD 对带有惩罚项的代理目标函数（等式 $\eqref{2.5}$ ）进行几个 epochs 的优化： L^{KLPEN} = \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t - \beta KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right] \tag{4.1} \label{4.1} 计算当前新旧策略的 KL 散度值： $d = \hat{\mathbb{E}}_t KL \left[\pi_{\theta_{old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)] \right]$ 如果 $d &lt; d_{targ}/1.5$，$\beta \leftarrow \beta /2$ 如果 $ d &gt; d_{targ}*1.5 $ ，$ \beta \leftarrow \beta*2 $ 更新后的 $\beta$ 值将用于下一次更新。其中 $d_{targ}$ 是一个超参数，$1.5$ 和 $2$ 都是一个启发值，可以自己设定。文中说算法对这两个启发值不是很敏感。初始的 $\beta$ 也是一个超参数，但是不敏感，会随着算法持续自适应更新。 5. PPO 算法5.1 代理目标函数改进算法中涉及对优势值 $\hat{A}_t$ 的估计，为了降低优势估计值的方差，可以使用“广义优势估计”（GAE）方法[2]： \hat{A}_t^{GAE} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l} \tag{5.1}当策略往前看 $T$ 步时（$T$ 小于 episode 的总数），上面公式变为： \hat{A}^{GAE}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \cdots +(\gamma \lambda)^{T-t+1} \delta_{T-1} \tag{5.2} \label{5.2} where \,\,\,\, \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \tag{5.3}当 $\lambda = 1$ 时，等式 $\eqref{5.2}$ 变为： \hat{A}^{(T)}_t = -V(S_t)+r_t+\gamma_{t+1}+\cdots+\gamma^{T-t+1}r_{T-1}+\gamma^{T-t}V(S_T) \tag{5.4}当 $\lambda = 0$ 时，等式 $\eqref{5.2}$ 变为 \hat{A}^{(1)}_t = -V(S_t)+r_t+\gamma V(S_{t+1}) \tag{5.5}在论文 [2] 中证明 $\hat{A}^{GAE}_t$ 实质上是 $\hat{A}^{(1)}_t,\hat{A}^{(2)}_t,\cdots,\hat{A}^{(k)}_t$ 的指数加权和。 如果策略网络和状态网络共用一套参数，代理目标函数还需要加上状态值函数的误差项。另外，为了鼓励智能体探索，可以给目标函数添加一个熵奖励项。最终，代理目标函数的形式为： L_t^{CLIP+VF+S}(\theta) = \hat{\mathbb{E}}_t[L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t)] \tag{5.6}其中 $c_1$，$c_2$ 是超参数，$S$ 表示熵奖励函数，$L_t^{VF}$ 表示状态值函数的误差项 $(V_\theta(s_t)-V_t^{targ})^2$ 。 但是实际上论文中的实验并没有加上 $L_t^{VF}$ 和 $L_t^{S}$ 这两项，说明它们未必在任何时候都能取得效果。 5.2 PPO 伪代码PPO 方法的伪代码如下： 首先用 $N$ 个智能体并行收集 $T$ 步的数据，构成大小为 $NT$ 的数据集，然后使用 minibatch SGD （或 Adam）方法优化代理目标函数。 6. 工程设置和实验比较网络结构和 TRPO 论文中的类似，两层全连接层，每层 64 个神经元，激活函数使用 tanh 非线性函数，输出为高斯分布策略的均值，标准差用另外一个独立的可变的参数来表示。另外，代理目标函数没有加上状态函数误差项和熵奖励项。 工程设置没有其他特别注意的点，下面是实验比较中采用的一些方法。 6.1 不同代理目标函数的比较 使用 7 个环境，3 个随机种子，结果取最后 100 个 episode 总奖励和。然后归一化，定义随机策略的结果为 0，最好的结果为 1 。最后跑 21 次 取平均。 超参数取多几个，然后进行比较： 6.2 连续动作空间的其他算法比较 6.3 两个有意思的评价标准(1) average reward per episode over entire training period 可以用来评价学习的速度。 (2) average reward per episode over last 100 episodes of training 可以用来评价最终学习的效果。 参考文献[1] S. Kakade and J. Langford. “Approximately optimal approximate reinforcement learn- ing”. In: ICML. Vol. 2. 2002, pp. 267–274. [2] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. “High-dimensional contin- uous control using generalized advantage estimation”. In: arXiv preprint arXiv:1506.02438 (2015).]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>PG</tag>
        <tag>PPO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TRPO & PPO 论文笔记（上）]]></title>
    <url>%2F2019%2F10%2F18%2FTRPO-PPO-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要涉及到策略梯度系列的两个算法，TRPO 和 PPO。TRPO 先提出来，PPO 实质上是对 TRPO 的改进。两篇论文的题目为 Trust Region Policy Optimization 和 Proximal Policy Optimization Algorithms 。 一、 TRPO：置信域策略优化1. TRPO简述本文提出了 Trust Region Policy Optimization (TRPO) 算法，主要是对 natural policy gradient 算法的改进，适用于大型的非线性策略函数，例如神经网络。传统方法中，基于策略梯度的模型有一个缺点就是采样效率太低，需要大量的样本才能让模型学习。 本文证明了可以通过最小化某个特定的目标函数，让策略每次都得到非平凡的提升。通过一系列的近似，将原本的目标函数改成实际的算法，称之为 TRPO 算法。 本文中给出两种具体的实现方式：single-path方法，vine方法。TRPO 算法可以同时优化非线性策略网络中成千上万的参数，这对于传统的策略梯度方法而言几乎是不可能的。 2. 相关知识定义随机策略 $\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$，定义 $\eta(\pi)$ 为期望折扣回报，公式如下： \eta(\pi) = \mathbb{E}_{s_0, a_0, \cdots} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t) \right] \tag{2.1} s_0 \sim \rho_0(s_0), \,\, a_t \sim \pi(a_t | s_t), \,\, s_{t+1} \sim \rho(s_{t+1} | s_t, a_t)动作值函数，价值函数和优势函数的定义： Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, \cdots} \left[ \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}) \right] \tag{2.2} V_\pi(s_{t}) = \mathbb{E}_{a_t, s_{t+1}, \cdots} \left[ \sum_{l=0}^{\infty} \gamma^l r(s_{t+l}) \right] \tag{2.3} A_\pi(s, a) = Q(s, a) - V(s) \tag{2.4} a_t \sim \pi(a_t | s_t), \,\, s_{t+1} \sim \rho(s_{t+1} | s_t, a_t)假设另一个策略 $\tilde{\pi}$，可以通过证明得到策略 $\tilde{\pi}$ 比策略 $\pi$ 的累积优势： \eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0, a_0, \cdots \sim \tilde{\pi}} \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] \tag{2.5} \label{2_5}上面的等式在论文的附录 A 中有证明。这是一个极其重要的等式，后面可以保证策略的优化是单调的，也就是不断朝着优势的方向优化。其中 $\mathbb{E}_{s_0, a_0, \cdots, \tilde{\pi}} [\dots]$ 表示动作服从策略 $\tilde{\pi}$，即 $a_t \sim \tilde{\pi}[\cdot | s_t]$。 令 $\rho_\pi$ 为折扣状态访问概率，写成如下公式： \rho_\pi(s) = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) \cdots , \tag{2.6}\label{2_6}现在重写公式 $\eqref{2_5}$，将原本在时间 $t$ 维度上的叠加写成在状态 $s$ 维度上的叠加，结合公式 $\eqref{2_6}$，可以得到： \begin{aligned} \eta(\tilde{\pi}) &= \eta(\pi) + \sum_{t=0}^\infty \sum_s P(s_t = s | \tilde{\pi}) \sum_a \tilde{\pi}(a|s) \gamma^t A_\pi(s, a) \\ &= \eta(\pi) + \sum_s \sum_{t=0} \gamma^t P(s_t=s | \tilde{\pi}) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \\ &= \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \end{aligned} \tag{2.7} \label{2_7}等式 $\eqref{2_7}$ 意味着，只要保证在每个状态 $s$ 满足 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) \ge 0$，那么策略更新 $\tilde{\pi} \rightarrow \pi$ 就一定能保证提高策略性能 $\eta$ 或者至少保持不变（当 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) = 0$ 时）。 那么策略 $\tilde{\pi}$ 就可以按照贪婪算法来进行迭代更新，即$\tilde{\pi}(s) = argmax_a A_\pi(s, a)$，只要至少一个状态-动作对拥有正的优势值 $A_\pi(s, a)$，并且该状态的访问概率 $P(s)$ 不为零，策略 $\tilde{\pi}$ 的性能 $\eta(\tilde{\pi})$ 就能得到提升。但是由于预测误差和估计误差的存在，对于部分状态 $s$ 而言，可能会存在 $\sum_a \tilde{\pi}(a|s) A_\pi(s, a) &lt; 0$。 等式 $\eqref{2_7}$ 中有关于新策略 $\tilde{\pi}$ 的状态访问概率 $\rho_{\tilde{\pi}}(s)$，这在实际的计算中是非常难以得到的，很难计算得到每个状态在新策略下的访问概率，甚至在旧策略 $\pi(s)$ 下的访问概率都不是通过直接计算（通常是无模型的环境），而是通过很多次采样求平均得到，更何况新策略的具体分布都不知道，也没法进行采样。为了便于计算，本文假设在每一步微小更新后，新策略 $\tilde{\pi}$ 和策略 $\pi$ 的状态访问概率是一样的，于是得到等式 $\eqref{2_8_}$： L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \tag{2.8} \label{2_8_}假设 $\pi_\theta(a|s)$ 是对参数向量 $\theta$ 完全可导的，可以证明 $L(\tilde{\pi})$ 和 $\eta(\tilde{\pi})$ 一阶近似(证明过程参考文献 [1] 的 Theorem 1)。也就是对于任意参数 $\theta_0$，都存在： \begin{aligned} L_{\pi_{\theta_0}}(\pi_{\theta_0}) &= \eta(\pi_{\theta_0}) \\ \triangledown_\theta L_{\pi_{\theta_0}}(\pi_\theta) |_{\theta = \theta_0} &= \triangledown_\theta \eta(\pi_\theta) |_{\theta = \theta_0} \end{aligned} \tag{2.9} \label{2_9}等式 $\eqref{2_9}$ 意味着只要步长足够小，$\pi_{\theta_0} \rightarrow \tilde{\pi}$，就能提升 $L_{\pi_{\theta_{old}}}$的性能，当然也能提升 $\eta$ 的性能。但是这个步长究竟取多小，才是这篇文章研究的主题。 Kakade 和 Langford 等人在 2002 年提出了一种 conservative policy iteration 的方法（参考文献[2]），通过这种方法可以保证每次更新后策略的性能 $\eta$ 都有一个下界。 策略迭代的公式如下所示： \pi_{new}(a|s) = (1 - \alpha) \pi_{old}(a|s) + \alpha \pi'(a|s) \\ where \,\, \pi' = \mathop{argmin}_{\pi'} L_{\pi_{old}}(\pi') \tag{2.10} \label{2_10}Kakade 和 Langford 等人证明了依据这个策略迭代后，$\eta$ 有一个下界，如下所示： \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1 - \gamma(1-\alpha))(1-\gamma)}\alpha^2 \\ where \,\, \epsilon = \mathop{max}_s |E_{a \sim \pi'(a|s)}[A_\pi(s,a)]| \tag{2.11} \label{2_11}其中 $\alpha, \gamma \in [0, 1]$，上面的不等式也可以简单写成： \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{2 \epsilon \gamma}{(1 - \gamma)^2} \alpha^2 \tag{2.12} \label{2_12}因为 $\alpha \ll 1$，所以不等式右边的减数只是变大一点点，也就是 $\eta$ 的下限变弱一点（变小）。 但是等式 $\eqref{2_10}$ 是一个混合的策略更新，也就是说既需要新策略，也需要旧策略来进行迭代，况且 $\pi’$ 的计算量也很大。所以本文旨在于找出适用于一般的随机策略的更新方式。 3. 单调提升的保证不等式 $\eqref{2_11}$ 意味着只要不等式右边的值在策略更新过程中得到提升，那么真实的性能 $\eta$ 也能得到提升。但是之前不等式 $\eqref{2_11}$ 的成立是针对一种混合策略迭代（conservative policy iteration），本文首次证明这个不等式对于一般的随机策略也是成立的，只要将 $\alpha$ 替换为衡量 $\pi$ 和 $\tilde{\pi}$ 的距离。一般的随机策略比混合策略更加适用于实际问题，因此这种延拓是很有必要的。 本文首先将 $\pi$ 和 $\tilde{\pi}$ 间的距离定义为总变差散度（Total variation divergence），定义为 $D_{TV}(p || q) = \frac{1}{2} \sum_i |p_i - q_i|$，这是离散型的概率分布 $p$ 和 $q$，对应的连续型概率分布则将求和改为积分。 根据 $D_{TV}(p || q)$ 的定义，定义 $D_{TV}^{max}$ 如下： D_{TV}^{max}(\pi, \tilde{\pi}) = \mathop{max}_s D_{TV}(\pi(\cdot|s) \parallel \tilde{\pi}(\cdot|s)) \tag{3.1}定理Ⅰ：令 $\alpha=D_{TV}^{max}(\pi, \tilde{\pi})$，可以证明下列不等式 $\eqref{3_2}$ 是成立的，证明见附录 A。 \eta(\pi_{new}) \ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2 \\ where \,\, \epsilon = \mathop{max}_{s, \, a} |A_\pi(s,a)| \tag{3.2} \label{3_2}至于为什么可以令 $\alpha=D_{TV}^{max}(\pi, \tilde{\pi})$，附录 A 倒数第三段有讲，可以参考文献[3]。 根据 TV 散度和 KL 散度的关系：$D_{TV}(p || q)^2 \le D_{KL}(p || q)$，可以得到不等式$\eqref{3_3}$。（TV 散度和 KL 散度其实都是 F 散度的具体形式） \eta(\tilde{\pi}) \ge L_{\pi}(\tilde{\pi}) - CD_{KL}^{max}(\pi, \tilde{\pi}),\\ where \,\, C = \frac{4\epsilon\gamma}{(1-\gamma)^2} \tag{3.3} \label{3_3}根据不等式 $\eqref{3_3}$ 得到寻找 $\tilde{\pi}$ 的算法，如图所示。 上述的算法可以保证产生性能单调提升的策略序列，也就是 $\eta(\pi_0) \le \eta(\pi_1) \le \eta(\pi_2) \le \dots$ 证明如下： let \,\, M_i(\pi) = L_{\pi_i}(\pi) - CD^{max}_{KL}(\pi_i, \pi) \\ \eta(\pi_{i+1}) \ge M_i(\pi_{i+1}) \,\, by \,\, equation \,\, (3.3) \\ \eta(\pi_i) = M_i (\pi_i) \\ \eta(\pi_{i+1}) - \eta(\pi_i) \ge M_i(\pi_{i+1}) - M(\pi_i) \tag{3.4}在上述的算法中，需要假设每次估计的优势值 $A_\pi(\pi)$ 都是正确的。另外，KL 散度是作为一个惩罚项去更新策略 $\tilde{\pi}$，但是惩罚项系数 $C$ 计算复杂度高，且依赖于正确的估计优势值 $A_\pi(\pi)$。另外大的惩罚系数 $C$ 通常会导致学习步长太小，但是又很难选择一个小的惩罚系数。这就引出 TRPO 方法，通过添加 KL 散度约束而不是惩罚项系数 $C$ 来更新策略。 4. 参数化策略的优化（TRPO 登场）上一小节的理论推导没有涉及到策略的参数化，以及假设在每个状态策略的性能都能被准确的估计。现在开始推导更加实际的算法，基于随机初始参数化的策略和有限的样本情况下。 参数化策略 $\pi(a|s)$ 写成 $\pi_\theta(a|s)$，对上一节的符号进行简写： $\eta(\theta) := \eta(\pi_\theta)$，$L_\theta(\tilde{\theta}) := L_{\pi_\theta}(\pi_\tilde{\theta})$，$D_{KL}(\theta \parallel \tilde{\theta}) := D_{KL}(\pi_\theta \parallel \pi_{\tilde{\theta}})$。上一小节证明了 $\eta(\theta) \ge L_{\theta_{old}}(\theta) - CD^{max}_{KL}(\theta_{old}, \theta)$，当 $\theta = \theta_{old}$时等号成立。因此可以通过下面的最大化公式来更新 $\theta$，保证目标 $\eta$ 单调提升： \theta = \mathop{maximize}_\theta [L_{\theta_{old}}(\theta) - CD^{max}_{KL}(\theta_{old}, \theta)] \tag{4.1}但是实际计算中，如果采用惩罚系数 $C$，$\theta$ 更新的步长通常非常小，收敛速度非常慢。因此采用一种可以采用稍大步长的方式，在新旧策略的 KL 散度上添加约束，代替理论上采用的惩罚系数 $C$。这个约束就称为 trust region constraint（置信域），如公式 $\eqref{4_2}$ 所示。 \mathop{maximize}_\theta L_{\theta_{old}}(\theta) \\ subject \,\, to \,\, D^{max}_{KL}(\theta_{old},\theta) \le \delta \tag{4.2} \label{4_2}但是还存在另外一个问题：约束条件 $D^{max}_{KL}(\theta_{old},\theta) \le \delta$ 表示在状态空间的每一点都要满足约束（结合KL散度的定义来看），这对于状态空间较大的普遍情况是不切实际的。因此采用了一个技巧，将 $D^{max}_{KL}$ 用 $\overline{D}^{\rho}_{KL}$ 代替： \overline{D}^{\rho}_{KL}(\theta_1, \theta_2) := \mathbb{E}_{s \sim \rho}[D_{KL}(\pi_1(\cdot| s) \parallel \pi_2(\cdot | s))] \tag{4.3}$D^{max}_{KL}$ 和 $\overline{D}^{\rho}_{KL}$ 的区别就在于前者需要采集到状态空间的每个点，而后者只需要利用已经采集到的状态空间中的样本。因此公式 $\eqref{4_2}$ 可以写成以下形式： \mathop{maximize}_\theta L_{\theta_{old}}(\theta) \\ subject \,\, to \,\, \overline{D}^{\rho_{\theta_{old}}}_{KL}(\theta_{old},\theta) \le \delta \tag{4.4} \label{4_4}5. 算法模型展开公式 $\eqref{4_4}$ 中的 $L_{\theta_{old}}$，可以得到: \mathop{maximize}_\theta \sum_s \rho_{\theta_{old}}(s) \sum_a \pi_\theta(a|s) A_{\theta_{old}}(s, a) \\ subject \,\, to \,\, \overline{D}^{\rho_{\theta_{old}}}_{KL}(\theta_{old}, \theta) \le \delta \tag{5.1} \label{5_1}接下来采用三个小技巧将公式 $\eqref{5_1}$ 改写成可以利用蒙特卡洛方法进行逼近的形式： 将 $\sum_s \rho_{\theta_{old}}(s)[\cdots]$ 替换为 $\frac{1}{1-\lambda}\mathbb{E}_{s \sim \rho_{\theta_{old}}}[\cdots]$。参考公式 $\eqref{2_6}$，对 $\lambda^iP(s_i)$ 求和取平均即可得到。 将优势值 $A_{\theta_{old}}$ 替换为 $Q$ 值 $Q_{\theta_{old}}$。 利用重要性采样方法替换在动作上的累加，采样分布设为 $q$，可以得到： \sum_a \pi_\theta(a|s_n)A_{\theta_{old}}(s_n, a) = \mathbb{E}_{a \sim q} \left[\frac{\pi_\theta(a|s_n)}{q(a|s_n)} A_{\theta_{old}}(s_n, a) \right] 最终公式 $\eqref{5_1}$ 改写成以下形式： \mathop{maximize}_\theta \,\, \mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim q} \left[\frac{\pi_\theta(a|s)}{q(a|s)} A_{\theta_{old}}(s, a)\right] \\ subject \,\, to \,\, \mathbb{E}_{s \sim \rho_{\theta_{old}}} [D_{KL}(\pi_{\theta_{old}}(\cdot | s) \parallel \pi_\theta(\cdot | s))] \le \delta \tag{5.2} \label{5_2}直观上来看，在梯度上升法中，我们判断最陡峭的方向，之后朝着那个方向前进。但是如果学习率太高的话，这样的行动也许让我们远离真正的目标函数，甚至会变成灾难。 而在信赖域中，我们用 $\delta$ 变量限制我们的搜索区域。前面已经用数学证明过，这样的区域可以保证在它到达局部或者全局最优策略之前，它的优化策略会优于当前策略。 当我们不断迭代下取，就可以到达最优点。 剩下要做的就是将公式 $\eqref{5_2}$ 中的期望利用采样的平均来代替，以及用经验估计值代替 $Q$ 值。采样的模式有两种，一种称为 single path，另一种称为 vine。 5.1 Single Path这种模式下的采样，首先采集一系列初始状态 $s_0 \sim \rho_0$，然后从每个初始状态开始，按照策略 $\pi_{\theta_{old}}$（也就是令 $q = \pi_{\theta_{old}}$） 进行一段时间仿真，生成轨迹 $s_0, a_0, s_1, a_1, \cdots , s_{T-1}, a_{T-1}, s_T$。最后 $Q_{\theta_{old}}(s, a)$ 的计算是利用轨迹中每一对 $(s_t, a_t)$ 计算未来折扣回报，其中下标 $t= 0, 1, 2, \cdots, T-1, T$。 5.2 Vine首先也是采集一系列初始状态 $s_0 \sim \rho_0$，然后从每个初始状态开始，按照策略 $\pi_{\theta_i}$ 进行一段时间仿真，生成轨迹。紧接着，我们沿着这些轨迹，选择 N 个状态组成一个子集，称为 rollout set。对于每个 rollout set 中的状态 $s_n$，我们根据 $a_{n, k} \sim q(\cdot | s_n)$ 采样 $K$ 个动作。$q(\cdot | s_n)$ 可以取值为 $q(\cdot | s_n) = \pi_i(\cdot | s_n)$，也可以是均匀分布，论文中建议对于连续问题采用第一种，对于离散问题采用第二种。 对于每个 $s_n$ 和动作 $a_{n, k}$，通过以 $s_n$ 和动作 $a_{n, k}$ 为起点进行一段仿真，得到一小段轨迹，然后估计 $\hat{Q}_{\theta_i}(s_n, a_n, k)$ 的值。 因为 vine 模式在每个状态上多做了一些 rollout，可以得到方差更小的 Q 值估计。但是 vine 模式也有对应的缺点，需要调用更多次仿真器，并且对于真实的物理系统，还需要系统支持回退到任意一个过去的状态。两种模式的采样示意图如下所示： 5.3 算法流程 使用 single path 或者 vine 模式进行采样，采集一系列的状态——动作对，并且利用蒙特卡洛方法估计它们的Q值。 对这些样本求平均，建立公式 $\eqref{5_2}$ 中的目标函数和约束。 解决带有约束的优化问题，对参数 $\theta$ 进行更新。使用的是共轭梯度算法（conjugate gradient algorithm）和线性搜索（line search）。 5.4 解决带约束的优化问题最后需要求解公式 $\eqref{5_2}$ ，即求解： maximize \,\, L_{\theta_{old}}(\theta) \,\, subject \,\, to \,\, \overline{D}_{KL}(\theta_{old},\theta) \le \delta \tag{5.3}算法分成两个部分：1) 对目标函数进行一阶逼近，对约束条件进行二阶逼近，然后计算梯度方向。2) 在该方向上进行线性搜索，保证我们可以在非线性约束条件下提升非线性目标函数。 首先对目标函数进行一阶逼近，以及对约束条件进行二阶逼近 [4]，结果如下： 因此目标函数可以重写为： \begin{aligned} \theta_{k+1}=\mathop{argmax}_\theta[g^T(\theta-\theta_k)] \\ s.t. \,\, \frac{1}{2}(\theta-\theta_k)F(\theta-\theta_k) \le \delta \end{aligned} \tag{5.4}通过构造拉格朗日乘法，以及结合 KKT 条件，可以解出： \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TF^{-1}g}}F^{-1}g \tag{5.5} \label{5_5}问题来了，$F^{-1}$ 的计算复杂度很高。所以，论文中利用了共轭梯度直接计算 $F^{-1}g$，而不去求逆。 共轭梯度算法的伪代码如下： 共轭梯度法类似于梯度下降法，但是它可以在最多 $N$ 次迭代之中找到最优点，其中 $N$ 表示模型之中的参数数量。 根据共轭梯度算法，如果 $Q \in \mathbb{R}^{n\times n}$ 是一个正定矩阵，那么最小值 $x^*$ 就等于: Q x^*=b \,\,\,\, \Rightarrow \,\,\,\, x^*=Q^{-1}b$Q$ 是一个正定矩阵，恰好我们的 $F$ 也是一个正定矩阵。因此可以通过共轭梯度算法求出搜索方向 $s \approx A^{-1}g$ 。那么等式 $\eqref{5_5}$ 可以写成： \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{s^TFs}}s \tag{5.6} \label{5_6}其中学习率为： \beta=\sqrt{\frac{2\delta}{s^TFs}} \tag{5.7}此时使用线性搜索方法，如果该学习率下得到的新的策略比原有的策略的 loss 有改进，或者满足 $\overline{D}_{KL}(\theta_{old}, \theta) \le \delta$ ，则直接使用新策略网络替代原有网络，否则指数减少 $\beta$，直到达到理想的训练效果。 所以 TRPO 算法的完整流程为（这个伪代码符号不太对应，后续会修改）： 不过，在 CGA 算法中，我们需要计算 $Qd_k$，也就是 $Fg_k$，论文中给出了该矩阵-向量乘积的计算方法。 首先假设分布 $\pi_\theta(\cdot|x)$ 可以用参数向量 $\mu_\theta(x)$ 来描述（$\mu$ 是分布的均值），即 $\pi(\mu|x)$ ，那么两个策略的 $KL$ 散度可以重新写成： D_{KL}(\pi_{\theta_{old}}(\cdot|x) \parallel \pi_{\theta}(\cdot|x))=kl(\mu_{\theta}(x),\mu_{old}) \tag{5.8} \label{5_8}其中 $kl$ 是两个均值参数向量对应的分布的 $KL$ 散度。对等式 $\eqref{5_8}$ 进行二次求导，可以获得： \frac{\partial \mu_a(x)}{\partial \theta_i} \frac{\partial \mu_b(x)}{\partial \theta_j} k''_{ab}(\mu_\theta(x),\mu_{old})+\frac{\partial^2\mu_a(x)}{\partial\theta_i \partial\theta_j}kl'_a(\mu_\theta(x),\mu_{old}) \tag{5.9} \label{5_9}式子 $\eqref{5_9}$ 的第一项表示对 $kl(\mu_{\theta}(x),\mu_{old})$ 进行二次求偏导，其中有求和的操作。如果 $\mu$ 用神经网络表示，那么第二项明显为 $0$，所以只剩第一项。令 $J := \frac{\partial \mu_a(x)}{\partial \theta_i}$ ，第一项可以写为 $J^TMJ$，其中 $M$ 表示均值参数向量 $\mu$ 的分布的费舍尔信息矩阵。$J$ 的计算通过神经网络的反向传播算法可以轻易求解，而 $M$ 的计算依赖于 $\mu$ 描述的分布的具体形式。注意到此时求解费舍尔信息矩阵 $M$ 需要的不再是一组数据 $x$，而是它们的均值 $\mu$。 现在，计算 $Fg$ 就可以写成 $J^TMJg$。 不过共轭梯度算法中需要迭代 $N$ 次，$N$ 表示模型中参数的数量。每一次迭代都要重新计算费舍尔信息矩阵和一个向量的乘积。论文中运用一个技巧，就是只迭代 $k=10$ 次，而不是迭代 $k=N$ 次，因为他们发现就算 $k$ 继续迭代，效果也提升不了多少，反而大大增加计算复杂度。 另外，文中还利用了另一个减少计算复杂度的技巧：因为费舍尔信息矩阵仅仅是一个度量量度，只要数据量足够，算出的值与真实值偏差不大，因此文中只采用 $10 \%$ 的数据计算费舍尔信息矩阵，大量减少计算复杂度。 6. 工程上的设置6.1 机器人实验机器人控制实验总共设计了三种，分别是蛇形机器人、跳跳机器人和两足机器人。如下图所示。 比较感兴趣的是他们状态和奖励怎么定义，上面三种机器人状态空间都是广义位置和广义速度，各自奖励函数定义如下： 蛇形机器人的状态空间有 10 个维度，奖励函数定义为：$r(x,u)=v_x-10^{-5} |u|^2$ ，其中 $v_x$ 表示在前进方向的速度，$u$ 表示所有所有关节的功耗。 跳跳机器人的状态空间有 12 个维度，奖励函数定义为：$r(x,u)=v_x-10^{-5} |u|^2+nonterminal$，如果回合未结束，即机器人没有倒，则 $nonterminal=1$，否则为 0。相当于一个额外的奖励。 两足机器人的状态空间有 18 个维度，奖励函数与跳跳机器人相同，并且额外增加了足底压力的惩罚，压力越大，惩罚越大。这是为了让两足机器人有更加顺滑的行走，而不是变成跳跳机器人。 实验中设定 $KL$ 散度的阈值 $\delta=0.01$。其余参数的设置如下表所示： 6.2 Atari 游戏实验参数设置如下： 6.3 网络结构对于机器人的控制，状态空间和动作空间都是连续的，我们利用高斯分布对策略进行建模。神经网络采用全连接层，输入是状态空间，输出是高斯分布的期望值，网络参数为 $\{W_i,b_i\}^L_{i=1}$。另外还有一组单独的参数 $r$ 表示标准差的 $log$ 值，与神经网络的输出（均值）共同描述高斯分布。即策略最终可以描述为： \pi_\theta(a|s)=\mathcal{N}\left(mean=NeuralNet(s;\{W_i,b_i\}^L_{i=1}),stdev=exp(r)\right)对于离散的动作空间，神经网络的输入是状态空间，对输出采用 $softmax$ 函数，取概率最大的动作。 7. 总结近年来，学界开始将优化方法中的信赖域（Trust region）方法引入增强学习，并在各种实验场景中取得了良好的效果。其中典型的有 TPRO 和受其影响衍生出的一系列前沿算法，如 PPO，Trust-PCL，ACER等。其中 PPO 已成为 OpenAI 的默认增强学习算法。 信赖域系增强学习方法，顾名思义，来源于优化论中的信赖域方法和机器学习中增强学习的结合。 先介绍一下信赖域方法。在 Jorge Nocedal 和 Stephen J. Wright 的《Numerical Optimization》一书的第 2.2 节介绍了解优化问题的两种策略：Line search 和 Trust region。本质上它们的作用都是在优化迭代过程中从当前点找寻下一点。它们的最大区别是先确定步长还是先确定方向。Line search 方法先确定方向再确定步长。而 Trust region 方法则先把搜索范围缩小到一个小的范围，小到能够用另一个函数（Model function）去近似目标函数（Objective function），然后通过优化这个 Model function 来得到参数更新的方向及步长。在该书的第三和第四章分别着力介绍了 Line search 和Trust region方法。 策略梯度方法是强化学习中非常重要而且历史悠久的一类方法。下面简单介绍一下策略梯度方法的重要历史节点。早在 1992 年 Ronald J. Williams 提出的 REINFORCE 算法就是 PG 的雏形。它的基本思想是考虑由参数 $\theta$ 控制的随机策略 $\pi(\theta)$，然后通过优化与策略相关的目标函数 $J(\theta)$（比如累积折扣回报和）来更新策略的参数。 之后， 行动者-评论家（Actor-Critic, AC）方法提出以解决原始策略梯度方法中方差较大，样本利用率低的问题。1998 年， Amari 提出《Natural Gradient Works Efficiently in Learning》， 通过自然梯度（Natural gradient）代替标准梯度（Standard gradient）来解决梯度算法面临平坦区域过早收敛或收敛速度过慢的问题，直观上就是，它使得参数在相对于参数不太敏感的地方，步长大；而在敏感的地方，步长小。此后，Kakade 在 2001 年的论文《Natural Policy Gradient》将自然梯度引入增强学习中的 PG 方法。策略参数的更新方向就变为自然梯度。Peters 等人在 2005 年的论文《Natural Actor-Critic》中讨论了它与 AC 框架的结合。之后在论文《Reinforcement Learning of Motor Skills with Policy Gradients》中对这些工作有些总结。 8. 参考文献[1] Sutton, R. S., McAllester, D., Singh, S., &amp; Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, 1057–1063. [2] Kakade, S., &amp; Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. 1Proceedings of the 19th International Conference on Machine Learning, 267–274. [3] Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and mixing times. American Mathematical Society, 2009. [4] Yi Da, Xu, Policy Gradient mathematics. 2019]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>PG</tag>
        <tag>TRPO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN代码实现]]></title>
    <url>%2F2019%2F10%2F16%2FDQN%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[DQN强化学习方法系列主要是由两篇文章提出，分别是 Playing Atari with Deep Reinforcement Learning 和 Human-level control through deep reinforcement learning。这两篇文章讲述的具体方法在之前的博客 DQN相关论文笔记中有过介绍，在这篇文章中分析DQN强化学习方法的代码实现细节。 1. 算法为代码 构建模型。确定在线 Q 网络（也称为估计 Q 网络）的层数和隐藏层神经元个数，并随机初始化权重。目标 Q 网络的架构与在线 Q 网络一致，权重初始化为在线 Q 网络的权重。 初始化经验回放池 $\mathcal{D}$。 进行训练。外循环为 M 个 episode 的训练。每个 episode 是智能体从行动开始到任务结束（或）任务超时的过程。 内循环为 T 个时间步长： 利用 $\epsilon$ -贪婪算法选择随机动作 $a_t$。 在仿真器中执行动作 $a_t$ 获得奖励 $r_t$ 和新的状态 $s_{t+1}$。 将一个 transition $(s_t, a_t, r_t, s_{t+1})$ 存入经验回放池。 对经验回报池进行采样，随机抽取 N 个 transition 构成一个 Mini-batch。 通过梯度下降最小化均方损失函数 $(y_j - Q(s_j, a_j; \theta))^2$ 更新参数 $\theta$。 每隔 $C$ 个时间步长将在线 Q 网络的权重 $\theta$ 赋值给目标 Q 网络的权重 $\theta^-$ 2. 代码实现（基于pytorch）下面用代码实现简单的 DQN 示例， 环境为 gym 中的小游戏 Cartpole-v0（让小车上的木棍保持竖立）。 2.1 构建网络构建 Q 值网络，采用两层全连接网络。输入是 4 维状态向量，输出是 2 个动作对应的 Q 值。策略采用 $\epsilon-$ 贪婪策略，有 $\epsilon$ 的概率选择随机动作，$1-\epsilon$的概率选择贪婪动作。123456789101112131415161718class Qnet(nn.Module): def __init__(self): super(Qnet, self).__init__() self.fc1 = nn.Linear(4, 256) self.fc2 = nn.Linear(256, 2) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x def sample_action(self, obs, epsilon): out = self.forward(obs) coin = random.random() if coin &lt; epsilon: return random.randint(0, 1) else: return out.argmax().item() 2.2 构建经验回放池经验回放池实际上是一个队列，当经验回放池满时，会抛弃旧的经验值，加入新采样的经验值。采样时，从经验回放池中随机抽取batch_size个经验值作为一个transition返回给训练机进行学习，代码如下：123456789101112131415161718192021222324252627class ReplayBuffer(): def __init__(self): self.buffer = collections.deque(maxlen=buffer_limit) def put(self, transition): self.buffer.append(transition) def sample(self, n): mini_batch = random.sample(self.buffer, n) s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] for transition in mini_batch: s, a, r, s_prime, done_mask = transition s_lst.append(s) a_lst.append([a]) r_lst.append([r]) s_prime_lst.append(s_prime) done_mask_lst.append([done_mask]) return torch.tensor(s_lst, dtype=torch.float), \ torch.tensor(a_lst), \ torch.tensor(r_lst, dtype=torch.float), \ torch.tensor(s_prime_lst, dtype=torch.float), \ torch.tensor(done_mask_lst) def size(self): return len(self.buffer) 2.3 构建训练模型估计 Q 网络的参数采用随机初始化，目标 Q 网络的参数复制估计 Q 网络的参数。$\epsilon$会随着 episode 的增长而降低，也就是后面的策略越来越靠近贪婪策略。外层循环是 $N$ 个 episode，内层循环是 $T$ 个时间步长。每个时间步长进行动作采样，仿真执行，存储经验这三个步骤。 注意伪代码中每个时间步长都更新一次，这里是每个 episode 更新一次，视具体情况而定，如果每个 episode 包含的时间步长很多，则选择每个时间步长更新一次，也可以是每隔几个时间步长更新一次。 另外还需要注意的是前期不会直接进行更新，而是等到经验回放池中的样本数量超过某个阈值才会开始训练。 最后就是每隔 $C$ 个 episode 将估计 Q 网络的参数复制给在线 Q 网络。1234567891011121314151617181920212223242526272829303132333435363738394041424344def train(): env = gym.make('CartPole-v1') q = Qnet() q_target = Qnet() q_target.load_state_dict(q.state_dict()) memory = ReplayBuffer() optimizer = optim.Adam(q.parameters(), lr=learning_rate) print_interval = 20 score = 0.0 for n_epi in range(N_episode): # epsilon = max(min_epsilon, max_epsilon - 0.01 * (n_epi / 200)) epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (n_epi / N_episode)) s = env.reset() for t in range(T): a = q.sample_action(torch.from_numpy(s).float(), epsilon) s_prime, r, done, info = env.step(a) done_mask = 0.0 if done else 1.0 memory.put((s,a,r,s_prime,done_mask)) s = s_prime score += r if done: break if memory.size() &gt; 2000: update(q, q_target, memory, optimizer) if n_epi%copy_time == 0: q_target.load_state_dict(q.state_dict()) if n_epi%print_interval==0 and n_epi!=0: print("# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, epsilon : &#123;:.1f&#125;%".format( n_epi, score/print_interval, memory.size(), epsilon*100)) score = 0.0 env.close() torch.save( &#123; 'q_state_dict': q.state_dict(), 'q_target_state_dict': q_target.state_dict(), 'optim_state_dict': optimizer.state_dict() &#125;, SAVE_PATH) 2.4 梯度下降更新参数采用adam优化器，对估计网络的参数 $\theta^Q$ 和 $\theta^\mu$ 进行mini-batch梯度下降优化。通过最小化均方损失函数,求 $\theta^Q$ 的梯度，利用adam优化器更新参数 $\theta^Q$。均方损失函数如下： L = \frac{1}{N} \sum_i((r + Q(s',a'|\theta_i^-)) - Q(s,a|\theta_i))^2其中 $\theta_i^-$ 是经过 $C$ 个步长从估计 Q 网络中复制而来，在上面的构建训练模型中有体现。 更新过程是从经验回放池中抽取 batch_size 个训练样本，利用梯度下降法进行反向传播更新参数。12345678910111213def update(q, q_target, memory, optimizer): for i in range(update_epoch): s, a, r, s_prime, done_mask = memory.sample(batch_size) q_out = q(s) q_a = q_out.gather(1,a) max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) target = r + gamma * max_q_prime * done_mask loss = F.smooth_l1_loss(q_a, target) optimizer.zero_grad() loss.backward() optimizer.step() 2.6 运行DDPG训练好模型之后，将网络的参数保存，然后在评测的时候重新载入模型参数。采用的策略也算是 $\epsilon-$ 贪婪策略，不过 $\epsilon$ 值采用最小值。代码如下所示：12345678910111213141516def evaluate(): q = Qnet() checkpoint = torch.load(SAVE_PATH) q.load_state_dict(checkpoint['q_state_dict']) q.eval() env = gym.make('CartPole-v1') while(1): s = env.reset() done = False while not done: a = q.sample_action(torch.from_numpy(s).float(), min_epsilon) s_prime, r, done, info = env.step(a) env.render() s = s_prime env.close() 2.7 超参数的设置超参数的设置很重要，但没有特定的方法指导，所以全凭经验。简单总结一些经验： 确定学习步长一般先确定它的数量级，比如学习步长的数量级为 $10^{-4}$，然后再对系数进行微调。在不影响后期收敛速度的情况下，学习步长选择低一点的。 batch_size 的大小对学习的稳定性有影响。如果 batch_size 太小，模型可能一开始取得较优值，后面训练的性能却开始下降，这是因为模型过拟合。所以 batch_size 不能太小。但是也不能太大，否则收敛速度过慢，训练过程太久。 copy_time 太大模型会陷入过拟合，太小则模型收敛速度过慢。 123456789101112learning_rate = 0.0002 #0.0001N_episode = 10000max_epsilon = 0.08min_epsilon = 0.001 #0.01T = 600train_threshold = 2000update_epoch = 10copy_time = 50SAVE_PATH = 'model/dqn.pt'batch_size = 64 #32buffer_limit = 50000gamma = 0.99]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先经验回放]]></title>
    <url>%2F2019%2F09%2F21%2F%E4%BC%98%E5%85%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%2F</url>
    <content type="text"><![CDATA[论文题目：Prioritized Experience Replay 1. 论文简述在 Nature DQN 和 Double DQN 论文中经验回放池的采样是基于均匀分布采样，一种更合理的方式应该考虑这些经验中哪些更具有对训练更有价值，也就是给这些经验值分配不同的优先级权重，在采样时这些重要的经验被抽取的概率对更大。DQN 论文中提及很早之前有研究做过一种 “Prioritized sweeping” 方法，就是实现经验回放的不均匀采样。本篇论文在前人的研究基础上提出一种新的框架——优先经验回放，使优先级更大的经验被选中的几率更大。“DQN + 优先经验回放”的方法在 Atari 游戏的测试中比 “DQN + 均匀经验回放”的方法更好（49个游戏有41个性能更优越）。 1.1 优先经验回放简述在线强化学习利用每次获得的经验流 $(s_t, a_t, r_t, s_{t+1})$ 来更新参数。最简单的更新方式就是每次获得一个经验流，在进行一次更新后就抛弃。这样做有两个问题： 由于状态之间存在关联，因此用来更新的经验值并非是独立的，这破坏了很多算法模型的独立性假设。 有些经验是十分罕见和宝贵的，有可能下一次学习还需要继续用到，因此直接丢弃是不理智的。 采用经验回放的方法可以减少需要用于训练的经验，加快训练，同时牺牲一些计算资源和存储资源来减少智能体和环境的交互。对于强化学习智能体而言，计算资源、存储资源和环境交互的次数相比，是一种 cheaper 资源。 优先经验回放背后的关键思想是有些经验在用于智能体的学习时会比另外一些经验更有意义。另外，有些经验可能在当前对于智能体的学习并不是很有帮助，但是在智能体能力提升之后，可能对智能体的学习会增加。 优先经验回放用 TD 误差的大小来衡量哪些经验对学习过程具有更大的贡献。但是使用优先经验回放会带来一点多样性的损失和引入偏差。本文引入随机优先 (stochastic prioritization) 来缓解多样性损失的问题，并通过重要性采样来纠正偏差。 优先经验回放的可以对每一个 transition（RL 中交互的原子单位，以下称为“经验”）计算优先级指标，也可以对一个经验序列计算优先级指标。 1.2 相关知识1993 年 Moore 和 Atkeson 等人就提出了 “prioritized sweeping” 的概念，根据状态更新后的变化值来对每个状态进行排序，优先选出更新后最优的状态。 但是这种方法只适用于有模型的强化学习问题。本文提出的优先经验回放是用在无模型的强化学习问题上，另外本文还新采用了两个技巧——随机优先方法 (stochastic prioritization) 和 重要性采样。 TD 误差其实提供了一种更新后的变化值的描述。在Q网络中，TD 误差就是目标 Q 网络计算的目标 Q 值和在线 Q 网络计算的 Q 值之间的差距。注意到在经验回放池里面的不同的样本由于 TD 误差的不同，对我们反向传播的作用是不一样的。TD 误差越大，那么对我们反向传播的作用越大。而TD 误差小的样本，由于TD 误差小，对反向梯度的计算影响不大。 2. 算法模型2.1 衡量经验优先级的标准优先经验回放最重要的部分是衡量每个经验优先级的标准。本文提出的第一种方法就是使用 TD 误差 来衡量经验的优先级。 TD 误差是指下一次更新后值的变化，如果 TD 误差大，说明用来更新的经验值具备更多的信息，因此优先级也更高。不过这种方法在一些环境下并不适用，那就是当奖励值是带有噪声的情况下。论文后面也讨论了其他衡量经验优先级的标准，目前先假定这种标准就是 TD 误差。 算法会将上一次计算的 TD 误差连同用于更新的经验共同存入经验回放池。如果是新的经验，并不知道其对应的 TD 误差，算法会给予这种经验最高的优先级，保证所有的经验都至少被回放一遍。 2.2 随机优先方法 (Stochastic prioritization)每次都抽取 TD 误差最大的那个经验的经验回放方法称之为贪婪经验回放方法 (greedy TD-error prioritization)。这种方法有以下三个问题： 一般为了减少遍历经验回放池的巨大时间代价，每次只会对被抽取的经验更新它的 TD 误差值。也就是说如果一个经验一开始它的 TD 误差值很小，那么可能很长时间内都不会回放这个经验。 对噪声脉冲十分敏感（也就是比较随机的奖励值），逼近目标函数的误差也会加重这种算法的不稳定性。 贪婪经验回放会只专注于一小部分初始 TD 误差值比较高的经验，导致缺乏多样性，系统会过拟合。 为了解决贪婪经验回放存在的若干问题，本文引入了随机优先经验回放的方法，是均匀经验回放和贪婪经验回放两种方法的折衷。随机优先经验回放让经验池中每个经验被抽中的概率与它们的 TD 误差值呈现单调关系，但是也保证对于低优先级的经验同样会有非零的概率被抽中。对于经验 $i$，被抽中的概率表示如下： P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}其中 $p_i &gt; 0$ 表示经验 $i$ 的优先级，指数 $\alpha$ 表示依赖优先级进行经验回放的程度，如果 $\alpha=0$，表示均匀经验回放。对于 $p_i$ 的定义，本文给出两种方法（实际应用大部分都使用第一种定义）： $p_i = |\delta_i| + \epsilon$，其中 $\epsilon$ 是一个正的常数，防止经验 $i$ 的 初始TD 误差值为0。 $p_i = \frac{1}{rank(i)}$，其中 $rank(i)$ 是根据 $|\delta_i|$ 的排名。 为了提高采样效率，采样的时间复杂度不能和经验回放池的大小 $N$ 成正比。对于第一种方法，采用基于 “sum-tree” 数据结构实现，采样和更新的时间复杂度是 $O(log N)$。对于第二种方法，采用二进制堆构建的优先队列，采样的时间复杂度为 $O(1)$，更新的时间复杂度为 $O(log N)$。 采样时，采用 $k$ 段概率相等的分段线性函数来近似经验的累积密度函数，采样时先根据概率抽取一段经验序列，再从一段经验中均匀随机抽取一个经验。如果是采用mini-batch的梯度优化方法，可以将 minibatch 的大小设置为 $k$，然后从每段经验序列中都抽取一个经验。 2.3 偏差补偿（偏差纠正）引入随机优先级概念后，仍然会存在问题。注意到，如果是通过正常的经验重放，则使用随机更新规则。因此，对经验进行抽样的方式必须与它们的原始分布相匹配。如果采用的是均匀经验回放，那么采样的方法也相应是随机采样，这样每个经验都会有同等的概率被抽到，因而不会引入偏差。 但是如果采用了优先经验回放，就需要采用优先级采样而抛弃随机采样，这样就会向高优先级的样本引入偏差（即更高概率被抽中）。这种情况下，更新模型权重会有过拟合的风险。与低优先级经验相比，具有高优先级的经验样本可能多次用于训练。因此，模型只会使用一小部分经验更新权重。 为了纠正这种偏差，可以使用重要性采样 (importance-sampling)，通过减少常见样本的权重来调整更新模型。 来纠正这种偏差： \omega_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta注意公式中 $P(i)$ 才是经验 $i$ 被抽取的概率，$\frac{1}{P(i)}$ 是它的倒数。$\beta$ 的作用是控制这些重要性采样权重对学习的影响程度。在实际运用中，$\beta$ 参数在训练过程中会逐步上升到 1。随着 $\beta$ 的提高，上述公式对高优先级的样本的权重几乎不更新，而对低优先级的样本的权重进行较大的提升。因为当后期动作-值 Q 开始收敛时，无偏性的更新对误差收敛是至关重要的。 采用重要性采样还可以限制梯度的大小，这对于深度网络的更新是十分有利的。深度网络的更新步长一般不能设置太大，而采用优先经验回放进行更新时，会明显增加高 TD 误差的经验用于网络更新的几率，这样会使深度网络的更新不稳定。采用重要性采样后，网络更新的梯度会受到限制。为了提高算法模型训练的稳定性，通常让 $\omega$ 除以 $1 / max_i \, \omega_i$ 进行标准化，保证梯度更新可以受到限制。 所以最终重要性采样的权重计算公式如下： \omega_j = \left( N \cdot P(j) \right)^{-\beta} / max_i \, \omega_i2.4 PER 伪代码综合“衡量经验优先级的标准”，“随机优先方法”和“重要性采样补偿偏差”，得到优先经验回放的算法伪代码如下： 注意伪代码中没有写出随机优先回放的技巧，不过具体实现中是要用到的。另外也没有说明当经验回放池满了之后怎么执行替换操作。有两种实现方式，一种是把优先级最低的经验给替换掉，另一种是轮流替换到每个位置。 3. 实现细节3.1 PER 具体实现相关细节我们不能只根据优先级对所有经验回放样本进行排序来实现优先经验回放。这样做对经验样本插入的时间复杂度为 $O(nlogn)$，采样过程的时间复杂度为 $O(n)$，因此这个效率并不高。需要另外引入一些数据结构来减小时间和空间复杂度。 上面 2.2 节提到对于经验优先级 $p(i)$ 的定义有两种方式，一种称为“排名优先级” (Rank-based prioritization)，另外一种称为“比例优先级” (Proportional prioritization)。下面对这两种定义给出具体的实现细节。（其实论文对于这部分的介绍比较笼统，建议直接看代码） 3.1.1 Rank-based prioritization 实现细节采用基于数组的二叉堆实现的优先队列来存储经验。运行时间上的改进来自于避免对采样分布的分区进行过多的重新计算。（这里论文并没有介绍很多） 3.1.2 Proportional prioritization 实现细节在这里使用的是 “sum-tree” 数据结构，它是二叉树，每个节点最多只有两个子节点。 每片树叶存储每个样本的优先级, 每个树枝节点只有两个分叉, 节点的值是两个分叉的和，那么根节点的值就是所有优先级的总和 $p_{total}$。这种数据结构给优先级的累计和的计算带来便利，插入（更新树）和采样的时间复杂度降为 $O(log N)$。 3.2 采样细节假设需要从经验池中抽取 $k$ 个经验（$minibatch = k$），首先将累积优先级范围 $[0, p_{total}]$ 等划分为 $k$ 个序列，然后在每个序列中进行均匀随机采样，最后将对应的经验从数据结构中剥离出来。 3.3 $SumTree$ 实现细节优先经验回放池的数据结构分为三块内容：树的节点索引、节点数据、以及一个单独存放经验的结构。 $SumTree$ 是一种树形结构, 每个叶子存储每个样本的优先级。每个父节点只有两个分支, 父节点的值是两个分支的和, 所以 $SumTree$ 的顶端就是所有优先级的和，如下图所示。 可以发现，叶结点的个数等于之前所有层的节点加起来再加1，设叶结点个数为 $N$，则整棵树的大小为 $2 * N - 1$。 另外还有一个数组（称为 $Data$ 结构）存储所有经验，相当于经验池。$Data$ 结构如下图所示： 注意 $SumTree$ 树和 $Data$ 数组存储的东西不一样，前者存储的是优先级，后者存储的是经验($transition$)。存储优先级的时候是从 $SumTree$ 的叶子节点开始的，其索引是从 $N - 1$ 开始。而这个优先级对应的经验在 $Data$ 数组中的存储是从 $0$ 开始的，可以看出优先值和对应的经验的索引差为 $N - 1$。 从经验池抽样时, 我们会将优先级的总和除以 $batchsize$（设为 $k$）, 分成 $k$ 那么多区间，每个区间的优先级变化范围是 n = \frac{sum(p)}{k}假设如图将所有节点的优先级加起来是 $42$ 的话, 我们需要抽 $6$ 个样本, 这时的区间拥有的优先级是这样： $[0-7]$，$[7-14]$，$[14-21]$，$[21-28]$, $[28-35]$, $[35-42]$ 然后在每个区间里随机选取一个数 $s$，从根节点开始比较，即 $idx=0$，如果左边的子节点比 $s$ 大，则走左边子节点这条，如果左边子节点小于 $s$，则走右子节点，但 $s$ 值要减去左子节点的数值，按照这个规则，一直找到叶结点，返回其索引，以及对应的优先级，还有从 对应的经验。 比如在区间 $[21-28]$ 里选到了 $24$, 就按照这个 $24$ 从最顶上的根节点开始向下比较. 首先看到根节点下面有两个子节点, 左边的子节点 $29$ 比 $24$ 大,所以走左边那条路。接着再对比 $29$ 下面的左边那个点 $13$, 这时 $13$ 比选中的 $24$ 小, 那我们就走右边的路, 并且将手中的值减去 $13$, 变成 $24-13=11$。接着拿着 $11$ 和 $16$ 左下角的 $12$ 比, 结果 $12$ 比 $11$ 大, 那我们就选 $12$ 当做这次选到的优先级，并且可以知道 $12$ 这个节点在树中的索引为 $9$，并且叶子节点的总数 $N = 8$，所以对应的经验在 $Data$ 经验池的索引为 $9 - (N - 1) = 2$。因此从 $Data$ 经验池中顺序取出第三个经验。 从上面 $SumTree$ 的结构图中我们可以注意到，第三个叶子节点优先级最高，它覆盖的采样区间为 $13-25$，也是最长的，因此会比其他节点更容易被采样到。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>经验回放池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN相关论文笔记（下）]]></title>
    <url>%2F2019%2F09%2F16%2FDQN%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要提及下面四篇关于DQN的著名论文的后两篇： [1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. 1–9. Retrieved from http://arxiv.org/abs/1312.5602 [2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 [3] Van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep reinforcement learning with double Q-Learning. 30th AAAI Conference on Artificial Intelligence, AAAI 2016, 2094–2100. [4] Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., &amp; De Frcitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. 33rd International Conference on Machine Learning, ICML 2016, 4(9), 2939–2947. DQN（Deep Q-Learning）算是 DRL 的开山之作，算是采用了 Value function approximation 的 critic-only 类算法，实现了从感知到动作的端对端学习法，由 DeepMind 在 NIPS 2013 上提出[1]，后在 Nature 2015 上提出改进版本[2]。Double-DQN[3] 和 Dualing-DQN[4] 都是 DQN 的改进版本，前者对训练算法进行了改进，后者对模型结构进行了改进。 Q-learning 存在对 Q 值估计偏高的问题，可能会导致非最优解和学习过程稳定性下降。DQN 是基于 Q-learning 的模型，所以本质上也有这个问题。 一、 Double DQN：解决 Q 值估计偏高 [3]1. Double DQN 简述1.1 Q-learning 存在 Q 值估计偏高问题直觉上，Q-learning 算法中估算目标 Q 值时采用的目标策略是最大化的贪婪策略，因此对目标策略的 Q 值估计值往往会偏高，最终导致确定状态下输出的动作的估计 Q 值也会偏高。另外环境噪声等因素也会让动作的估计 Q 值偏高。总而言之，只要学习过程出现动作-值不准确，无论误差来源于什么地方，动作-值都会被高估。这是 Q-learning 算法中普遍遇到的问题，因此有必要解决这一问题，提高算法的稳定性。 那么是否 Q 值估计偏高一定会给算法带来问题呢？不一定。理想状态下，如果所有的动作-值都被统一地估计偏高相同的程度，对于选取动作是没有影响的，因此算法的性能也不会受到影响。但是，这种估计偏高的程度并不是统一地出现在每个动作-值的估计上，有些动作-值估计偏高的程度大，而有些动作-值估计偏高的程度小，如果非最优动作的动作-值被偏高估计超过最优动作的估计动作-值，那就会造成算法性能的下降。 由于 DQN 算法是基于 Q-learning，因此 DQN 本质上也存在这个问题。 1.2 本篇论文的工作本篇论文提出的 Double DQN 是根据 (van Hasselt, 2010) 的 idea 提出的。本篇论文首先证明了 Q-learning 的估计偏高问题的确会影响到算法的性能，并且这是一种普遍现象。进而本文证明 Double Q-learing 能缓解这种问题，并结合 Double Q-learing 和 DQN 提出了一种解决估计偏高，提高 DQN 算法性能的方法，同时在 Atari 各项游戏中取得更优的成绩。 2. 相关知识关于 Q-learning 和 DQN 的背景知识在这里不再赘述，可以阅读《DQN 相关论文笔记（上）》。 Double Q-learning 背后的思想是将动作的选择和动作-值的估计解耦，让它们使用不同的 Q 函数（网络）。注意 Nature DQN 虽然提出了独立目标 Q 网络，但是实际上目标 Q 网络的参数只是相当于在线 Q 网络的参数的延迟更新，本质上它们的参数是同一个参数。 van Hasselt 在2010年提出了 Double Q-learning 的思想。Double Q-learning 采用两组独立的参数 $\theta$ 和 $\theta’$，每个样本随机更新其中一个参数。为了进行更清晰的对比，将传统的在线 Q-learning 算法 的目标 Q 函数改写为： y_t^Q = r_{t+1} + \gamma Q(s_{t+1},\mathop{argmax}_a Q(S_{t+1},a;\theta_t);\theta_t)double Q-learning 的目标 Q 函数： y_t^{DoubleQ} = r_{t+1} + \gamma Q(s_{t+1}, \mathop{argmax}_a Q(s_{t+1}, a; \theta_t);\theta_t')可以看到仍然采用在线 Q 函数的当前的参数 $\theta_t$ 来执行贪婪策略（动作选择），但是采用新的参数 $\theta_t’$ 来重新公平地评估这个策略。第二组参数 $\theta_t’$ 可以通过对称地交换 $\theta$ 和 $\theta’$ 的角色进行更新，也就是说 $\theta$ 和 $\theta’$ 轮流交换负责动作选择和动作评估两个过程。 3. 估计误差引发过度估计证明本文证明了无论什么样的估计误差，都会给动作-值的带来偏高的估计。 论文中给出了定理一，证明一旦存在估计误差，传统的在线 Q-learning 算法的目标 Q 函数的估计一定会偏高，也就是具备非零下限；而 Double Q-learning 可以让下限为零。定理一如下： Theorem 1. Consider a state $s$ in which all the true optimal action values are equal at $Q_*(s,a)=V_*(s)$ for some $V_*(s)$. Let $Q_t$ be arbitrary value estimates that are on the whole unbiased in the sense that $\sum_a(Q_t(s,a)-V_*(s))=0$, but that are not all correct, such that $\frac{1}{m}\sum_a(Q_t(s,a)-V_*(s))^2=C$ for some $C &gt; 0$, where $m \ge 2$ is the number of actons in s. Under these conditions, $max_a Q_t(s,a) \ge V_*(s) + \sqrt{\frac{C}{m-1}}$. This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learing estimate is zero. 定理一表明即使动作-值估计的平均值是正确的，但是一旦有某个动作-值估计错误，带来的估计误差都会让目标 Q 函数的估计值偏高。下限 $\sqrt{\frac{C}{m-1}}$ 需要结合具体值分析。一般来说动作个数 $m$ 越多，估计偏高的程度越大。 本文通过实验证明了传统的在线 Q-learning 出现估计偏高的问题是很普遍的： 并不是只有特定的真实动作-值函数才会导致目标 Q 值函数出现估计偏高，不同的真实动作-值函数都会导致这种问题。 并不是阶数不够的动作-值逼近函数才会出现估计值偏高的问题，阶数高的动作-值逼近函数也会出现估计值偏高的问题，并且可能会更加严重。因为高阶的动作-值逼近函数会匹配所有的样本点，但是也存在过拟合问题，对于潜在的未被采集的样本点，可能估计偏离很大。 估计偏高的问题会随着训练过程不断传播，导致越来越严重，从而让训练过程发散，造成算法性能下降。 4. 算法模型4.1 Double DQN 的改进结合 Double Q-learning 和 DQN 已有的结构，本文提出利用在线 Q 网络执行贪婪策略的动作选择，但是用目标 Q 网络对动作-值进行评估。Double DQN 的目标 Q 函数如下： y_t^{DoubleDQN} = r_{t+1} + \gamma Q(s_{t+1}, \mathop{argmax}_a Q(s_{t+1}, a; \theta_t), \theta_t^-)对比 DQN 的目标 Q 函数： y_t^{DQN} = r_{t+1} + \gamma \mathop{argmax}_a Q(s_{t+1}, a; \theta_{t}^-)注意 Double DQN 的目标 Q 网络的参数更新与 Nature DQN 中的更新方式相同，也就是每隔一段周期，目标 Q 网络的参数直接复制为在线 Q 网络的参数的副本。 4.2 Double DQN 和 DQN 的区别DQN 动作的选择和动作的评估是依托于同一个目标 Q 网络的，也就是说从目标 Q 网络中执行贪婪策略选择动作之后，继续用目标 Q 网络对该动作进行评估。 然而 Double DQN 中动作的选择和动作的评估是解耦的，并不是依托于同一个网络。动作的选择依托的是在线 Q 网络，而动作的评估依托的是目标 Q 网络。也就是说先从在线 Q 网络中执行贪婪策略选择动作之后，再用目标 Q 网络对该动作重新进行评估。 5. 工程设置上的调整 将目标 Q 网络参数的更新周期从 10000 变成 30000，增大了更新周期。 $\epsilon$ -贪婪策略的超参数 $\epsilon$ 在学习时变成从 0.1 到 0.01，在评估时，用的是 $\epsilon = 0.001$。 给 Q 网络的最后一层加上偏置项，所有动作共享这个偏置项。 二、 Dueling DQN: 优势函数 [4]这篇论文提出了针对 model-free RL 的 dueling network框架。它是对传统 DQN 架构层面上的改动，将基于状态的 V 函数（value function）和状态相关的优势函数（advantage function）分离。Advantage 函数的思想基于1993年 Baird 提出的 Advantage updating。除了传统的 V 函数外，引入的 Advantage 函数 A(x, u) 的定义是当采取动作 $u$ 相比于采取当前最优动作能多带来多少累积折扣奖励。简单粗暴得说，就是选这个动作比当前最优动作（或其它动作）好多少。 1. Dueling DQN 简述1.1 Dueling DQN 的改进点Dueling DQN 是一种更适用于 model-free RL 的神经网络架构，如下图所示： 上图的第一行代表之前的 DQN 网络架构，第二行代表 Dueling DQN 网络架构。可以看出 Dueling 网络框架末端有两条分流（绿线前的红色柱子），分别代表状态函数和优势函数。不过 Dueling 网络框架只采用一套卷积层，也就是说状态函数和优势函数共享一套卷积网络参数。分流出来的状态函数和优势函数最后通过一层特殊的聚集层（图中的绿线）汇合成动作-值函数 Q。 直觉上，Dueling 网络架构可以学习到哪些状态是有价值的，而不需要学习每个状态中每个动作的价值。可以预见到，引入优势函数后，对于新加入的相似动作可以很快学习，因为它们可以基于现有的状态函数来学习。另外这种 Dueling 网络架构可以运用在很多 model-free 的 RL 方法上，不仅局限于 DQN。 论文中举了一个赛车游戏的例子：状态函数专注于远处（地平线）和分数，也就是长期目标，优势函数专注于附近障碍，也就是短期目标。这样状态函数和动作函数就能学习到不同时间层次的策略。这种学习的做法有几个好处： 一是状态函数可以得到更多的学习机会，因为以往一次只更新一个动作对应的 Q 函数。 二是状态函数的泛化性更好，当动作越多时优势越明显。直观上看，当有新动作加入时，它并不需要从零开始学习。 三是因为 Q 函数在动作和状态维度上的绝对数值往往差很多，这会引起噪声和贪婪策略的突变，而用该方法可以改善这个问题。 因此，通过解耦计算 V(s)，找出对于那些任何行为都不会被影响的状态尤其有用。在这种情况下，不必计算每个动作的值。例如，向右或向左移动仅在存在碰撞风险时才去关注。而且，在大多数状态下，无论选择何种行动，对发生的事情没有任何影响。 1.2 Dueling DQN 的来源早在1993年 Baird 就提出将状态函数和优势函数分开的概念。1995年 Harmon 他们发现优势函数学习比 Q-learning 收敛速度更快，然后在1996年 Harmon 和 Baird 就提出只依赖于优势函数的学习方法。 Dueling 网络架构可以用在很多 Model-free 的 RL 方法上，不仅仅是 DQN 这类值函数逼近的方法，还可以运用在策略梯度一类的方法上。2000 年 Sutton 首先将优势函数用在策略梯度的方法上，2015 年 Schulman 等人估计优势函数的值来降低策略梯度算法的方差。 2. 相关知识关于 RL、 Q-learning、DQN 和 Double DQN 的基础知识可以参考前面的笔记，这里不再赘述。这里着重介绍一下优势 A 函数的定义和意义，以及优先经验回放方法（Prioritized Replay）。 2.1 优势函数优势函数的定义如下： A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)需要注意的是，策略 $\pi$ 的所有动作的平均优势为零，即： \mathbb{E}_{a \sim \pi(s)}[A^\pi(s,a)] = 0这个等式可以明显从状态函数 $V^\pi(s)$ 的定义看出： V^\pi(s) = \mathbb{E}_{a \sim \pi(s)}[Q^\pi(s,a)]直觉上，状态 V 函数是评估当前特定状态的价值，它是综合执行所有动作带来的价值后给出的期望（平均）价值。而动作-值 Q 函数评估的是特定的动作在该状态下带来的价值。很明显，动作-值 Q 函数减去状态 V 函数得到的优势 A 函数，表征的是一个动作在当前状态下的重要性。这种重要性忽略当前状态的价值，只强调动作本身具备多大的价值（相对于其他动作而言）。 从公式的定义上来看，动作-值函数的值越大，优势函数的值也越大，似乎没有必要单独对优势函数进行学习。但这是针对所有动作的动作-值函数都能正确估计到当前状态的价值的情况下，也就是说估计的状态函数在不同的动作上是确定的并且是相等的。但是在利用动作-值函数进行学习的算法中，并不能保证针对每个动作输出的动作-值都包含着对当前状态的价值的正确估计。因此通过解耦估计，Dueling DQN 可以直观地了解哪些状态是（或不是）有价值的，在有价值的状态下找到优势函数 A 值最大的动作比单纯找到动作-值函数 Q 值最大的动作更有意义。 2.2 优先经验回放Schaul 在2016年将 Prioritized replay 方法和 DDQN 方法结合起来，并在 Atari 系列游戏取得当年最优的分数。 优先经验回放背后的思想是提高那些具有高预期学习进度的经验元组的回放概率。计算学习进度的预期是通过 TD 误差的绝对值公式。经验池中 TD 误差绝对值越大的样本被抽取出来训练的概率越大，加快了最优策略的学习 Dueling 网络结构可以作为很多创新性的算法的一个补充，也就是说无论算法模型采用均匀经验回放还是优先经验回放，采用这种网络机构都可以提升算法的性能。 3. 算法模型3.1 Dueling 网络的分流和汇合Dueling 网络结构的低层卷积层和 DQN 是相同的，但是在卷积层之后不是单个全连接层，而是两个分流的全连接层，分别估计价值函数和优势函数。然后两个分流的输出在下一层被合并成一个动作-值函数，为每个动作输出 Q 值。 将两条全连接层的分流合并输出一个动作-值估计 Q 值需要考虑以下两个约束条件： 优势函数满足 $\mathbb{E}_{a \sim \pi(s)} [A^\pi(s,a)]=0$ 对于确定的策略，动作 $a^* = argmax_{a’ \in \mathcal{A}} Q(s, a’)$，满足 $Q(s, a^*) = V(s)$，因此 $A(s, a^*) = 0$ 在 Dueling 网络架构中，全连接层状态网络分流可以表示为 $V(s;\theta, \beta)$，这是一个标量。另一条全连接层优势网络分流可以表示为 $A(s,a;\theta, \alpha)$，但这是一个矢量。参数 $\theta$ 表示网络卷积层的参数，$\alpha$ 和 $\beta$ 分别表示两条全连接层分流的参数。 根据优势函数的定义，动作-值函数表示为： Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha)注意到 $V(s;\theta,\beta)$ 是标量，因此计算时需要将其复制 $|\mathcal{A}|$ 次。 3.2 优势函数的限定但是我们需要意识到上述公式，对状态函数和优势函数的估计并不总是正确的。对于一个确定的Q，有无数种 V 和 A 的组合可以得到 Q ，也就是说无法找到确定的 V 和 A。文中称之为可识别性问题（identifiability issue）。再用通俗一点的例子来讲，现在需要学习的是状态 V 网络和优势 A 网络，需要让这两个网络合起来的动作-值 Q 网络达到最优。但是可能状态 V 网络学习得并不好（假设学到了一个偏低的评估值），为了让动作-值 Q 网络符合目标网络的要求，优势 A 网络的学习也会产生一定的偏差（也就是学到一个偏高的估计值）。这样虽然看起来动作-值 Q 接近了目标值，但是状态网络和优势网络都学习得不好，等到实际运用网络的时候，状态网络和优势网络的弊端就会暴露出来，从而导致动作-值 Q 网络的性能低于预期。 因此我们需要对 A 进行一定的限定，强制我们的优势函数在选中的行动上具有0优势。也就是保证该状态下各种动作的优势函数大小排序关系不变的前提下，限制动作的优势值，缩小 Q 值的变化范围，从而加强状态函数 V 的学习。在 Dueling 网络架构下，状态 V 网络的学习本质上是要比优势 A 网络的学习要重要的。 论文中第一种做法是利用所有动作中优势函数的最大值对估计的优势函数进行限定，公式如下： Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + \left(A(s,a;\theta, \alpha) - \mathop{max}_{a' \in \mathcal{A}} A(s,a';\theta,\alpha) \right)个人觉得利用最大操作对优势函数进行限定可能存在限定过于严格的问题，也就是说动作-值函数的值过度依赖于状态函数的值，从而导致算法的探索性能下降（受限于局部最优状态）。 因此论文提出了第二种做法，利用所有动作中优势函数的平均值对估计的优势函数进行限定，公式如下： Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + \left(A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|} \mathop{\sum}_{a' \in \mathcal{A}}(s,a';\theta,\alpha) \right)另外论文还尝试了利用“最大化限定 + softmax Q 值”的方法，不过结果与第二种做法差别不是很大。 加上对优势函数的限定，虽然会破坏原始公式的语义（因为多减了一个限定常数），但是这种限定的方法会让算法更加稳定，因为动作的优势值不必每次都需要学习达到最优值，只要达到平均优势值即可，同时加上对优势函数的限定也会加强状态函数的学习。 其实论文中并没有太多关于为什么对优势函数进行限定的解释，但限定的这个步骤实际上是论文最大的创新点之一。 4. 工程设置上的调整4.1 网络结构网络的卷积层和Nature DQN 是一样的，也是三层卷积层，激活函数用的都是非线性激活函数。不过 Dueling 网络结构在卷积层之后分成状态流和优势流，都是用 512 个神经元的全连接层来表示。状态流输出一个状态值，优势流输出 N 个动作优势值（N 是动作的个数）。 4.2 优化过程的调整学习率比 Nature DQN 稍微降低，变成6.25 10-5。同时采用了 gradient clipping* 技术，在反向传播时将梯度的绝对值钳制在小于或等于10的范围内。 4.3 优先经验回放采用 Schaul 等人在2016年提出的 Prioritized Experience Replay 优先经验回放技术。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN相关论文笔记（上）]]></title>
    <url>%2F2019%2F09%2F12%2FDQN%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇笔记主要提及下面四篇关于DQN的著名论文的前两篇： [1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. 1–9. Retrieved from http://arxiv.org/abs/1312.5602 [2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 [3] Van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep reinforcement learning with double Q-Learning. 30th AAAI Conference on Artificial Intelligence, AAAI 2016, 2094–2100. [4] Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., &amp; De Frcitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. 33rd International Conference on Machine Learning, ICML 2016, 4(9), 2939–2947. DQN（Deep Q-Learning）算是 DRL 的开山之作，算是采用了 Value function approximation 的 critic-only 类算法，实现了从感知到动作的端对端学习法，由 DeepMind 在 NIPS 2013 上提出[1]，后在 Nature 2015 上提出改进版本[2]。Double-DQN[3] 和 Dualing-DQN[4] 都是 DQN 的改进版本，前者对训练算法进行了改进，后者对模型结构进行了改进。 一、 DQN：成功将DL和RL结合 [1]1. DQN 简述2013年这篇论文第一个提出利用深层强化学习模型从高维度传感器信号中学习控制策略。模型由卷积神经网络构成，通过本文提出的方法（Q-learning的变种），实现从原始像素输入到值函数输出。 1.1 RL 结合深度学习的问题在深度学习盛行之前，RL需要依靠手工设计特征，并组合成值函数或策略函数。RL性能好坏很大部分取决于特征的质量。深度学习盛行之后，可以直接中原始的计算机视觉和语音信号中自动提取特征。深度学习的方式是利用一堆神经网络（例如卷积神经网络，多层感知机，循环神经网络，有限玻尔兹曼机等）进行深层联合，并采用监督或非监督模型进行学习。自然而然考虑是否能将深度学习和强化学习进行结合，利用深度学习进行特征提取，而不必手工设计特征。 但是将深度学习与强化学习结合并非易事，起码存在三个关键问题： 采用深度学习的方式通常需要很多带标签的训练数据，但是RL每次与环境交互只能获得一个奖励值，与深度学习的训练集相比，奖励值十分稀疏，并且还会带有噪声以及延迟。 深度学习算法假设数据集样本是独立的，但是在强化学习中，状态序列具有很大的关联度，状态之间并不能认为是服从独立分布的。 另外RL当学习到新的动作策略时，状态分布和动作分布都会发生改变，然而深度学习是假定数据具有固定的分布。 1.2 本篇论文的工作本篇论文采用卷积神经网络来克服深度学习和RL结合的问题，并直接从视觉信号（像素）学习到成功的控制策略。提出Q-learning的改进版本对网络参数进行训练。同时为了解决数据关联度高，训练过程中数据分布不稳定的问题，本文采用了经验回放的机制，，每次随机抽取先前的一小部分transition进行训练，这样可以平滑训练集的分布。 2. 相关知识动作空间是离散的，表示为 $\mathcal{A} = {1, \dots, K}$。 在每个时间步长，agent 会从 $\mathcal{A}$ 中挑选一个动作 $a_t$。 本文认为环境是部分可观察的，因此只用当前的观测值 $x_t$ 来表示当前环境的状态是不够的。因此将 $t$ 时刻的状态 $s_t$ 定义为动作值和观测值的序列，也就是 $s_t = x_1, a_1, x_2, \cdots, a_{t-1}, x_t$。 （这里描述的环境是动态的，部分感知的，因此状态的描述需要依赖于动作值和观察值的序列，但是如果环境是完全感知的，静态的，一般都是将 $x_t$ 等同于 $s_t$。DDPG的论文就是这么做的） 定义 $t$ 时刻的折扣奖励 $R_t = \sum_{t’=t}^T \gamma^{t’-t}r_{t’}$，其中 $T$ 是结束的时刻，$\gamma$ 是折扣因子。 定义最优动作-值函数 $Q^*(s,a) = max_\pi \mathbb{E} [R_t|s = s_t, a = a_t, \pi]$，$\pi$ 是动作分布（动作空间的各个动作被选择的概率）。最优动作-值函数满足贝尔曼方程，也就是可以写成： Q^*(s,a) = \mathbb{E}_{s' \sim \mathcal{E}} \left[r + \gamma max_{a'} \, Q^*(s', a') |s, a \right]上述公式中的 $r$ 和 $s’$ 由 $s$，$a$ 决定。通过上述的贝尔曼方程形式的动作-值函数，可以得到一种求解最优动作-值函数的迭代方法： Q_{i+1}(s, a) = \mathbb{E} [r + \gamma max_{a'} Q_i(s', a') | s, a]其中 $i$ 是迭代次数，当 $i \rightarrow \infty$时，$Q_i \rightarrow Q^*$。 但是实际上这种方法非常有限，对于状态空间庞大的环境，根本不适用。因为这种迭代形式的方法需要为每一个 $\{s, a\}$ 序列都求解一个动作-值，无法对庞大的状态空间进行泛化。 为了应对庞大的状态空间，过去的RL方法通常用线性函数（有时候也用非线性函数）来对动作-值函数进行逼近，表示为 $Q(s,a;\theta) \approx Q^*(s,a)$。而本文就是将线性函数逼近的方法改成神经网络逼近，该网络称之为 Q 网络。 Q 网络的参数更新通过最小化损失函数，其定义为 Q 网络输出的 Q 值和目标 Q 值的均方误差函数： L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[(y_i - Q_i(s,a;\theta_i))^2 \right]y_i = \mathbb{E}_{s' \sim \mathcal{E}} \left[r + \gamma max_{a'} Q(s', a'; \theta_{i-1})|s, a \right]其中 $\rho(s, a)$ 称之为行为分布，也就是 $s$ 和 $a$ 序列的概率分布。 $y_t$ 是目标 Q 值，$\theta_{i-1}$ 在迭代过程中是保持不变的。需要注意的是，和监督学习不同，监督学习的目标通常是固定的，并且与网络的参数无关。然而此处目标的计算也依赖于网络的参数。 对均方误差损失函数计算 $\theta_i$ 的梯度： \nabla {\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cot), s' \sim \mathcal{E}} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]在实际计算中，并不会计算梯度的完整期望，一般为了简便采用SGD方法来最小化上述损失函数。也就是，在每个时间不长，只选择一个样本来计算损失函数的梯度，代替损失函数梯度的期望值，这种方法就是我们熟悉的 Q-learning 方法。 Q-learning 方法是 model-free 和 off-policy 类型的方法。 model-free 是指不需要对环境进行建模，而是通过从环境中收集样本进行学习。另外，Q-learning 的目标策略和行动策略不是同一种策略，这种方式也称为 off-policy。目标策略采用的是贪婪策略，即 $a = max_a Q(s,a;\theta)$，行动策略采用的是 $\epsilon$ -贪婪策略，有 $1-\epsilon$ 的概率选择贪婪策略，有 $\epsilon$ 的概率选择随机策略。 在深度学习兴起之前，人们普遍认为将 model-free 强化学习算法和非线性函数逼近，或者 off-policy 结合的方法都可能导致 Q 网络发散。所以传统的强化学习一般都采用线性函数逼近动作-值函数。近年来，由于深度学习的兴起，越来越多研究采用深度学习方法与强化学习方法结合。有一篇工作和本文提出的 DQN 方法比较像，称之为 neural fitted Q-learning（NFQ）。不过有两点主要区别： NFQ 采用批梯度下降方法最小化损失函数，DQN 采用随机梯度下降方法，相对来说，随机梯度下降方法的计算代价更小。 NFQ 采用深层自动编码器对视觉输入信号进行特征提取，利用非监督方法训练得到低维度的状态表示。然后再将低维度的状态表示应用 NFQ 学习控制策略（相当于将两种网络组合到一起，两种网络是单独训练的）。但是 DQN 直接应用视觉输入信号，不做任何处理，最终会学习到和动作-值显著关联的特征，也就是学习到的特征会根据获取到的动作-值而变化（相当于特征提取网络嵌入到 Q 网络，它们是一起训练的）。虽然文中是说不做任何处理，但是实际上还是对视觉信号做了灰度化以及下采样的预处理的，只不过输入依然是像素点而已。 3. 算法模型3.1 DQN 的关键点DQN 模型的目标是将强化学习和深层神经网络结合起来，只需要输入原始 RGB 图像并通过 SGD 对样本进行训练便可以输出最优 Q 网络。 DQN 模型中最重要的一个技巧是采用了经验回放机制。在每个时间步长，都会将经验 $e_t = (s_t, a_t, r_t, s_{t+1})$ 存储在数据集 $\mathcal{D} = \{e_1, e_2, \dots, e_N\}$ 中（其实终止标志 $done$ 也要存进去，表示当前状态是否还有后继状态），并且会保留许多 episode 的经验，即新的 episode 开启时，经验回放池 $\mathcal{D}$ 不会重置。 DQN 模型中采用 off-policy，与Q-learning是一样的，目标策略采用贪婪策略，行动策略采用 $\epsilon$ -贪婪策略。 因为从经验回放池中抽取的历史经验具有不一样的长度（因为状态实际上是一个动作-观测值序列），因此定义函数 $\phi$ 来将长度不一致的历史经验输出为固定长度的历史经验表示。函数 $\phi$ 的具体定义参考3.3节。 算法伪代码如下所示： 等式（3）就是上面的均方误差损失函数： \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot), s' \sim \mathcal{E}} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]因为样本是从经验回放池 $\mathcal{D}$ 中均匀采样，因此写成下面这种形式会更清晰一些：\nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} \left[ \left(r + \gamma max_{a'} Q(s', a';\theta_{i-1}) - Q(s, a;\theta_i) \right) \nabla_{\theta_i} Q(s, a;\theta_i) \right]$U(\mathcal{D})$ 表示经验回放池 $\mathcal{D}$ 中样本的均匀分布。 3.2 DQN 的优点DQN 与传统的在线 Q-learning相比，有以下一些优点： 每个时间步长的经验都有可能用于未来很多次参数更新过程，因此提高了数据的利用效率。 比起 Q-learning 直接用连续的样本学习，DQN 采用经验回放池的机制打破了数据间的关联性，降低了每次更新的方差。 通过使用经验重放，对行为分布的许多先前状态进行平均，平滑了训练样本数据分布，避免了参数的振荡或发散。另外 DQN 必须采用 off-policy，因为当前参数和生成样本时的参数已经不同，而使用旧参数生成的动作来采样更新当前参数，很容易陷入局部最优点或者震荡。 4. 工程上的设置4.1 预处理函数 $\phi$ 的定义 将原本 210 * 160 像素，128 种颜色的图像下采样为 110 * 84 像素的灰度图像。 然后裁剪 84 * 84 像素的图像区域，主要包含 agent 正在运作的相关区域。 最后 $\phi$ 将裁剪后的历史状态最后 4 帧图像堆叠成一个向量，输入到 Q 网络中。 4.2 网络结构 第一层隐藏层采用 16 个 8 * 8 卷积核，步长为 4。采用非线性激活函数。 第二层隐藏层采用 32 个 4 * 4 卷积核，步长为 2。采用非线性激活函数。 最后一层隐藏层采用全连接层，并输出 256 个非线性激活函数的值。 输出层采用全连接线性的方式，并为每个动作输出一个 Q 值。 非线性激活函数第二篇论文中有提及，应该是 $max(0, x)$。 4.3 其他设置 由于每个游戏环境的 reward 范围不同，因此论文将正的 reward 限制为1，负的 reward 限制为-1，reward 值为0时保持不变。 采用 RMSProp 优化算法，mini-batch 的大小是32。$\epsilon$ -贪婪算法中的超参数 $\epsilon$ 在前面一百万时间步长中从1 退火到 0.1，最后保持不变。 训练一千万时间步长，经验回放池中存储最近一百万个帧。 游戏画面每隔 $k$ 帧，agent 就会选择一个新的动作。因为游戏仿真器的运算速度大于 DQN 模型计算动作的速度，为了防止画面卡顿，故此采用这样一个跳帧的方法。 $k$ 一般取值为4。 性能的度量标准采用的是预测的动作-价值函数 Q，而不是总的奖励值。 对于学习后的模型，选择动作还是采用 $\epsilon$ -贪婪策略，其中 $\epsilon$ 取值0.05。 5. 可以提升的地方经验回放池只能存储最近的 $N$ 条经验，采样用的是均匀分布。但是一种更合理的方式应该是给这些经验值分配不同的重要性权重，从而将新的经验代替那些不重要的经验。 二、 Nature DQN：独立目标函数 [2]1. 使用深层网络描述 Q 函数的问题已经有相关研究指出使用非线性函数逼近器对动作-值函数（Q 函数）进行逼近存在发散问题。这种不稳定来源于以下一些原因： 用于训练的状态是一个序列，该序列中的前后的状态高度相关。 Q 网络的参数有微小更新就会导致策略发生巨大变化，并因此导致训练样本分布的巨大变化。 目标函数中使用的 Q 函数（目标 Q 函数）和待优化 Q 函数（在线 Q 函数或估计 Q 函数）之间存在参数联系，每次更新的目标都是固定上次更新的参数得来，优化目标跟着优化过程一直在变。 第一篇论文解决了前两个问题，但第三个问题还是存在的。目标 Q 函数参数的计算仍然依赖于估计 Q 函数的参数。 2. 论文中提出的解决方法在第一篇 DQN 论文中，通过使用经验回放有效的解决了前两个问题，通过存储并随机采样经验来打破了样本之间的相关性，同时平滑了训练样本数据分布。 第三个问题则是这次改进完成的。通过让在线 Q 函数参数更新一定周期之后再去更新目标 Q 函数的参数，从而降低了目标 Q 函数与在线 Q 函数之间的相关性。 迭代更新 i 次后的损失函数表示如下： L_i(\theta_i) = \mathbb{E}_{(s,a,r,s') \sim U(\mathcal{D})} \left[ (r + \gamma max_{a'} Q(s',a';\theta_i^-) - Q(s,a;\theta_i))^2 \right]对比上篇论文3.1节的公式，可以发现目标 Q 函数中的参数是 $\theta^-$，而不是$\theta_{i-1}$。$\theta^-$ 是每经过 C 个步长从在线 Q 函数中复制而来，然后保留不变。在目标 Q 网络参数更新和在线 Q 网络参数更新之间增加一个时延可以减缓参数更新的分散和抖动。 最后本篇论文中还有一个小改进：进行误差裁剪。将上述损失函数中的 $r+\gamma Q(s,a;\theta^- - Q(s,a;\theta_i))$ 裁剪到范围（-1， 1），这可以提高算法的稳定性。 本篇论文的伪代码如下： 可以发现比上篇论文的伪代码多了独立目标 Q 网络的参数 $\theta^-$。另外奖励裁剪和误差裁剪并没有体现在伪代码中，但实际应用时是用到的。 3. 工程上的设置3.1 预处理 对每一帧进行编码时，取当前帧和前一帧每个像素颜色值的最大值。 将 RGB 帧转换为灰度帧，并裁剪大小为84 * 84。 预处理函数 $\phi$ 将每四个邻近帧作为状态输入到 Q 网络。 3.2 网络结构 不像以前一些方法，采用状态-动作对作为输入。DQN 的 Q 网络将状态表示作为输入，然后为每个动作输出一个 Q 值。 输入层是经过预处理和函数 $\phi$ 映射的 84 * 84 * 4 的灰度像素值 第一层隐藏层采用32个8 * 8，步长为4的卷积核，采用非线性激活函数（可能是 $\max(0,x)$）。 第二层隐藏层采用64个4 * 4，步长为2的卷积核，采用非线性激活函数。 第三层隐藏层采用64个3 * 3，步长为1的卷积核，采用非线性激活函数。 第四层隐藏层采用全连接层，输出为512维向量，采用非线性激活函数。 输出层是一个全连接层，对每个动作值对应一个输出。 这篇 DQN 论文相对于第一篇的网络结构更加复杂和庞大，隐藏层多了一层。 3.3 超参数的选取所有超参数的选取并非通过系统的搜索而得到，只是在一些游戏上做非正式的搜索得到的。因为超参数的选取十分重要，因此放一下论文中的截图，以备之后参考： 3.4 其他设置奖励裁剪、优化算法、跳帧数与第一篇 DQN 论文是一样的。 3.5 评估过程的设置每个游戏玩30次，采用 $\epsilon$ -贪婪策略，其中$\epsilon=0.05$。 4. DQN 学习到的特征表示DQN 可以将视觉上相似的画面表示成邻近的特征，并且还可以将奖励相似但视觉上不相似的画面也表示为邻近的特征，这说明 DQN 学习到的特征能够很好地预测 Q 值。上图是采用 t-SNE 方法绘制的最后一层隐藏层输出的状态特征分布图。颜色越红，表示动作-值越大。左下方，右上方和中下方三组图，每组里面的图像从视觉上看是相似的，经过 DQN 输出的特征表示也是邻近的。而左上方，中上方和右下方三组图，虽然每组里面的图像从视觉上看不相似，但是它们的动作-值是相似的，因此经过 DQN 输出的特征表示也是邻近的。 5. DQN 的核心点这篇论文中指出 DQN 的核心之处有三点： 使用了经验回放池 使用了独立的目标 Q 函数 深度卷积网络的设计 6. DQN 目前不能解决的问题long-term credit assignment 问题，也就是无法处理需要长远规划的策略。如果决策需要考虑的时间维度太长，DQN 可能无法学习出比较合适的策略。 7. DQN 的神经生物学基础DQN 是端到端强化学习方法，特征的学习和策略的学习并不是分开的，而是结合通过卷积神经网络结合在一起。获得的奖励会随时影响到卷积网络中的特征学习，然后制定出更好的策略，获取更好的奖励。在生物神经学中，已经有证据表明，在感知学习过程中，奖励信号可能会影响灵长类视觉皮质内表征的特征。 另外 DQN 中最重要的技巧——经验回放池，在神经生物学中也可以找到相似的机制。哺乳动物的海马体中存在一种物理机制，会在休息时间将最近经历过的经验轨迹重新激活（快速回放）。这意味着可以推测出动作-值函数可以通过历史经验进行学习。 不过海马体中对重要经验会存在更为深刻的印象，因此经验回放池也可以对经历过的经验分配不同的偏置权重，这是强化学习中的另一个话题，称为 prioritized sweeping。（上一篇论文中也提到过）]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next渲染数学公式]]></title>
    <url>%2F2019%2F09%2F11%2Fhexo-next%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[博客中通常会用latex来写数学公式，但是浏览器并不总是能渲染这些公式，导致在浏览器上看是一堆latex代码，这就很烦了。有一种比较简便的方法去渲染这些公式，就是在浏览器中添加相关的mathjax插件。遗憾的是，并不能保证每个浏览器都会有相应的mathjax插件，并且浏览博客的读者浏览器也不能保证一定装了这些插件。为了从根源上解决问题，我们直接让hexo具备渲染mathjax的能力，这样无论浏览器是否开启mathjax插件，公式都可以完美呈现在读者面前。 参考博客：https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/ 1. 使用Kramed 代替 MarkedMarked渲染引擎不可以渲染mathjax，但是Kramed可以。因此将hexo默认的Marked引擎卸载，装上Kramed。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：12345// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125; 变成：12345function formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ // return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$'); return text;&#125; 2. 停止使用hexo-math卸载原来的hexo-math：1npm uninstall hexo-math --save 安装hexo-renderer-mathjax包：1npm install hexo-renderer-mathjax --save 3. 更改默认转义规则因为 hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 , 所以我们需要对默认的规则进行修改.首先， 打开&lt; your-hexo-project &gt;/node_modules/kramed/lib/rules/inline.js,把下列代码：1escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改为：1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 把下列代码：1em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改为：1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 4. 配置中开启mathjax在主题 _config.yml 中开启 Mathjax， 首先找到math字段，然后修改两个enable为true，也就是将下列代码：12345math: enable: false ... mathjax: enable: false 更改为：12345math: enable: true ... mathjax: enable: true 需要注意的是，不同版本的next主题，上面的math和mathjax的嵌套关系可能不一致，总之就是把math和mathjax的enable全都设置为true就可以了。 5. 博客中开启mathjax在博客的模板中添加mathjax: true，注意冒号后面是有一个空格的。123456---title: Testing Mathjax with Hexocategory: Uncategorizeddate: 2017/05/03mathjax: true--- 如果嫌弃每次都需要声明开启mathjax，可以在&lt; your-hexo-project &gt;/scaffolds/post.md新增一句mathjax: true1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:mathjax: true--- 6. 更新Mathjax的CDN链接（可选）这个是可选的，一般情况下直接下载hexo-renderer-mathjax是不需要修改CDN的。如果以上配置都弄好后，还是不能渲染公式，可以尝试更新Mathjax的CDN链接。更新方法是：打开/node_modules/hexo-renderer-mathjax/mathjax.html，然后把标签\更改为：1&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; 注意：网上给出的CDN链接都未必百分百可用，建议多找几个试一下。可以去搜索mathjax国内cdn，参考网址：https://www.bootcdn.cn/mathjax/]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG代码实现]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1. 算法伪代码 首先是构建模型。构建并随机初始化估计网络 $Q(s,a|\theta^Q)$ 和 $\mu(s|\theta^\mu)$ 的权重 $\theta^Q$ 和 $\theta^\mu$。 构建目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，并对权重赋值： $\theta^{Q\prime} \leftarrow \theta^Q$，$\theta^{\mu\prime} \leftarrow \theta^{\mu}$。（公式中的导数符号如果看不清楚放大一点就可以了。。。） 初始化经验回放池 $R$。 接下来进行训练。外层循环是进行M个episode的训练，每个episode是智能体从行动开始到任务结束（或任务超时）的过程。为了进行有效探索，对确定性的动作 $\mu(s_t)$ 加上噪声（随机过程）$\mathcal{N}$。 初始化随机过程 $\mathcal{N}$。 获取初始状态值 $s_1$。 内层循环的次数是每个episode的时间长度 $T$： 根据确定性策略选取动作，并对动作添加噪声：$a_t = \mu(s_t|\theta^\mu)$。 执行动作 $a_t$，获取奖励 $r_t$ 和新的状态值 $s_{t+1}$。 将一个transition($s_t$, $a_t$, $r_t$, $s_{t+1}$)加入经验回放池。 对经验回报池进行采样，随机抽取 $N$ 个transitions构成一个mini-batch。 通过最小化Q值均方损失函数 $L$ 更新 $\theta^Q$，通过计算策略梯度 $\bigtriangledown_{\theta^\mu}$ 更新 $\theta^\mu$ 通过soft-update更新目标网络参数： \theta^{Q\prime} \leftarrow \tau\theta^Q + (1-\tau)\theta^{Q\prime}\theta^{\mu\prime} \leftarrow \tau\theta^\mu + (1-\tau)\theta^{\mu\prime} 2. 代码实现（基于pytorch）下面用代码实现简单的DDPG示例，用于gym中的小游戏Pendulum-v0（让摆锤倒立）。 2.1 构建网络构建描述动作-值函数的网络，采用三层全连接神经网络，输入是3维状态向量和1维动作值，输出是Q值。隐藏层采用relu激活函数，输出层不需要激活函数。代码如下：12345678910111213141516class QNet(nn.Module): def __init__(self): super(QNet, self).__init__() self.fc_s = nn.Linear(3, 64) self.fc_a = nn.Linear(1,64) self.fc_q = nn.Linear(128, 32) self.fc_3 = nn.Linear(32,1) def forward(self, x, a): h1 = F.relu(self.fc_s(x)) h2 = F.relu(self.fc_a(a)) cat = torch.cat([h1,h2], dim=1) q = F.relu(self.fc_q(cat)) q = self.fc_3(q) return q 构建描述动作策略的网络，采用三层全连接神经网络，输入是3维状态向量，输出是动作（连续的控制信号）。隐藏层采用relu激活函数，输出层采用tanh激活函数，动作值范围限定在[-1, 1]。代码如下：123456789101112class MuNet(nn.Module): def __init__(self): super(MuNet, self).__init__() self.fc1 = nn.Linear(3, 128) self.fc2 = nn.Linear(128, 64) self.fc_mu = nn.Linear(64, 1) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2] return mu 2.2 构建经验回放池经验回放池实际上是一个队列，当经验回放池满时，会抛弃旧的经验值，加入新采样的经验值。采样时，从经验回放池中随机抽取batch_size个经验值作为一个transition返回给训练机进行学习，代码如下：12345678910111213141516171819202122232425class ReplayBuffer(): def __init__(self): self.buffer = collections.deque(maxlen=buffer_limit) def put(self, transition): self.buffer.append(transition) def sample(self, n): mini_batch = random.sample(self.buffer, n) s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] for transition in mini_batch: s, a, r, s_prime, done_mask = transition s_lst.append(s) a_lst.append([a]) r_lst.append([r]) s_prime_lst.append(s_prime) done_mask_lst.append([done_mask]) return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \ torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \ torch.tensor(done_mask_lst) def size(self): return len(self.buffer) 2.3 构建Ornstein Uhlenbeck（OU）噪声OU过程是一种序贯相关过程，在DDPG中用于实现探索。OU过程满足下面的随机微分方程： dx_t = \theta(\mu - x(t)) dt + \sigma dW_t其中 $dW_t$ 是维纳过程（也称为布朗运动），满足： \triangle z = \epsilon \sqrt{\triangle t} ,\quad \epsilon \sim N(0, 1)$\triangle z$ 是变化量，$\epsilon$ 是标准正态分布。 OU过程的实现代码如下：1234567891011class OrnsteinUhlenbeckNoise: def __init__(self, mu): self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1 self.mu = mu self.x_prev = np.zeros_like(self.mu) def __call__(self): x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \ self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape) self.x_prev = x return x 2.4 构建训练模型估计网络的参数采用随机初始化，目标网络的参数复制估计网络的参数值。外层循环总共执行 $N$ 个episode，内层循环是每个episode的最大时间步长 $T$，超过 $T$ 之后重置状态，开启新的episode。每个episode结束之后，对估计网络和目标网络的参数进行训练和更新。需要注意的是，前期不会直接训练，直到经验回放池中的样本数量超过某个阈值才会开始训练策略。训练模型的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def train(): env = gym.make('Pendulum-v0') memory = ReplayBuffer() q, q_target = QNet(), QNet() q_target.load_state_dict(q.state_dict()) mu, mu_target = MuNet(), MuNet() mu_target.load_state_dict(mu.state_dict()) score = 0.0 print_interval = 20 mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu) q_optimizer = optim.Adam(q.parameters(), lr=lr_q) ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1)) for n_epi in range(N): s = env.reset() for t in range(T): # maximum length of episode is 200 for Pendulum-v0 a = mu(torch.from_numpy(s).float()) a = a.item() + ou_noise()[0] s_prime, r, done, info = env.step([a]) memory.put((s,a,r/100.0,s_prime,done)) score +=r s = s_prime if done: break if memory.size()&gt;train_threshold: for i in range(update_epoch): update(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer) soft_update(mu, mu_target) soft_update(q, q_target) if n_epi%print_interval==0 and n_epi!=0: print("# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;".format(n_epi, score/print_interval)) score = 0.0 torch.save(&#123; 'q_state_dict': q.state_dict(), 'q_target_state_dict': q_target.state_dict(), 'mu_state_dict': mu.state_dict(), 'mu_target_state_dict': mu_target.state_dict(), 'q_optimizer_state_dict': q_optimizer.state_dict(), 'mu_optimizer_state_dict': mu_optimizer.state_dict() &#125;, SAVE_PATH) 2.5 梯度下降更新参数采用adam优化器，对估计网络的参数 $\theta^Q$ 和 $\theta^\mu$ 进行mini-batch梯度下降优化。通过最小化均方损失函数,求 $\theta^Q$ 的梯度，利用adam优化器更新参数 $\theta^Q$。均方损失函数如下： L = \frac{1}{N} \sum_i(y_i - Q(s_i,a_i|\theta^Q))^2y_i = r_i + \gamma Q\prime(s_{t+1}, \mu\prime(s_{t+1}|\theta^{\mu\prime}) | \theta^{Q\prime})虽然实际上 $y_i$ 中也包含参数 $\theta^Q$，但是计算过程中通常忽略 $y_i$ 的梯度。 通过计算策略梯度，利用adam优化器更新参数 $\theta^\mu$ \bigtriangledown_{\theta^\mu} J \approx \frac{1}{N} \sum_i \bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_i, a=\mu(s_i)} \bigtriangledown_{\theta^\mu} \mu(s|\theta^\mu)|_{s=s_i}12345678910111213def update(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer): s,a,r,s_prime,done_mask = memory.sample(batch_size) target = r + gamma * q_target(s_prime, mu_target(s_prime)) q_loss = F.smooth_l1_loss(q(s,a), target.detach()) q_optimizer.zero_grad() q_loss.backward() q_optimizer.step() mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss. mu_optimizer.zero_grad() mu_loss.backward() mu_optimizer.step() 最后采用soft-update对目标网络 $Q\prime(s,a|\theta^{Q\prime})$ 和 $\mu\prime(s|\theta^{\mu\prime})$，代码如下：123def soft_update(net, net_target): for param_target, param in zip(net_target.parameters(), net.parameters()): param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau) 2.6 运行DDPG训练好模型之后，将网络的参数保存，然后在评测的时候重新载入模型参数。采用确定性的动作进行评测，代码如下所示：1234567891011121314def evaluate(): env = gym.make('Pendulum-v0') checkpoint = torch.load(SAVE_PATH) mu = MuNet() mu.load_state_dict(checkpoint['mu_state_dict']) mu.eval() while 1: s = env.reset() done = False while not done: a = mu(torch.from_numpy(s).float()) s_prime, r, done, info = env.step([a.item()]) env.render() s = s_prime 2.7 超参数的设置超参数的设置很重要，但没有特定的方法指导，所以全凭经验。本实验设置的超参数如下所示： 1234567891011lr_mu = 0.0005lr_q = 0.001gamma = 0.99batch_size = 32buffer_limit = 50000tau = 0.005 # for target network soft updateupdate_epoch = 10train_threshold = 2000T = 300N = 2000SAVE_PATH = 'PATH/ddpg.pt']]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDPG论文笔记]]></title>
    <url>%2F2019%2F09%2F10%2FDDPG%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 简述论文题目：《CONTINUOUS CONTROL WITH DEEP REINFORCEMENTLEARNING》。该论文提出了基于 deterministic policy gradient 的 DDPG(deep deterministic policy gradient) 算法，能够运用在连续的动作空间中，能够 learn policy “end to end”。 1.1 处理连续动作空间的问题 DQN存在的问题是只能处理低维度，离散的动作空间。 不能直接把Q-learning用在连续的动作空间中。因为Q-learning需要在每一次迭代中寻找最优的$a_t$。对于参数空间很大并且不受约束的近似函数和动作空间，寻找最优的$a_t$是非常非常慢的。 连续动作空间的离散化：离散化后的动作数量随着自由度呈指数增长，导致离散后的动作空间太大，很难进行有效地探索。而且过于稀疏的离散化抛弃了动作空间的本身的结构信息。 1.2 处理庞大状态空间的问题 非线性函数逼近器（例如神经网络）的收敛性无法保证，但是这种形式的函数逼近器对于庞大的状态空间的学习和泛化而言是必要的。 1.2 DDPG 的关键点 model-free, off-policy, actor-critic, using deep function approximators based on the the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) 借鉴DQN两个重要技巧，对神经网络表示的value functions进行学习： the network is trained off-policy with samples from a replay buffer to minimize correlations between samples. the network is trained with a target Q network to give consistent targets during temporal difference backups. use batch normalization (Ioffe &amp; Szegedy, 2015) DDPG的优点在于它的简洁，只需要actor-critic框架和简单的学习算法，能够在实际的控制问题上发挥优势，比需要很多动力学建模的规划算法效果好。 2. 相关知识重点是目标函数，动作-值函数以及它的贝尔曼方程（迭代形式），Q-learning算法（即off-policy方法，行动策略和目标策略不是同一种策略）。 2.1 符号定义 t 时刻观测值： $\boldsymbol{x_t}$ 假定环境完全可知，t 时刻状态值： $\boldsymbol{s_t} = \boldsymbol{x_t}$ t 时刻动作： $\boldsymbol{a_t} \in \mathbb{R}^N $ 奖励值： $r_t$，标量 动作策略： $\pi : \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ 初始状态分布： $p(s_1)$ 状态转移概率： $p(s_{t+1} | s_t, a_t)$ 奖励函数： $r(s_t, a_t)$ 2.2 公式定义 折扣奖励函数： $R_t = \sum_{i=t}^T \gamma ^ {i - t} r(s_i, a_i)$, $\gamma \in [0, 1]$ 折扣状态访问分布： $\rho^\pi(s’) := \int_{\mathcal{S}} \sum_{t=1}^\infty \gamma^{t-1} p_1(s) p(s \rightarrow s’, t, \pi) ds$ 目标函数： $J(\pi_\theta) = \int_\mathcal{S} \rho^{\pi}(s) \int_{\mathcal{A}} \pi_\theta(s, a) R_1(s, a) da ds =\mathbb{E}_{s_i \sim \rho^\pi, a_i \sim \pi}[R_1]$ 动作-值函数： $Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{i \gt t} \sim \rho^\pi, a_{i \gt t} \sim \pi}[R_t | s_t, a_t]$ 动作-值函数的贝尔曼方程（迭代形式）： $Q^\pi(s_t, a_t)=\mathbb{E}_{s_{t+1} \sim \rho^\pi} \left[r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} [Q^\pi (s_{t+1}, a_{t+1})] \right]$ 如果动作策略是确定性策略，表示为 $\mu : \mathcal{S} \rightarrow \mathcal{A}$，则动作-值函数的贝尔曼方程的形式为： Q^\mu(s_{t}, a_{t}) = \mathbb{E}_{s_{t+1} \sim \rho^\mu} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu (s_{t+1}))\right ] 假设用参数 $\theta^Q$ 去逼近动作-值函数并采用Q-learning，则定义损失函数如下： L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta, a_t \sim \beta} \left[(Q(s_t, a_t | \theta^Q) - y_t)^2 \right] y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q)注意虽然$y_t$中也含有$\theta^Q$，但是对$L(\theta^Q)$求梯度时，通常忽略$y_t$，不对其求导。 3. 算法DDPG算法基于DPG算法，并采纳DQN中的两个重要技巧。接下来简单介绍一下基础算法DPG。 3.1 DPG算法DPG算法采用确定性策略 $\mu(s | \theta^\mu)$，每一时刻都将状态映射成确定的动作，同时也采用actor-critic框架。critic用参数为$\theta^Q$的函数 $Q(s,a|\theta^Q)$表示，并通过Q-learning和贝尔曼方程的方式进行学习。actor对目标函数应用链式法则（即策略梯度）更新参数 $\theta^\mu$： \begin{aligned} \bigtriangledown_{\theta^\mu} J &\approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \bigtriangledown_{\theta^\mu}Q(s, a|\theta^Q) |_{s=s_t, a=\mu(s_t|\theta^\mu)} \right] \\ &= \mathbb{E}_{s_t \sim \rho^\beta} \left[\bigtriangledown_a Q(s, a|\theta^Q)|_{s=s_t, a=\mu(s_t)} \bigtriangledown_{\theta^\mu}\mu(s|\theta^\mu)|_{s=s_t} \right] \end{aligned}Q-learning采用的是异策略，行动策略表示为 $\beta$，目标策略表示为 $\mu$。 上述目标函数中，最外层是关于状态 $s_t$ 分布的平均，$s_t$ 就是通过执行 $\beta$ 策略采样来的。但是最里层动作-值函数如果展开成贝尔曼方程的形式，下一时刻采用的动作策略则是 $\mu$，如2.2节的第六条公式。 DPG 采用 SGD 随机梯度更新规则。采用批梯度下降是很难收敛的。 3.2 DDPG算法绝大多数优化算法都是假设样本是独立且均匀分布的，但是强化学习采集的样本本身就是一个序列，所以相邻间的样本是有关联的，不满足假设条件。另外，通常是采用mini-batch更新的方式，而不是在线更新。 为了解决样本相互关联的问题，DQN采用了经验回放池，具有固定大小，存储每次探索的 $(s_t, a_t, r_t, s_{t+1})$。当经验回放池满了之后，会丢弃最旧的数据，增加新采样数据。每次更新的时候，actor和critic都会统一从经验回放池中抽取一个mini-batch进行更新。经验回放池可以尽可能地设置得大一点，这样每次抽取的样本关联度会降低。 如果直接按照2.2节的第七条公式去更新网络 $Q(s, a|\theta^Q)$，在大多数环境中都是会发散的。因为该公式的计算目标值 $y_t$ 时也用了网络 $Q(s, a|\theta^Q)$ 的值。很明显，这个网络在还没有收敛的时候，作为目标的一部分进行学习，结果很容易发散。因为目标本身就好像在追求一个不稳定的值。 为了避免目标Q值 $y_t$ 中包含不稳定的网络 $Q(s, a|\theta^Q)$ 输出，采用称为“soft target update” 的更新方法。首先，重新复制一份actor网络，$\mu’(s|\theta^{\mu’})$和critic网络，$Q’(s, a|\theta^{Q’})$，它们用来计算目标Q值。目标网络 $\mu’(s|\theta^{\mu’})$ 和 $Q’(s, a|\theta^{Q’})$ 随着学习网络 $\mu(s|\theta^{\mu})$ 和 $Q(s, a|\theta^Q)$ 更新而更新，但是目标网络的更新幅度远小于学习网络： $\theta’ \leftarrow \tau \theta + (1 - \tau) \theta$， $\tau \ll 1$。这意味着目标网络变化缓慢，提高了目标值计算的稳定性。（不过应该没有彻底解决这一问题，毕竟还是目标网络用于目标值计算时也并非稳定的，只是变化缓慢而已。）但是降低目标网络的更新幅度，可能会让学习网络的更新变慢，也就是牺牲更新速度来提供收敛稳定性。不过文中提到，在实际中，学习的稳定性比学习的速度要重要得多。 另一个问题就是观测值包含不同元素(例如位置信息和速度信息)，这些元素包含不一样的物理量度，变化值范围不同，如果采用这些不同量度的元素，可能会导致网络学习效率变低，也很难在不同环境中泛化。因此需要将所有这些元素重新归一化到相同的范围。采用的方法是Ioffe 和 Szegedy 提出的batch normalization。这种方法会将每个mini-batch中的样本每个维度归一到相同的均值和方差。同时保留这个均值和方差，以用于测试的样本的归一化。对 $\mu$ 网络和 Q 网络的所有隐藏层和状态输入都进行归一化。 为了提高探索效率，采用的行动策略 $\mu’$是对目标策略 $\mu(s_t|\theta_t^\mu)$加上噪声 $\mathcal{N}$：$\mu’(s_t) = \mu(s_t|\theta_t^\mu) + \mathcal{N}$。论文中采用的噪声是Ornstein-Unlenbeck process（O-U过程）。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用HEXO搭建个人博客(Ubuntu 18)]]></title>
    <url>%2F2019%2F09%2F05%2F%E4%BD%BF%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[参考官方文档https://hexo.io/zh-cn/docs 1.安装nvm1wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh 将以下语句添加到.bashrc文件：12export NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm 开始安装1nvm install stable 2.安装HEXO1npm install -g hexo-cli 3.创建blog文件夹1234hexo init blog #通过hexo创建一个blog项目cd blog npm installhexo server #开发服务 4.配置git地址在blog项目根目录下里找到_config.yml文件，找到Deployment，然后按照如下修改：1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master 5.安装 hexo-deployer-git自动部署发布工具1npm install hexo-deployer-git --save 需要在博客目录下安装 6.生成静态文件部署到githubhexo clean &amp;&amp; hexo g &amp;&amp; hexo d 7.打开博客地址：yourname.github.io部署到github后可能得等一会才会生效，少则一分钟，多则半小时。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
